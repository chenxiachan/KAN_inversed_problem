{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f8a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 固定壁面温度的一维稳态计算\n",
    "\n",
    "# % 参数设置\n",
    "# L = 0.24;          % 墙体的厚度，单位为米 (240mm)\n",
    "# W = 1.0;           % 墙体的长度，单位为米 (1000mm)\n",
    "# alpha = 2.5*10^(-6);      % 热扩散率，单位为平方米每秒\n",
    "# T0 = 20;           % x=0处的温度，单位为摄氏度\n",
    "# TL = 40;           % x=L处的温度，单位为摄氏度\n",
    "# Ti = 25;           % 初始平均温度，单位为摄氏度\n",
    "\n",
    "# % 计算温度分布\n",
    "# x = linspace(0, L, 100); % 在墙的厚度方向生成100个点以计算温度分布\n",
    "# T = T0 + (TL - T0) * (x / L); % 线性温度分布计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553725ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6  # Thermal diffusivity of the soil in m^2/s\n",
    "# T0 = 20        # Initial surface temperature in Celsius\n",
    "# T1 = 40        # Surface temperature after change in Celsius\n",
    "# L = 4          # Calculation thickness in meters\n",
    "# dx = 0.1       # Spatial step in meters\n",
    "# dt = 1800      # Time step in seconds\n",
    "# tMax = 86400   # Simulation duration in seconds (1 day)\n",
    "# W = 30         # Image width for plotting in meters\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)  # Space grid\n",
    "# t = np.arange(dt, tMax + dt, dt)  # Time grid, start from dt to avoid divide by zero\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Normalize the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# x_mean = np.mean(x_data, axis=0)\n",
    "# x_std = np.std(x_data, axis=0) + 1e-8  # Add small constant to avoid division by zero\n",
    "# x_data_norm = (x_data - x_mean) / x_std\n",
    "\n",
    "# y_mean = np.mean(y_data)\n",
    "# y_std = np.std(y_data) + 1e-8  # Add small constant to avoid division by zero\n",
    "# y_data_norm = (y_data - y_mean) / y_std\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data_norm, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data_norm, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Initialize KAN with adjusted parameters\n",
    "# model = KAN(width=[2, 10, 1], grid=20, k=3, seed=0)  # Input layer now has 2 neurons\n",
    "\n",
    "# # Plot KAN at initialization\n",
    "# model(dataset['train_input'])\n",
    "# model.plot(beta=100)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Train the model with early stopping and learning rate scheduling\n",
    "# train_with_early_stopping(model, dataset, steps=500, patience=100, initial_lr=0.0025)\n",
    "\n",
    "# # Plot trained KAN\n",
    "# model.plot()\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "# # # Continue training to almost machine precision with conservative settings\n",
    "# # train_with_early_stopping(model, dataset, steps=200, patience=50, initial_lr=0.00001)\n",
    "\n",
    "# # Obtain the symbolic formula in terms of normalized data\n",
    "# symbolic_formula_normalized = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula (Normalized):\")\n",
    "# print(symbolic_formula_normalized)\n",
    "\n",
    "# # Reverse normalization for symbolic formula using sympy\n",
    "# x_sym, t_sym = symbols('x t')\n",
    "# normalized_formula = sympify(symbolic_formula_normalized)\n",
    "\n",
    "# # Replace normalized variables with original scale variables\n",
    "# original_formula = normalized_formula * (y_std / x_std[0]) + (y_mean - y_std / x_std[0] * x_mean[0])\n",
    "\n",
    "# print(\"Discovered Symbolic Formula (Original):\")\n",
    "# print(original_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_norm = (x_test - x_mean) / x_std\n",
    "# x_test_tensor = torch.tensor(x_test_norm, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "# predicted_temperature = predicted_temperature * y_std + y_mean  # Denormalize the prediction\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231d59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6  # Thermal diffusivity of the soil in m^2/s\n",
    "# T0 = 20        # Initial surface temperature in Celsius\n",
    "# T1 = 40        # Surface temperature after change in Celsius\n",
    "# L = 4          # Calculation thickness in meters\n",
    "# dx = 0.1       # Spatial step in meters\n",
    "# dt = 1800      # Time step in seconds\n",
    "# tMax = 86400   # Simulation duration in seconds (1 day)\n",
    "# W = 30         # Image width for plotting in meters\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)  # Space grid\n",
    "# t = np.arange(dt, tMax + dt, dt)  # Time grid, start from dt to avoid divide by zero\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Prepare the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Initialize KAN with adjusted parameters\n",
    "# model = KAN(width=[2, 20, 1], grid=20, k=3, seed=0)  # Input layer now has 2 neurons\n",
    "\n",
    "# # Plot KAN at initialization\n",
    "# model(dataset['train_input'])\n",
    "# model.plot(beta=100)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Train the model with early stopping and learning rate scheduling\n",
    "# train_with_early_stopping(model, dataset, steps=500, patience=100, initial_lr=0.0025)\n",
    "\n",
    "# # Plot trained KAN\n",
    "# model.plot()\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "# # # Continue training to almost machine precision with conservative settings\n",
    "# # train_with_early_stopping(model, dataset, steps=200, patience=50, initial_lr=0.00001)\n",
    "\n",
    "# # Obtain the symbolic formula\n",
    "# symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ddb6edf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6\n",
    "# T0 = 20\n",
    "# T1 = 40\n",
    "# L = 4\n",
    "# dx = 0.1\n",
    "# dt = 1800\n",
    "# tMax = 86400\n",
    "# W = 30\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)\n",
    "# t = np.arange(dt, tMax + dt, dt)\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Prepare the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Initial training with a coarse grid\n",
    "# initial_grid = 3\n",
    "# model = KAN(width=[2, 2, 2, 1], grid=initial_grid, k=3, seed=0)\n",
    "# train_with_early_stopping(model, dataset, steps=1000, patience=100, initial_lr=0.002)\n",
    "\n",
    "# # Iteratively refine the grid and retrain the model\n",
    "# grids = [5, 10, 20, 50, 100]\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "\n",
    "# for grid in grids:\n",
    "#     new_model = KAN(width=[2, 2, 2, 1], grid=grid, k=3).initialize_from_another_model(model, dataset['train_input'])\n",
    "#     train_with_early_stopping(new_model, dataset, steps=500, patience=100, initial_lr=0.00005)\n",
    "#     model = new_model  # Update the model to the new refined grid model\n",
    "\n",
    "#     # Collect training and test losses\n",
    "#     with torch.no_grad():\n",
    "#         train_outputs = model(dataset['train_input'])\n",
    "#         train_loss = torch.nn.functional.mse_loss(train_outputs, dataset['train_label']).item()\n",
    "#         test_outputs = model(dataset['test_input'])\n",
    "#         test_loss = torch.nn.functional.mse_loss(test_outputs, dataset['test_label']).item()\n",
    "#         train_losses.append(train_loss)\n",
    "#         test_losses.append(test_loss)\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "\n",
    "# # Obtain the symbolic formula\n",
    "# symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training and test losses over different grid refinements\n",
    "# plt.figure()\n",
    "# plt.plot(grids, train_losses, marker='o', label='Train Loss')\n",
    "# plt.plot(grids, test_losses, marker='o', label='Test Loss')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel('Grid Size')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.title('Training and Test Losses over Grid Refinements')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0e540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1000, Loss: 1.006316065788269, Validation Loss: 1.371787428855896\n",
      "Step 2/1000, Loss: 1.0023128986358643, Validation Loss: 1.359310269355774\n",
      "Step 3/1000, Loss: 0.9985527396202087, Validation Loss: 1.3476669788360596\n",
      "Step 4/1000, Loss: 0.9950290322303772, Validation Loss: 1.3368865251541138\n",
      "Step 5/1000, Loss: 0.9917342066764832, Validation Loss: 1.3269546031951904\n",
      "Step 6/1000, Loss: 0.9886600375175476, Validation Loss: 1.3178017139434814\n",
      "Step 7/1000, Loss: 0.9857967495918274, Validation Loss: 1.3093459606170654\n",
      "Step 8/1000, Loss: 0.9831345081329346, Validation Loss: 1.301518440246582\n",
      "Step 9/1000, Loss: 0.9806627631187439, Validation Loss: 1.294265866279602\n",
      "Step 10/1000, Loss: 0.9783706068992615, Validation Loss: 1.2875436544418335\n",
      "Step 11/1000, Loss: 0.97624671459198, Validation Loss: 1.2813127040863037\n",
      "Step 12/1000, Loss: 0.9742792844772339, Validation Loss: 1.2755358219146729\n",
      "Step 13/1000, Loss: 0.9724573493003845, Validation Loss: 1.2701786756515503\n",
      "Step 14/1000, Loss: 0.9707695245742798, Validation Loss: 1.2652071714401245\n",
      "Step 15/1000, Loss: 0.969205379486084, Validation Loss: 1.2605899572372437\n",
      "Step 16/1000, Loss: 0.9677556157112122, Validation Loss: 1.2562960386276245\n",
      "Step 17/1000, Loss: 0.9664110541343689, Validation Loss: 1.2522974014282227\n",
      "Step 18/1000, Loss: 0.9651638865470886, Validation Loss: 1.2485681772232056\n",
      "Step 19/1000, Loss: 0.964007556438446, Validation Loss: 1.2450841665267944\n",
      "Step 20/1000, Loss: 0.9629359245300293, Validation Loss: 1.241824746131897\n",
      "Step 21/1000, Loss: 0.9619439840316772, Validation Loss: 1.2387703657150269\n",
      "Step 22/1000, Loss: 0.9610271453857422, Validation Loss: 1.2359050512313843\n",
      "Step 23/1000, Loss: 0.9601820707321167, Validation Loss: 1.233214259147644\n",
      "Step 24/1000, Loss: 0.9594050049781799, Validation Loss: 1.2306855916976929\n",
      "Step 25/1000, Loss: 0.9586933255195618, Validation Loss: 1.2283083200454712\n",
      "Step 26/1000, Loss: 0.9580438733100891, Validation Loss: 1.2260736227035522\n",
      "Step 27/1000, Loss: 0.9574540853500366, Validation Loss: 1.2239737510681152\n",
      "Step 28/1000, Loss: 0.9569212198257446, Validation Loss: 1.2220022678375244\n",
      "Step 29/1000, Loss: 0.9564422369003296, Validation Loss: 1.2201539278030396\n",
      "Step 30/1000, Loss: 0.9560143351554871, Validation Loss: 1.2184239625930786\n",
      "Step 31/1000, Loss: 0.9556341171264648, Validation Loss: 1.216808557510376\n",
      "Step 32/1000, Loss: 0.9552984833717346, Validation Loss: 1.2153042554855347\n",
      "Step 33/1000, Loss: 0.9550034999847412, Validation Loss: 1.213908314704895\n",
      "Step 34/1000, Loss: 0.9547455906867981, Validation Loss: 1.2126178741455078\n",
      "Step 35/1000, Loss: 0.9545210003852844, Validation Loss: 1.211430311203003\n",
      "Step 36/1000, Loss: 0.9543253779411316, Validation Loss: 1.2103431224822998\n",
      "Step 37/1000, Loss: 0.9541550874710083, Validation Loss: 1.2093533277511597\n",
      "Step 38/1000, Loss: 0.9540061950683594, Validation Loss: 1.2084585428237915\n",
      "Step 39/1000, Loss: 0.9538751244544983, Validation Loss: 1.2076553106307983\n",
      "Step 40/1000, Loss: 0.9537583589553833, Validation Loss: 1.2069404125213623\n",
      "Step 41/1000, Loss: 0.9536526203155518, Validation Loss: 1.2063097953796387\n",
      "Step 42/1000, Loss: 0.9535552859306335, Validation Loss: 1.2057592868804932\n",
      "Step 43/1000, Loss: 0.9534639120101929, Validation Loss: 1.2052843570709229\n",
      "Step 44/1000, Loss: 0.953376054763794, Validation Loss: 1.204879879951477\n",
      "Step 45/1000, Loss: 0.9532900452613831, Validation Loss: 1.2045402526855469\n",
      "Step 46/1000, Loss: 0.9532045722007751, Validation Loss: 1.2042597532272339\n",
      "Step 47/1000, Loss: 0.9531185030937195, Validation Loss: 1.204032063484192\n",
      "Step 48/1000, Loss: 0.9530307054519653, Validation Loss: 1.203850507736206\n",
      "Step 49/1000, Loss: 0.9529401659965515, Validation Loss: 1.203708529472351\n",
      "Step 50/1000, Loss: 0.9528465867042542, Validation Loss: 1.2035988569259644\n",
      "Step 51/1000, Loss: 0.9527490139007568, Validation Loss: 1.2035146951675415\n",
      "Step 52/1000, Loss: 0.9526470303535461, Validation Loss: 1.2034488916397095\n",
      "Step 53/1000, Loss: 0.95253986120224, Validation Loss: 1.2033944129943848\n",
      "Step 54/1000, Loss: 0.9524268507957458, Validation Loss: 1.2033445835113525\n",
      "Step 55/1000, Loss: 0.9523075222969055, Validation Loss: 1.2032928466796875\n",
      "Step 56/1000, Loss: 0.9521806836128235, Validation Loss: 1.203233003616333\n",
      "Step 57/1000, Loss: 0.9520459175109863, Validation Loss: 1.2031595706939697\n",
      "Step 58/1000, Loss: 0.9519021511077881, Validation Loss: 1.2030670642852783\n",
      "Step 59/1000, Loss: 0.9517484307289124, Validation Loss: 1.2029513120651245\n",
      "Step 60/1000, Loss: 0.951583981513977, Validation Loss: 1.202807903289795\n",
      "Step 61/1000, Loss: 0.951407790184021, Validation Loss: 1.2026338577270508\n",
      "Step 62/1000, Loss: 0.9512190222740173, Validation Loss: 1.2024258375167847\n",
      "Step 63/1000, Loss: 0.9510166049003601, Validation Loss: 1.202182650566101\n",
      "Step 64/1000, Loss: 0.9507996439933777, Validation Loss: 1.2019020318984985\n",
      "Step 65/1000, Loss: 0.9505671858787537, Validation Loss: 1.2015831470489502\n",
      "Step 66/1000, Loss: 0.9503183364868164, Validation Loss: 1.2012255191802979\n",
      "Step 67/1000, Loss: 0.9500519037246704, Validation Loss: 1.2008287906646729\n",
      "Step 68/1000, Loss: 0.9497670531272888, Validation Loss: 1.2003929615020752\n",
      "Step 69/1000, Loss: 0.9494625926017761, Validation Loss: 1.1999183893203735\n",
      "Step 70/1000, Loss: 0.9491373300552368, Validation Loss: 1.1994054317474365\n",
      "Step 71/1000, Loss: 0.9487900137901306, Validation Loss: 1.1988542079925537\n",
      "Step 72/1000, Loss: 0.9484193325042725, Validation Loss: 1.1982648372650146\n",
      "Step 73/1000, Loss: 0.948023796081543, Validation Loss: 1.197637677192688\n",
      "Step 74/1000, Loss: 0.9476017355918884, Validation Loss: 1.1969720125198364\n",
      "Step 75/1000, Loss: 0.9471518397331238, Validation Loss: 1.1962674856185913\n",
      "Step 76/1000, Loss: 0.9466719627380371, Validation Loss: 1.1955229043960571\n",
      "Step 77/1000, Loss: 0.9461601376533508, Validation Loss: 1.194736361503601\n",
      "Step 78/1000, Loss: 0.9456146359443665, Validation Loss: 1.1939059495925903\n",
      "Step 79/1000, Loss: 0.9450330138206482, Validation Loss: 1.1930289268493652\n",
      "Step 80/1000, Loss: 0.9444131851196289, Validation Loss: 1.1921019554138184\n",
      "Step 81/1000, Loss: 0.9437525868415833, Validation Loss: 1.1911208629608154\n",
      "Step 82/1000, Loss: 0.9430487155914307, Validation Loss: 1.1900814771652222\n",
      "Step 83/1000, Loss: 0.9422988295555115, Validation Loss: 1.1889783143997192\n",
      "Step 84/1000, Loss: 0.9415000081062317, Validation Loss: 1.1878058910369873\n",
      "Step 85/1000, Loss: 0.9406492710113525, Validation Loss: 1.1865577697753906\n",
      "Step 86/1000, Loss: 0.9397430419921875, Validation Loss: 1.185227394104004\n",
      "Step 87/1000, Loss: 0.9387778043746948, Validation Loss: 1.1838072538375854\n",
      "Step 88/1000, Loss: 0.9377497434616089, Validation Loss: 1.182289958000183\n",
      "Step 89/1000, Loss: 0.9366546869277954, Validation Loss: 1.180667519569397\n",
      "Step 90/1000, Loss: 0.9354879260063171, Validation Loss: 1.178931713104248\n",
      "Step 91/1000, Loss: 0.9342446327209473, Validation Loss: 1.1770737171173096\n",
      "Step 92/1000, Loss: 0.9329196214675903, Validation Loss: 1.1750850677490234\n",
      "Step 93/1000, Loss: 0.931506872177124, Validation Loss: 1.172956943511963\n",
      "Step 94/1000, Loss: 0.9300000667572021, Validation Loss: 1.170680284500122\n",
      "Step 95/1000, Loss: 0.9283928275108337, Validation Loss: 1.1682463884353638\n",
      "Step 96/1000, Loss: 0.9266780614852905, Validation Loss: 1.165645956993103\n",
      "Step 97/1000, Loss: 0.9248483777046204, Validation Loss: 1.1628704071044922\n",
      "Step 98/1000, Loss: 0.9228960275650024, Validation Loss: 1.1599104404449463\n",
      "Step 99/1000, Loss: 0.9208124279975891, Validation Loss: 1.1567572355270386\n",
      "Step 100/1000, Loss: 0.9185892343521118, Validation Loss: 1.1534019708633423\n",
      "Step 101/1000, Loss: 0.9162172675132751, Validation Loss: 1.149835467338562\n",
      "Step 102/1000, Loss: 0.9136872291564941, Validation Loss: 1.1479629278182983\n",
      "Step 103/1000, Loss: 0.9123519659042358, Validation Loss: 1.1460212469100952\n",
      "Step 104/1000, Loss: 0.9109598398208618, Validation Loss: 1.1440114974975586\n",
      "Step 105/1000, Loss: 0.9095107913017273, Validation Loss: 1.1419342756271362\n",
      "Step 106/1000, Loss: 0.9080047607421875, Validation Loss: 1.1397897005081177\n",
      "Step 107/1000, Loss: 0.9064410924911499, Validation Loss: 1.1375783681869507\n",
      "Step 108/1000, Loss: 0.904819905757904, Validation Loss: 1.1353000402450562\n",
      "Step 109/1000, Loss: 0.9031403660774231, Validation Loss: 1.1329554319381714\n",
      "Step 110/1000, Loss: 0.9014021158218384, Validation Loss: 1.130543828010559\n",
      "Step 111/1000, Loss: 0.8996041417121887, Validation Loss: 1.128065586090088\n",
      "Step 112/1000, Loss: 0.8977463245391846, Validation Loss: 1.1255199909210205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 113/1000, Loss: 0.8958274722099304, Validation Loss: 1.1229074001312256\n",
      "Step 114/1000, Loss: 0.8938472867012024, Validation Loss: 1.1202269792556763\n",
      "Step 115/1000, Loss: 0.8918049931526184, Validation Loss: 1.1174790859222412\n",
      "Step 116/1000, Loss: 0.8897001147270203, Validation Loss: 1.114662528038025\n",
      "Step 117/1000, Loss: 0.8875318765640259, Validation Loss: 1.1117775440216064\n",
      "Step 118/1000, Loss: 0.8852999806404114, Validation Loss: 1.1088236570358276\n",
      "Step 119/1000, Loss: 0.8830040097236633, Validation Loss: 1.1058006286621094\n",
      "Step 120/1000, Loss: 0.8806434869766235, Validation Loss: 1.102708101272583\n",
      "Step 121/1000, Loss: 0.878217875957489, Validation Loss: 1.0995458364486694\n",
      "Step 122/1000, Loss: 0.8757271766662598, Validation Loss: 1.0963135957717896\n",
      "Step 123/1000, Loss: 0.8731709718704224, Validation Loss: 1.0930109024047852\n",
      "Step 124/1000, Loss: 0.8705492615699768, Validation Loss: 1.0896377563476562\n",
      "Step 125/1000, Loss: 0.8678617477416992, Validation Loss: 1.086193561553955\n",
      "Step 126/1000, Loss: 0.8651086091995239, Validation Loss: 1.0826783180236816\n",
      "Step 127/1000, Loss: 0.8622896671295166, Validation Loss: 1.079092025756836\n",
      "Step 128/1000, Loss: 0.8594049215316772, Validation Loss: 1.0754340887069702\n",
      "Step 129/1000, Loss: 0.856454610824585, Validation Loss: 1.0717042684555054\n",
      "Step 130/1000, Loss: 0.8534388542175293, Validation Loss: 1.0679025650024414\n",
      "Step 131/1000, Loss: 0.850357711315155, Validation Loss: 1.0640286207199097\n",
      "Step 132/1000, Loss: 0.847211480140686, Validation Loss: 1.0600817203521729\n",
      "Step 133/1000, Loss: 0.8440003395080566, Validation Loss: 1.0560617446899414\n",
      "Step 134/1000, Loss: 0.840724527835846, Validation Loss: 1.0519680976867676\n",
      "Step 135/1000, Loss: 0.8373843431472778, Validation Loss: 1.047800064086914\n",
      "Step 136/1000, Loss: 0.8339800834655762, Validation Loss: 1.0435569286346436\n",
      "Step 137/1000, Loss: 0.8305119872093201, Validation Loss: 1.0392374992370605\n",
      "Step 138/1000, Loss: 0.8269802927970886, Validation Loss: 1.0348412990570068\n",
      "Step 139/1000, Loss: 0.8233853578567505, Validation Loss: 1.030367136001587\n",
      "Step 140/1000, Loss: 0.8197274208068848, Validation Loss: 1.0258136987686157\n",
      "Step 141/1000, Loss: 0.8160068988800049, Validation Loss: 1.021180272102356\n",
      "Step 142/1000, Loss: 0.8122239112854004, Validation Loss: 1.0164657831192017\n",
      "Step 143/1000, Loss: 0.8083789944648743, Validation Loss: 1.011669397354126\n",
      "Step 144/1000, Loss: 0.8044723868370056, Validation Loss: 1.0067905187606812\n",
      "Step 145/1000, Loss: 0.8005044460296631, Validation Loss: 1.0018283128738403\n",
      "Step 146/1000, Loss: 0.7964754700660706, Validation Loss: 0.9967820048332214\n",
      "Step 147/1000, Loss: 0.7923859357833862, Validation Loss: 0.9916514158248901\n",
      "Step 148/1000, Loss: 0.7882363200187683, Validation Loss: 0.9864358901977539\n",
      "Step 149/1000, Loss: 0.7840270400047302, Validation Loss: 0.9811349511146545\n",
      "Step 150/1000, Loss: 0.7797586917877197, Validation Loss: 0.9757480025291443\n",
      "Step 151/1000, Loss: 0.775431752204895, Validation Loss: 0.970274806022644\n",
      "Step 152/1000, Loss: 0.7710465788841248, Validation Loss: 0.9647147059440613\n",
      "Step 153/1000, Loss: 0.7666038870811462, Validation Loss: 0.9590675830841064\n",
      "Step 154/1000, Loss: 0.762104332447052, Validation Loss: 0.9533331394195557\n",
      "Step 155/1000, Loss: 0.7575485110282898, Validation Loss: 0.9475111365318298\n",
      "Step 156/1000, Loss: 0.7529371380805969, Validation Loss: 0.9416015148162842\n",
      "Step 157/1000, Loss: 0.7482709288597107, Validation Loss: 0.9356046319007874\n",
      "Step 158/1000, Loss: 0.7435507774353027, Validation Loss: 0.9295211434364319\n",
      "Step 159/1000, Loss: 0.7387774586677551, Validation Loss: 0.9233517646789551\n",
      "Step 160/1000, Loss: 0.7339519262313843, Validation Loss: 0.9170975089073181\n",
      "Step 161/1000, Loss: 0.7290752530097961, Validation Loss: 0.9107598662376404\n",
      "Step 162/1000, Loss: 0.724148690700531, Validation Loss: 0.9043408632278442\n",
      "Step 163/1000, Loss: 0.7191734313964844, Validation Loss: 0.8978426456451416\n",
      "Step 164/1000, Loss: 0.7141505479812622, Validation Loss: 0.8912674784660339\n",
      "Step 165/1000, Loss: 0.709081768989563, Validation Loss: 0.8846184611320496\n",
      "Step 166/1000, Loss: 0.703968346118927, Validation Loss: 0.8778984546661377\n",
      "Step 167/1000, Loss: 0.6988120079040527, Validation Loss: 0.8711106181144714\n",
      "Step 168/1000, Loss: 0.6936143040657043, Validation Loss: 0.8642584085464478\n",
      "Step 169/1000, Loss: 0.6883769631385803, Validation Loss: 0.8573452234268188\n",
      "Step 170/1000, Loss: 0.6831017732620239, Validation Loss: 0.8503748178482056\n",
      "Step 171/1000, Loss: 0.6777908205986023, Validation Loss: 0.8433505892753601\n",
      "Step 172/1000, Loss: 0.6724456548690796, Validation Loss: 0.8362762331962585\n",
      "Step 173/1000, Loss: 0.6670684814453125, Validation Loss: 0.829155683517456\n",
      "Step 174/1000, Loss: 0.6616612672805786, Validation Loss: 0.8219925761222839\n",
      "Step 175/1000, Loss: 0.6562260389328003, Validation Loss: 0.8147906064987183\n",
      "Step 176/1000, Loss: 0.6507648825645447, Validation Loss: 0.807553768157959\n",
      "Step 177/1000, Loss: 0.6452797651290894, Validation Loss: 0.800286054611206\n",
      "Step 178/1000, Loss: 0.639772891998291, Validation Loss: 0.7929915189743042\n",
      "Step 179/1000, Loss: 0.6342464089393616, Validation Loss: 0.7856744527816772\n",
      "Step 180/1000, Loss: 0.6287024617195129, Validation Loss: 0.7783389091491699\n",
      "Step 181/1000, Loss: 0.6231430172920227, Validation Loss: 0.7709898352622986\n",
      "Step 182/1000, Loss: 0.6175702810287476, Validation Loss: 0.7636314630508423\n",
      "Step 183/1000, Loss: 0.6119864583015442, Validation Loss: 0.7562687397003174\n",
      "Step 184/1000, Loss: 0.6063938140869141, Validation Loss: 0.7489067316055298\n",
      "Step 185/1000, Loss: 0.6007943153381348, Validation Loss: 0.7415499091148376\n",
      "Step 186/1000, Loss: 0.5951902866363525, Validation Loss: 0.7342034578323364\n",
      "Step 187/1000, Loss: 0.5895838737487793, Validation Loss: 0.7268717885017395\n",
      "Step 188/1000, Loss: 0.583977222442627, Validation Loss: 0.7195595502853394\n",
      "Step 189/1000, Loss: 0.5783725380897522, Validation Loss: 0.7122711539268494\n",
      "Step 190/1000, Loss: 0.5727719664573669, Validation Loss: 0.7050105333328247\n",
      "Step 191/1000, Loss: 0.5671777725219727, Validation Loss: 0.6977812647819519\n",
      "Step 192/1000, Loss: 0.5615921020507812, Validation Loss: 0.6905864477157593\n",
      "Step 193/1000, Loss: 0.5560171008110046, Validation Loss: 0.6834291815757751\n",
      "Step 194/1000, Loss: 0.5504549741744995, Validation Loss: 0.6763112545013428\n",
      "Step 195/1000, Loss: 0.5449076294898987, Validation Loss: 0.6692349314689636\n",
      "Step 196/1000, Loss: 0.5393774509429932, Validation Loss: 0.6622014045715332\n",
      "Step 197/1000, Loss: 0.533866286277771, Validation Loss: 0.6552114486694336\n",
      "Step 198/1000, Loss: 0.5283761620521545, Validation Loss: 0.6482660174369812\n",
      "Step 199/1000, Loss: 0.5229093432426453, Validation Loss: 0.6413649916648865\n",
      "Step 200/1000, Loss: 0.5174674391746521, Validation Loss: 0.6345082521438599\n",
      "Step 201/1000, Loss: 0.5120524764060974, Validation Loss: 0.6276958584785461\n",
      "Step 202/1000, Loss: 0.5066663026809692, Validation Loss: 0.6243146061897278\n",
      "Step 203/1000, Loss: 0.5039883255958557, Validation Loss: 0.6209606528282166\n",
      "Step 204/1000, Loss: 0.5013258457183838, Validation Loss: 0.6176331639289856\n",
      "Step 205/1000, Loss: 0.4986792504787445, Validation Loss: 0.6143308281898499\n",
      "Step 206/1000, Loss: 0.49604859948158264, Validation Loss: 0.6110527515411377\n",
      "Step 207/1000, Loss: 0.49343425035476685, Validation Loss: 0.6077976226806641\n",
      "Step 208/1000, Loss: 0.49083614349365234, Validation Loss: 0.6045642495155334\n",
      "Step 209/1000, Loss: 0.48825448751449585, Validation Loss: 0.6013515591621399\n",
      "Step 210/1000, Loss: 0.4856894016265869, Validation Loss: 0.5981585383415222\n",
      "Step 211/1000, Loss: 0.4831409156322479, Validation Loss: 0.5949839949607849\n",
      "Step 212/1000, Loss: 0.4806089997291565, Validation Loss: 0.5918270945549011\n",
      "Step 213/1000, Loss: 0.4780937433242798, Validation Loss: 0.5886868834495544\n",
      "Step 214/1000, Loss: 0.4755951762199402, Validation Loss: 0.5855624079704285\n",
      "Step 215/1000, Loss: 0.47311314940452576, Validation Loss: 0.5824530124664307\n",
      "Step 216/1000, Loss: 0.47064775228500366, Validation Loss: 0.5793582797050476\n",
      "Step 217/1000, Loss: 0.4681989252567291, Validation Loss: 0.5762771964073181\n",
      "Step 218/1000, Loss: 0.465766578912735, Validation Loss: 0.5732096433639526\n",
      "Step 219/1000, Loss: 0.46335071325302124, Validation Loss: 0.5701552033424377\n",
      "Step 220/1000, Loss: 0.4609512686729431, Validation Loss: 0.5671135187149048\n",
      "Step 221/1000, Loss: 0.45856815576553345, Validation Loss: 0.564084529876709\n",
      "Step 222/1000, Loss: 0.4562011957168579, Validation Loss: 0.561068058013916\n",
      "Step 223/1000, Loss: 0.45385050773620605, Validation Loss: 0.5580640435218811\n",
      "Step 224/1000, Loss: 0.45151591300964355, Validation Loss: 0.5550726652145386\n",
      "Step 225/1000, Loss: 0.4491972327232361, Validation Loss: 0.5520938634872437\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 226/1000, Loss: 0.4468945562839508, Validation Loss: 0.5491279363632202\n",
      "Step 227/1000, Loss: 0.444607675075531, Validation Loss: 0.5461750626564026\n",
      "Step 228/1000, Loss: 0.4423365592956543, Validation Loss: 0.5432354807853699\n",
      "Step 229/1000, Loss: 0.44008100032806396, Validation Loss: 0.5403094291687012\n",
      "Step 230/1000, Loss: 0.4378410875797272, Validation Loss: 0.5373971462249756\n",
      "Step 231/1000, Loss: 0.43561652302742004, Validation Loss: 0.5344990491867065\n",
      "Step 232/1000, Loss: 0.43340733647346497, Validation Loss: 0.5316153764724731\n",
      "Step 233/1000, Loss: 0.4312133491039276, Validation Loss: 0.528746485710144\n",
      "Step 234/1000, Loss: 0.4290345311164856, Validation Loss: 0.5258926153182983\n",
      "Step 235/1000, Loss: 0.42687076330184937, Validation Loss: 0.5230541229248047\n",
      "Step 236/1000, Loss: 0.4247218668460846, Validation Loss: 0.5202312469482422\n",
      "Step 237/1000, Loss: 0.4225878119468689, Validation Loss: 0.5174242258071899\n",
      "Step 238/1000, Loss: 0.4204684793949127, Validation Loss: 0.514633297920227\n",
      "Step 239/1000, Loss: 0.41836366057395935, Validation Loss: 0.5118587613105774\n",
      "Step 240/1000, Loss: 0.4162735044956207, Validation Loss: 0.5091007947921753\n",
      "Step 241/1000, Loss: 0.4141975939273834, Validation Loss: 0.5063595175743103\n",
      "Step 242/1000, Loss: 0.412136048078537, Validation Loss: 0.5036352276802063\n",
      "Step 243/1000, Loss: 0.4100886285305023, Validation Loss: 0.5009279847145081\n",
      "Step 244/1000, Loss: 0.4080553352832794, Validation Loss: 0.4982379674911499\n",
      "Step 245/1000, Loss: 0.4060359597206116, Validation Loss: 0.4955652356147766\n",
      "Step 246/1000, Loss: 0.4040304720401764, Validation Loss: 0.49290984869003296\n",
      "Step 247/1000, Loss: 0.40203872323036194, Validation Loss: 0.4902718961238861\n",
      "Step 248/1000, Loss: 0.4000606834888458, Validation Loss: 0.487651526927948\n",
      "Step 249/1000, Loss: 0.3980961740016937, Validation Loss: 0.4850486218929291\n",
      "Step 250/1000, Loss: 0.3961451053619385, Validation Loss: 0.48246335983276367\n",
      "Step 251/1000, Loss: 0.3942073881626129, Validation Loss: 0.47989559173583984\n",
      "Step 252/1000, Loss: 0.39228296279907227, Validation Loss: 0.47734543681144714\n",
      "Step 253/1000, Loss: 0.3903716206550598, Validation Loss: 0.47481289505958557\n",
      "Step 254/1000, Loss: 0.3884733021259308, Validation Loss: 0.4722978174686432\n",
      "Step 255/1000, Loss: 0.3865880072116852, Validation Loss: 0.46980026364326477\n",
      "Step 256/1000, Loss: 0.3847154378890991, Validation Loss: 0.46732017397880554\n",
      "Step 257/1000, Loss: 0.38285574316978455, Validation Loss: 0.46485745906829834\n",
      "Step 258/1000, Loss: 0.3810085952281952, Validation Loss: 0.4624120891094208\n",
      "Step 259/1000, Loss: 0.37917405366897583, Validation Loss: 0.45998409390449524\n",
      "Step 260/1000, Loss: 0.37735193967819214, Validation Loss: 0.45757320523262024\n",
      "Step 261/1000, Loss: 0.37554219365119934, Validation Loss: 0.4551796019077301\n",
      "Step 262/1000, Loss: 0.3737446367740631, Validation Loss: 0.45280301570892334\n",
      "Step 263/1000, Loss: 0.3719593584537506, Validation Loss: 0.45044344663619995\n",
      "Step 264/1000, Loss: 0.37018609046936035, Validation Loss: 0.4481008052825928\n",
      "Step 265/1000, Loss: 0.36842480301856995, Validation Loss: 0.4457750618457794\n",
      "Step 266/1000, Loss: 0.36667540669441223, Validation Loss: 0.44346609711647034\n",
      "Step 267/1000, Loss: 0.36493781208992004, Validation Loss: 0.4411737620830536\n",
      "Step 268/1000, Loss: 0.36321187019348145, Validation Loss: 0.4388982057571411\n",
      "Step 269/1000, Loss: 0.36149758100509644, Validation Loss: 0.4366391599178314\n",
      "Step 270/1000, Loss: 0.3597947955131531, Validation Loss: 0.4343966543674469\n",
      "Step 271/1000, Loss: 0.3581034243106842, Validation Loss: 0.43217065930366516\n",
      "Step 272/1000, Loss: 0.35642337799072266, Validation Loss: 0.4299609959125519\n",
      "Step 273/1000, Loss: 0.35475456714630127, Validation Loss: 0.42776769399642944\n",
      "Step 274/1000, Loss: 0.35309696197509766, Validation Loss: 0.42559075355529785\n",
      "Step 275/1000, Loss: 0.3514503836631775, Validation Loss: 0.42343005537986755\n",
      "Step 276/1000, Loss: 0.349814772605896, Validation Loss: 0.42128556966781616\n",
      "Step 277/1000, Loss: 0.3481900990009308, Validation Loss: 0.4191572368144989\n",
      "Step 278/1000, Loss: 0.34657615423202515, Validation Loss: 0.4170450270175934\n",
      "Step 279/1000, Loss: 0.3449729084968567, Validation Loss: 0.414948970079422\n",
      "Step 280/1000, Loss: 0.3433803617954254, Validation Loss: 0.4128689765930176\n",
      "Step 281/1000, Loss: 0.341798335313797, Validation Loss: 0.41080501675605774\n",
      "Step 282/1000, Loss: 0.3402267396450043, Validation Loss: 0.4087570905685425\n",
      "Step 283/1000, Loss: 0.33866554498672485, Validation Loss: 0.406725138425827\n",
      "Step 284/1000, Loss: 0.3371146321296692, Validation Loss: 0.4047090411186218\n",
      "Step 285/1000, Loss: 0.33557388186454773, Validation Loss: 0.40270882844924927\n",
      "Step 286/1000, Loss: 0.3340432345867157, Validation Loss: 0.40072450041770935\n",
      "Step 287/1000, Loss: 0.3325227200984955, Validation Loss: 0.39875587821006775\n",
      "Step 288/1000, Loss: 0.331012099981308, Validation Loss: 0.39680296182632446\n",
      "Step 289/1000, Loss: 0.3295113146305084, Validation Loss: 0.39486563205718994\n",
      "Step 290/1000, Loss: 0.3280203342437744, Validation Loss: 0.3929439187049866\n",
      "Step 291/1000, Loss: 0.3265390992164612, Validation Loss: 0.3910376727581024\n",
      "Step 292/1000, Loss: 0.3250674307346344, Validation Loss: 0.3891468346118927\n",
      "Step 293/1000, Loss: 0.32360532879829407, Validation Loss: 0.3872712552547455\n",
      "Step 294/1000, Loss: 0.32215267419815063, Validation Loss: 0.385410875082016\n",
      "Step 295/1000, Loss: 0.3207094371318817, Validation Loss: 0.38356560468673706\n",
      "Step 296/1000, Loss: 0.319275438785553, Validation Loss: 0.381735235452652\n",
      "Step 297/1000, Loss: 0.3178507387638092, Validation Loss: 0.37991979718208313\n",
      "Step 298/1000, Loss: 0.31643515825271606, Validation Loss: 0.37811917066574097\n",
      "Step 299/1000, Loss: 0.31502866744995117, Validation Loss: 0.3763331174850464\n",
      "Step 300/1000, Loss: 0.3136311173439026, Validation Loss: 0.37456151843070984\n",
      "Step 301/1000, Loss: 0.3122424781322479, Validation Loss: 0.37280431389808655\n",
      "Step 302/1000, Loss: 0.3108627200126648, Validation Loss: 0.3719324469566345\n",
      "Step 303/1000, Loss: 0.3101768493652344, Validation Loss: 0.3710665702819824\n",
      "Step 304/1000, Loss: 0.30949434638023376, Validation Loss: 0.37020644545555115\n",
      "Step 305/1000, Loss: 0.3088151216506958, Validation Loss: 0.3693520426750183\n",
      "Step 306/1000, Loss: 0.3081391751766205, Validation Loss: 0.36850300431251526\n",
      "Step 307/1000, Loss: 0.30746641755104065, Validation Loss: 0.367659330368042\n",
      "Step 308/1000, Loss: 0.306796669960022, Validation Loss: 0.3668206036090851\n",
      "Step 309/1000, Loss: 0.30612999200820923, Validation Loss: 0.3659868538379669\n",
      "Step 310/1000, Loss: 0.3054662048816681, Validation Loss: 0.3651578426361084\n",
      "Step 311/1000, Loss: 0.30480527877807617, Validation Loss: 0.36433354020118713\n",
      "Step 312/1000, Loss: 0.30414721369743347, Validation Loss: 0.36351364850997925\n",
      "Step 313/1000, Loss: 0.30349189043045044, Validation Loss: 0.36269816756248474\n",
      "Step 314/1000, Loss: 0.3028392791748047, Validation Loss: 0.3618869185447693\n",
      "Step 315/1000, Loss: 0.30218935012817383, Validation Loss: 0.3610798120498657\n",
      "Step 316/1000, Loss: 0.3015420138835907, Validation Loss: 0.3602766692638397\n",
      "Step 317/1000, Loss: 0.3008972704410553, Validation Loss: 0.35947751998901367\n",
      "Step 318/1000, Loss: 0.30025508999824524, Validation Loss: 0.35868215560913086\n",
      "Step 319/1000, Loss: 0.29961544275283813, Validation Loss: 0.35789060592651367\n",
      "Step 320/1000, Loss: 0.2989782691001892, Validation Loss: 0.357102632522583\n",
      "Step 321/1000, Loss: 0.29834359884262085, Validation Loss: 0.35631829500198364\n",
      "Step 322/1000, Loss: 0.29771125316619873, Validation Loss: 0.35553744435310364\n",
      "Step 323/1000, Loss: 0.2970813810825348, Validation Loss: 0.3547600209712982\n",
      "Step 324/1000, Loss: 0.2964538037776947, Validation Loss: 0.3539859354496002\n",
      "Step 325/1000, Loss: 0.29582861065864563, Validation Loss: 0.35321518778800964\n",
      "Step 326/1000, Loss: 0.2952057123184204, Validation Loss: 0.3524476885795593\n",
      "Step 327/1000, Loss: 0.29458513855934143, Validation Loss: 0.3516834080219269\n",
      "Step 328/1000, Loss: 0.29396679997444153, Validation Loss: 0.35092225670814514\n",
      "Step 329/1000, Loss: 0.2933506667613983, Validation Loss: 0.3501642942428589\n",
      "Step 330/1000, Loss: 0.29273682832717896, Validation Loss: 0.3494093716144562\n",
      "Step 331/1000, Loss: 0.2921251654624939, Validation Loss: 0.3486575484275818\n",
      "Step 332/1000, Loss: 0.29151567816734314, Validation Loss: 0.34790879487991333\n",
      "Step 333/1000, Loss: 0.2909083962440491, Validation Loss: 0.34716302156448364\n",
      "Step 334/1000, Loss: 0.29030323028564453, Validation Loss: 0.3464202880859375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 335/1000, Loss: 0.2897002398967743, Validation Loss: 0.3456803858280182\n",
      "Step 336/1000, Loss: 0.2890992760658264, Validation Loss: 0.3449435532093048\n",
      "Step 337/1000, Loss: 0.28850045800209045, Validation Loss: 0.34420961141586304\n",
      "Step 338/1000, Loss: 0.28790369629859924, Validation Loss: 0.3434787094593048\n",
      "Step 339/1000, Loss: 0.28730908036231995, Validation Loss: 0.3427506387233734\n",
      "Step 340/1000, Loss: 0.28671643137931824, Validation Loss: 0.3420255184173584\n",
      "Step 341/1000, Loss: 0.2861258387565613, Validation Loss: 0.3413033187389374\n",
      "Step 342/1000, Loss: 0.2855372726917267, Validation Loss: 0.3405839800834656\n",
      "Step 343/1000, Loss: 0.28495073318481445, Validation Loss: 0.33986756205558777\n",
      "Step 344/1000, Loss: 0.2843661606311798, Validation Loss: 0.3391540050506592\n",
      "Step 345/1000, Loss: 0.28378358483314514, Validation Loss: 0.3384433090686798\n",
      "Step 346/1000, Loss: 0.2832029461860657, Validation Loss: 0.33773547410964966\n",
      "Step 347/1000, Loss: 0.2826242744922638, Validation Loss: 0.33703047037124634\n",
      "Step 348/1000, Loss: 0.2820475697517395, Validation Loss: 0.336328387260437\n",
      "Step 349/1000, Loss: 0.28147274255752563, Validation Loss: 0.33562907576560974\n",
      "Step 350/1000, Loss: 0.28089988231658936, Validation Loss: 0.3349325954914093\n",
      "Step 351/1000, Loss: 0.2803289294242859, Validation Loss: 0.33423885703086853\n",
      "Step 352/1000, Loss: 0.27975982427597046, Validation Loss: 0.33354803919792175\n",
      "Step 353/1000, Loss: 0.27919262647628784, Validation Loss: 0.33285990357398987\n",
      "Step 354/1000, Loss: 0.27862727642059326, Validation Loss: 0.3321745693683624\n",
      "Step 355/1000, Loss: 0.27806374430656433, Validation Loss: 0.33149203658103943\n",
      "Step 356/1000, Loss: 0.2775021493434906, Validation Loss: 0.3308122456073761\n",
      "Step 357/1000, Loss: 0.27694234251976013, Validation Loss: 0.3301351070404053\n",
      "Step 358/1000, Loss: 0.27638429403305054, Validation Loss: 0.3294607102870941\n",
      "Step 359/1000, Loss: 0.27582812309265137, Validation Loss: 0.32878902554512024\n",
      "Step 360/1000, Loss: 0.27527371048927307, Validation Loss: 0.32812005281448364\n",
      "Step 361/1000, Loss: 0.2747211456298828, Validation Loss: 0.32745373249053955\n",
      "Step 362/1000, Loss: 0.2741703391075134, Validation Loss: 0.32679009437561035\n",
      "Step 363/1000, Loss: 0.2736212909221649, Validation Loss: 0.32612910866737366\n",
      "Step 364/1000, Loss: 0.2730740010738373, Validation Loss: 0.3254706561565399\n",
      "Step 365/1000, Loss: 0.27252843976020813, Validation Loss: 0.32481497526168823\n",
      "Step 366/1000, Loss: 0.27198466658592224, Validation Loss: 0.3241617679595947\n",
      "Step 367/1000, Loss: 0.27144259214401245, Validation Loss: 0.3235113024711609\n",
      "Step 368/1000, Loss: 0.27090224623680115, Validation Loss: 0.3228633403778076\n",
      "Step 369/1000, Loss: 0.27036359906196594, Validation Loss: 0.3222179412841797\n",
      "Step 370/1000, Loss: 0.2698266804218292, Validation Loss: 0.3215751349925995\n",
      "Step 371/1000, Loss: 0.26929140090942383, Validation Loss: 0.32093486189842224\n",
      "Step 372/1000, Loss: 0.2687578797340393, Validation Loss: 0.32029709219932556\n",
      "Step 373/1000, Loss: 0.2682259678840637, Validation Loss: 0.3196618854999542\n",
      "Step 374/1000, Loss: 0.26769569516181946, Validation Loss: 0.31902918219566345\n",
      "Step 375/1000, Loss: 0.2671671211719513, Validation Loss: 0.31839895248413086\n",
      "Step 376/1000, Loss: 0.26664021611213684, Validation Loss: 0.3177712559700012\n",
      "Step 377/1000, Loss: 0.2661149203777313, Validation Loss: 0.31714606285095215\n",
      "Step 378/1000, Loss: 0.26559123396873474, Validation Loss: 0.3165232539176941\n",
      "Step 379/1000, Loss: 0.2650691568851471, Validation Loss: 0.3159030079841614\n",
      "Step 380/1000, Loss: 0.2645486891269684, Validation Loss: 0.3152851462364197\n",
      "Step 381/1000, Loss: 0.264029860496521, Validation Loss: 0.3146698474884033\n",
      "Step 382/1000, Loss: 0.26351261138916016, Validation Loss: 0.3140569031238556\n",
      "Step 383/1000, Loss: 0.26299697160720825, Validation Loss: 0.3134463131427765\n",
      "Step 384/1000, Loss: 0.2624828517436981, Validation Loss: 0.31283819675445557\n",
      "Step 385/1000, Loss: 0.2619703412055969, Validation Loss: 0.31223249435424805\n",
      "Step 386/1000, Loss: 0.2614593803882599, Validation Loss: 0.31162911653518677\n",
      "Step 387/1000, Loss: 0.26094990968704224, Validation Loss: 0.31102821230888367\n",
      "Step 388/1000, Loss: 0.2604420781135559, Validation Loss: 0.31042957305908203\n",
      "Step 389/1000, Loss: 0.25993573665618896, Validation Loss: 0.3098333775997162\n",
      "Step 390/1000, Loss: 0.2594309151172638, Validation Loss: 0.30923953652381897\n",
      "Step 391/1000, Loss: 0.25892767310142517, Validation Loss: 0.308648020029068\n",
      "Step 392/1000, Loss: 0.25842586159706116, Validation Loss: 0.30805879831314087\n",
      "Step 393/1000, Loss: 0.2579255998134613, Validation Loss: 0.30747199058532715\n",
      "Step 394/1000, Loss: 0.25742682814598083, Validation Loss: 0.3068874478340149\n",
      "Step 395/1000, Loss: 0.25692957639694214, Validation Loss: 0.3063051998615265\n",
      "Step 396/1000, Loss: 0.25643375515937805, Validation Loss: 0.3057252764701843\n",
      "Step 397/1000, Loss: 0.25593942403793335, Validation Loss: 0.30514758825302124\n",
      "Step 398/1000, Loss: 0.25544658303260803, Validation Loss: 0.3045722544193268\n",
      "Step 399/1000, Loss: 0.2549552023410797, Validation Loss: 0.303999125957489\n",
      "Step 400/1000, Loss: 0.2544652819633484, Validation Loss: 0.3034282922744751\n",
      "Step 401/1000, Loss: 0.25397682189941406, Validation Loss: 0.30285966396331787\n",
      "Step 402/1000, Loss: 0.25348979234695435, Validation Loss: 0.30257636308670044\n",
      "Step 403/1000, Loss: 0.25324687361717224, Validation Loss: 0.30229395627975464\n",
      "Step 404/1000, Loss: 0.2530045807361603, Validation Loss: 0.30201229453086853\n",
      "Step 405/1000, Loss: 0.2527627646923065, Validation Loss: 0.30173152685165405\n",
      "Step 406/1000, Loss: 0.25252145528793335, Validation Loss: 0.30145150423049927\n",
      "Step 407/1000, Loss: 0.25228068232536316, Validation Loss: 0.30117231607437134\n",
      "Step 408/1000, Loss: 0.25204038619995117, Validation Loss: 0.3008938431739807\n",
      "Step 409/1000, Loss: 0.2518005967140198, Validation Loss: 0.30061614513397217\n",
      "Step 410/1000, Loss: 0.2515611946582794, Validation Loss: 0.3003391623497009\n",
      "Step 411/1000, Loss: 0.25132235884666443, Validation Loss: 0.3000629246234894\n",
      "Step 412/1000, Loss: 0.2510839104652405, Validation Loss: 0.29978734254837036\n",
      "Step 413/1000, Loss: 0.25084593892097473, Validation Loss: 0.29951247572898865\n",
      "Step 414/1000, Loss: 0.2506083548069, Validation Loss: 0.29923829436302185\n",
      "Step 415/1000, Loss: 0.2503712773323059, Validation Loss: 0.2989647686481476\n",
      "Step 416/1000, Loss: 0.25013458728790283, Validation Loss: 0.29869192838668823\n",
      "Step 417/1000, Loss: 0.24989831447601318, Validation Loss: 0.298419713973999\n",
      "Step 418/1000, Loss: 0.24966248869895935, Validation Loss: 0.29814809560775757\n",
      "Step 419/1000, Loss: 0.24942699074745178, Validation Loss: 0.2978772222995758\n",
      "Step 420/1000, Loss: 0.2491919845342636, Validation Loss: 0.2976069152355194\n",
      "Step 421/1000, Loss: 0.2489573210477829, Validation Loss: 0.29733720421791077\n",
      "Step 422/1000, Loss: 0.2487230747938156, Validation Loss: 0.29706814885139465\n",
      "Step 423/1000, Loss: 0.24848923087120056, Validation Loss: 0.2967996597290039\n",
      "Step 424/1000, Loss: 0.24825577437877655, Validation Loss: 0.2965317666530609\n",
      "Step 425/1000, Loss: 0.24802272021770477, Validation Loss: 0.29626449942588806\n",
      "Step 426/1000, Loss: 0.24779000878334045, Validation Loss: 0.2959977984428406\n",
      "Step 427/1000, Loss: 0.24755774438381195, Validation Loss: 0.29573163390159607\n",
      "Step 428/1000, Loss: 0.2473258078098297, Validation Loss: 0.29546600580215454\n",
      "Step 429/1000, Loss: 0.2470942586660385, Validation Loss: 0.29520100355148315\n",
      "Step 430/1000, Loss: 0.24686306715011597, Validation Loss: 0.29493656754493713\n",
      "Step 431/1000, Loss: 0.24663229286670685, Validation Loss: 0.2946726083755493\n",
      "Step 432/1000, Loss: 0.246401846408844, Validation Loss: 0.2944093346595764\n",
      "Step 433/1000, Loss: 0.24617180228233337, Validation Loss: 0.29414650797843933\n",
      "Step 434/1000, Loss: 0.24594208598136902, Validation Loss: 0.2938842475414276\n",
      "Step 435/1000, Loss: 0.2457127720117569, Validation Loss: 0.29362252354621887\n",
      "Step 436/1000, Loss: 0.24548380076885223, Validation Loss: 0.2933613061904907\n",
      "Step 437/1000, Loss: 0.24525520205497742, Validation Loss: 0.29310059547424316\n",
      "Step 438/1000, Loss: 0.24502694606781006, Validation Loss: 0.2928404211997986\n",
      "Step 439/1000, Loss: 0.24479906260967255, Validation Loss: 0.292580783367157\n",
      "Step 440/1000, Loss: 0.2445715218782425, Validation Loss: 0.29232171177864075\n",
      "Step 441/1000, Loss: 0.2443443238735199, Validation Loss: 0.2920631468296051\n",
      "Step 442/1000, Loss: 0.24411749839782715, Validation Loss: 0.2918050289154053\n",
      "Step 443/1000, Loss: 0.24389101564884186, Validation Loss: 0.2915474474430084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 444/1000, Loss: 0.24366487562656403, Validation Loss: 0.29129037261009216\n",
      "Step 445/1000, Loss: 0.24343906342983246, Validation Loss: 0.29103389382362366\n",
      "Step 446/1000, Loss: 0.24321365356445312, Validation Loss: 0.29077786207199097\n",
      "Step 447/1000, Loss: 0.24298857152462006, Validation Loss: 0.29052236676216125\n",
      "Step 448/1000, Loss: 0.24276380240917206, Validation Loss: 0.29026737809181213\n",
      "Step 449/1000, Loss: 0.2425394058227539, Validation Loss: 0.29001280665397644\n",
      "Step 450/1000, Loss: 0.2423153519630432, Validation Loss: 0.2897588610649109\n",
      "Step 451/1000, Loss: 0.24209162592887878, Validation Loss: 0.28950536251068115\n",
      "Step 452/1000, Loss: 0.24186822772026062, Validation Loss: 0.289252370595932\n",
      "Step 453/1000, Loss: 0.2416451871395111, Validation Loss: 0.28899988532066345\n",
      "Step 454/1000, Loss: 0.24142244458198547, Validation Loss: 0.2887479066848755\n",
      "Step 455/1000, Loss: 0.24120007455348969, Validation Loss: 0.2884964048862457\n",
      "Step 456/1000, Loss: 0.24097801744937897, Validation Loss: 0.28824540972709656\n",
      "Step 457/1000, Loss: 0.2407563328742981, Validation Loss: 0.2879948914051056\n",
      "Step 458/1000, Loss: 0.2405349165201187, Validation Loss: 0.2877449691295624\n",
      "Step 459/1000, Loss: 0.24031390249729156, Validation Loss: 0.287495493888855\n",
      "Step 460/1000, Loss: 0.2400931864976883, Validation Loss: 0.2872465252876282\n",
      "Step 461/1000, Loss: 0.23987281322479248, Validation Loss: 0.2869980037212372\n",
      "Step 462/1000, Loss: 0.23965273797512054, Validation Loss: 0.286749929189682\n",
      "Step 463/1000, Loss: 0.23943297564983368, Validation Loss: 0.286502480506897\n",
      "Step 464/1000, Loss: 0.23921363055706024, Validation Loss: 0.28625547885894775\n",
      "Step 465/1000, Loss: 0.2389945387840271, Validation Loss: 0.2860089838504791\n",
      "Step 466/1000, Loss: 0.23877577483654022, Validation Loss: 0.2857629358768463\n",
      "Step 467/1000, Loss: 0.23855732381343842, Validation Loss: 0.2855173945426941\n",
      "Step 468/1000, Loss: 0.23833921551704407, Validation Loss: 0.2852723300457001\n",
      "Step 469/1000, Loss: 0.23812143504619598, Validation Loss: 0.28502777218818665\n",
      "Step 470/1000, Loss: 0.23790395259857178, Validation Loss: 0.28478366136550903\n",
      "Step 471/1000, Loss: 0.23768678307533264, Validation Loss: 0.2845400869846344\n",
      "Step 472/1000, Loss: 0.23746994137763977, Validation Loss: 0.2842969596385956\n",
      "Step 473/1000, Loss: 0.23725344240665436, Validation Loss: 0.28405433893203735\n",
      "Step 474/1000, Loss: 0.23703722655773163, Validation Loss: 0.28381216526031494\n",
      "Step 475/1000, Loss: 0.23682135343551636, Validation Loss: 0.2835704982280731\n",
      "Step 476/1000, Loss: 0.23660574853420258, Validation Loss: 0.2833292484283447\n",
      "Step 477/1000, Loss: 0.23639045655727386, Validation Loss: 0.2830885350704193\n",
      "Step 478/1000, Loss: 0.2361755222082138, Validation Loss: 0.2828482687473297\n",
      "Step 479/1000, Loss: 0.23596088588237762, Validation Loss: 0.2826085686683655\n",
      "Step 480/1000, Loss: 0.2357465773820877, Validation Loss: 0.2823692262172699\n",
      "Step 481/1000, Loss: 0.23553256690502167, Validation Loss: 0.2821303904056549\n",
      "Step 482/1000, Loss: 0.2353188395500183, Validation Loss: 0.2818920314311981\n",
      "Step 483/1000, Loss: 0.23510541021823883, Validation Loss: 0.2816541790962219\n",
      "Step 484/1000, Loss: 0.2348923236131668, Validation Loss: 0.28141671419143677\n",
      "Step 485/1000, Loss: 0.23467952013015747, Validation Loss: 0.281179815530777\n",
      "Step 486/1000, Loss: 0.2344670593738556, Validation Loss: 0.2809433043003082\n",
      "Step 487/1000, Loss: 0.234254851937294, Validation Loss: 0.2807072699069977\n",
      "Step 488/1000, Loss: 0.23404298722743988, Validation Loss: 0.28047171235084534\n",
      "Step 489/1000, Loss: 0.23383143544197083, Validation Loss: 0.2802366316318512\n",
      "Step 490/1000, Loss: 0.23362012207508087, Validation Loss: 0.28000202775001526\n",
      "Step 491/1000, Loss: 0.23340918123722076, Validation Loss: 0.27976787090301514\n",
      "Step 492/1000, Loss: 0.23319852352142334, Validation Loss: 0.27953416109085083\n",
      "Step 493/1000, Loss: 0.2329881340265274, Validation Loss: 0.2793009281158447\n",
      "Step 494/1000, Loss: 0.23277810215950012, Validation Loss: 0.27906814217567444\n",
      "Step 495/1000, Loss: 0.23256827890872955, Validation Loss: 0.2788357436656952\n",
      "Step 496/1000, Loss: 0.23235882818698883, Validation Loss: 0.2786038815975189\n",
      "Step 497/1000, Loss: 0.2321496605873108, Validation Loss: 0.2783724069595337\n",
      "Step 498/1000, Loss: 0.23194073140621185, Validation Loss: 0.27814143896102905\n",
      "Step 499/1000, Loss: 0.23173214495182037, Validation Loss: 0.27791088819503784\n",
      "Step 500/1000, Loss: 0.23152385652065277, Validation Loss: 0.27768081426620483\n",
      "Step 501/1000, Loss: 0.23131583631038666, Validation Loss: 0.27745121717453003\n",
      "Step 502/1000, Loss: 0.2311081439256668, Validation Loss: 0.27733656764030457\n",
      "Step 503/1000, Loss: 0.23100441694259644, Validation Loss: 0.2772221267223358\n",
      "Step 504/1000, Loss: 0.23090077936649323, Validation Loss: 0.2771078050136566\n",
      "Step 505/1000, Loss: 0.23079730570316315, Validation Loss: 0.27699360251426697\n",
      "Step 506/1000, Loss: 0.23069387674331665, Validation Loss: 0.27687960863113403\n",
      "Step 507/1000, Loss: 0.2305905520915985, Validation Loss: 0.27676576375961304\n",
      "Step 508/1000, Loss: 0.23048731684684753, Validation Loss: 0.2766520380973816\n",
      "Step 509/1000, Loss: 0.2303842157125473, Validation Loss: 0.2765384316444397\n",
      "Step 510/1000, Loss: 0.23028120398521423, Validation Loss: 0.27642500400543213\n",
      "Step 511/1000, Loss: 0.23017825186252594, Validation Loss: 0.2763116955757141\n",
      "Step 512/1000, Loss: 0.2300753891468048, Validation Loss: 0.27619850635528564\n",
      "Step 513/1000, Loss: 0.22997263073921204, Validation Loss: 0.2760855555534363\n",
      "Step 514/1000, Loss: 0.2298699915409088, Validation Loss: 0.2759726941585541\n",
      "Step 515/1000, Loss: 0.22976742684841156, Validation Loss: 0.2758599519729614\n",
      "Step 516/1000, Loss: 0.2296649068593979, Validation Loss: 0.27574729919433594\n",
      "Step 517/1000, Loss: 0.22956249117851257, Validation Loss: 0.2756347954273224\n",
      "Step 518/1000, Loss: 0.22946013510227203, Validation Loss: 0.27552247047424316\n",
      "Step 519/1000, Loss: 0.22935788333415985, Validation Loss: 0.2754102945327759\n",
      "Step 520/1000, Loss: 0.22925572097301483, Validation Loss: 0.275298148393631\n",
      "Step 521/1000, Loss: 0.2291536182165146, Validation Loss: 0.2751862406730652\n",
      "Step 522/1000, Loss: 0.2290516197681427, Validation Loss: 0.2750743627548218\n",
      "Step 523/1000, Loss: 0.22894969582557678, Validation Loss: 0.2749626636505127\n",
      "Step 524/1000, Loss: 0.22884783148765564, Validation Loss: 0.2748510539531708\n",
      "Step 525/1000, Loss: 0.22874604165554047, Validation Loss: 0.2747395932674408\n",
      "Step 526/1000, Loss: 0.22864437103271484, Validation Loss: 0.27462825179100037\n",
      "Step 527/1000, Loss: 0.2285427302122116, Validation Loss: 0.2745169997215271\n",
      "Step 528/1000, Loss: 0.22844119369983673, Validation Loss: 0.2744058668613434\n",
      "Step 529/1000, Loss: 0.22833971679210663, Validation Loss: 0.2742948532104492\n",
      "Step 530/1000, Loss: 0.22823837399482727, Validation Loss: 0.2741839587688446\n",
      "Step 531/1000, Loss: 0.2281370311975479, Validation Loss: 0.27407318353652954\n",
      "Step 532/1000, Loss: 0.22803577780723572, Validation Loss: 0.27396252751350403\n",
      "Step 533/1000, Loss: 0.2279345840215683, Validation Loss: 0.27385199069976807\n",
      "Step 534/1000, Loss: 0.22783350944519043, Validation Loss: 0.27374154329299927\n",
      "Step 535/1000, Loss: 0.22773247957229614, Validation Loss: 0.27363118529319763\n",
      "Step 536/1000, Loss: 0.22763150930404663, Validation Loss: 0.27352094650268555\n",
      "Step 537/1000, Loss: 0.22753068804740906, Validation Loss: 0.2734109163284302\n",
      "Step 538/1000, Loss: 0.22742986679077148, Validation Loss: 0.2733009159564972\n",
      "Step 539/1000, Loss: 0.2273291051387787, Validation Loss: 0.27319106459617615\n",
      "Step 540/1000, Loss: 0.22722849249839783, Validation Loss: 0.27308133244514465\n",
      "Step 541/1000, Loss: 0.22712790966033936, Validation Loss: 0.2729716897010803\n",
      "Step 542/1000, Loss: 0.22702738642692566, Validation Loss: 0.2728620767593384\n",
      "Step 543/1000, Loss: 0.22692695260047913, Validation Loss: 0.272752583026886\n",
      "Step 544/1000, Loss: 0.22682657837867737, Validation Loss: 0.27264323830604553\n",
      "Step 545/1000, Loss: 0.22672627866268158, Validation Loss: 0.27253398299217224\n",
      "Step 546/1000, Loss: 0.22662605345249176, Validation Loss: 0.2724248766899109\n",
      "Step 547/1000, Loss: 0.22652588784694672, Validation Loss: 0.2723158299922943\n",
      "Step 548/1000, Loss: 0.22642585635185242, Validation Loss: 0.27220696210861206\n",
      "Step 549/1000, Loss: 0.22632580995559692, Validation Loss: 0.2720981538295746\n",
      "Step 550/1000, Loss: 0.22622588276863098, Validation Loss: 0.2719894349575043\n",
      "Step 551/1000, Loss: 0.22612600028514862, Validation Loss: 0.2718808054924011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 552/1000, Loss: 0.22602619230747223, Validation Loss: 0.27177220582962036\n",
      "Step 553/1000, Loss: 0.225926473736763, Validation Loss: 0.2716638147830963\n",
      "Step 554/1000, Loss: 0.22582677006721497, Validation Loss: 0.27155551314353943\n",
      "Step 555/1000, Loss: 0.22572720050811768, Validation Loss: 0.2714473009109497\n",
      "Step 556/1000, Loss: 0.22562766075134277, Validation Loss: 0.2713392376899719\n",
      "Step 557/1000, Loss: 0.22552824020385742, Validation Loss: 0.27123120427131653\n",
      "Step 558/1000, Loss: 0.22542884945869446, Validation Loss: 0.27112334966659546\n",
      "Step 559/1000, Loss: 0.22532951831817627, Validation Loss: 0.2710154950618744\n",
      "Step 560/1000, Loss: 0.22523029148578644, Validation Loss: 0.27090781927108765\n",
      "Step 561/1000, Loss: 0.22513112425804138, Validation Loss: 0.27080029249191284\n",
      "Step 562/1000, Loss: 0.2250320315361023, Validation Loss: 0.27069273591041565\n",
      "Step 563/1000, Loss: 0.2249329835176468, Validation Loss: 0.2705853581428528\n",
      "Step 564/1000, Loss: 0.22483401000499725, Validation Loss: 0.27047809958457947\n",
      "Step 565/1000, Loss: 0.22473512589931488, Validation Loss: 0.27037087082862854\n",
      "Step 566/1000, Loss: 0.2246362864971161, Validation Loss: 0.27026376128196716\n",
      "Step 567/1000, Loss: 0.22453752160072327, Validation Loss: 0.27015677094459534\n",
      "Step 568/1000, Loss: 0.2244388312101364, Validation Loss: 0.27004989981651306\n",
      "Step 569/1000, Loss: 0.22434023022651672, Validation Loss: 0.26994314789772034\n",
      "Step 570/1000, Loss: 0.22424162924289703, Validation Loss: 0.2698364555835724\n",
      "Step 571/1000, Loss: 0.2241431623697281, Validation Loss: 0.2697299122810364\n",
      "Step 572/1000, Loss: 0.22404474020004272, Validation Loss: 0.26962336897850037\n",
      "Step 573/1000, Loss: 0.22394637763500214, Validation Loss: 0.2695170044898987\n",
      "Step 574/1000, Loss: 0.22384805977344513, Validation Loss: 0.2694106996059418\n",
      "Step 575/1000, Loss: 0.22374987602233887, Validation Loss: 0.2693045437335968\n",
      "Step 576/1000, Loss: 0.2236517369747162, Validation Loss: 0.2691984176635742\n",
      "Step 577/1000, Loss: 0.2235536277294159, Validation Loss: 0.2690924108028412\n",
      "Step 578/1000, Loss: 0.22345562279224396, Validation Loss: 0.26898661255836487\n",
      "Step 579/1000, Loss: 0.2233576625585556, Validation Loss: 0.26888078451156616\n",
      "Step 580/1000, Loss: 0.22325973212718964, Validation Loss: 0.2687750458717346\n",
      "Step 581/1000, Loss: 0.2231619656085968, Validation Loss: 0.2686694860458374\n",
      "Step 582/1000, Loss: 0.22306418418884277, Validation Loss: 0.26856398582458496\n",
      "Step 583/1000, Loss: 0.2229665219783783, Validation Loss: 0.2684585452079773\n",
      "Step 584/1000, Loss: 0.2228688895702362, Validation Loss: 0.26835325360298157\n",
      "Step 585/1000, Loss: 0.2227713167667389, Validation Loss: 0.2682480812072754\n",
      "Step 586/1000, Loss: 0.22267387807369232, Validation Loss: 0.268142968416214\n",
      "Step 587/1000, Loss: 0.22257640957832336, Validation Loss: 0.2680380344390869\n",
      "Step 588/1000, Loss: 0.22247906029224396, Validation Loss: 0.26793310046195984\n",
      "Step 589/1000, Loss: 0.22238177061080933, Validation Loss: 0.2678282558917999\n",
      "Step 590/1000, Loss: 0.22228452563285828, Validation Loss: 0.26772356033325195\n",
      "Step 591/1000, Loss: 0.22218739986419678, Validation Loss: 0.26761892437934875\n",
      "Step 592/1000, Loss: 0.22209030389785767, Validation Loss: 0.2675143778324127\n",
      "Step 593/1000, Loss: 0.22199323773384094, Validation Loss: 0.26740995049476624\n",
      "Step 594/1000, Loss: 0.22189629077911377, Validation Loss: 0.2673056423664093\n",
      "Step 595/1000, Loss: 0.22179938852787018, Validation Loss: 0.26720142364501953\n",
      "Step 596/1000, Loss: 0.22170253098011017, Validation Loss: 0.2670973241329193\n",
      "Step 597/1000, Loss: 0.22160577774047852, Validation Loss: 0.2669932544231415\n",
      "Step 598/1000, Loss: 0.22150908410549164, Validation Loss: 0.2668893039226532\n",
      "Step 599/1000, Loss: 0.22141247987747192, Validation Loss: 0.2667854428291321\n",
      "Step 600/1000, Loss: 0.2213158756494522, Validation Loss: 0.2666817009449005\n",
      "Step 601/1000, Loss: 0.22121936082839966, Validation Loss: 0.2665780484676361\n",
      "Step 602/1000, Loss: 0.22112290561199188, Validation Loss: 0.2665262818336487\n",
      "Step 603/1000, Loss: 0.22107470035552979, Validation Loss: 0.26647451519966125\n",
      "Step 604/1000, Loss: 0.22102653980255127, Validation Loss: 0.26642274856567383\n",
      "Step 605/1000, Loss: 0.22097840905189514, Validation Loss: 0.26637110114097595\n",
      "Step 606/1000, Loss: 0.220930278301239, Validation Loss: 0.2663194239139557\n",
      "Step 607/1000, Loss: 0.22088216245174408, Validation Loss: 0.2662677764892578\n",
      "Step 608/1000, Loss: 0.22083407640457153, Validation Loss: 0.2662162482738495\n",
      "Step 609/1000, Loss: 0.22078603506088257, Validation Loss: 0.2661646604537964\n",
      "Step 610/1000, Loss: 0.22073794901371002, Validation Loss: 0.26611313223838806\n",
      "Step 611/1000, Loss: 0.22068993747234344, Validation Loss: 0.2660616338253021\n",
      "Step 612/1000, Loss: 0.22064195573329926, Validation Loss: 0.2660101354122162\n",
      "Step 613/1000, Loss: 0.22059392929077148, Validation Loss: 0.2659587562084198\n",
      "Step 614/1000, Loss: 0.22054600715637207, Validation Loss: 0.26590731739997864\n",
      "Step 615/1000, Loss: 0.22049804031848907, Validation Loss: 0.26585590839385986\n",
      "Step 616/1000, Loss: 0.22045008838176727, Validation Loss: 0.2658045291900635\n",
      "Step 617/1000, Loss: 0.22040215134620667, Validation Loss: 0.26575326919555664\n",
      "Step 618/1000, Loss: 0.22035427391529083, Validation Loss: 0.26570194959640503\n",
      "Step 619/1000, Loss: 0.220306396484375, Validation Loss: 0.2656506597995758\n",
      "Step 620/1000, Loss: 0.22025850415229797, Validation Loss: 0.2655993103981018\n",
      "Step 621/1000, Loss: 0.22021065652370453, Validation Loss: 0.26554811000823975\n",
      "Step 622/1000, Loss: 0.22016280889511108, Validation Loss: 0.2654969096183777\n",
      "Step 623/1000, Loss: 0.22011499106884003, Validation Loss: 0.2654457688331604\n",
      "Step 624/1000, Loss: 0.22006720304489136, Validation Loss: 0.2653946280479431\n",
      "Step 625/1000, Loss: 0.22001942992210388, Validation Loss: 0.26534348726272583\n",
      "Step 626/1000, Loss: 0.21997162699699402, Validation Loss: 0.26529234647750854\n",
      "Step 627/1000, Loss: 0.21992391347885132, Validation Loss: 0.26524123549461365\n",
      "Step 628/1000, Loss: 0.21987617015838623, Validation Loss: 0.2651902139186859\n",
      "Step 629/1000, Loss: 0.21982845664024353, Validation Loss: 0.2651391923427582\n",
      "Step 630/1000, Loss: 0.21978075802326202, Validation Loss: 0.26508820056915283\n",
      "Step 631/1000, Loss: 0.21973305940628052, Validation Loss: 0.2650371789932251\n",
      "Step 632/1000, Loss: 0.2196853756904602, Validation Loss: 0.2649862468242645\n",
      "Step 633/1000, Loss: 0.21963773667812347, Validation Loss: 0.26493531465530396\n",
      "Step 634/1000, Loss: 0.21959011256694794, Validation Loss: 0.26488444209098816\n",
      "Step 635/1000, Loss: 0.2195424884557724, Validation Loss: 0.26483359932899475\n",
      "Step 636/1000, Loss: 0.21949484944343567, Validation Loss: 0.26478275656700134\n",
      "Step 637/1000, Loss: 0.2194472700357437, Validation Loss: 0.26473191380500793\n",
      "Step 638/1000, Loss: 0.21939970552921295, Validation Loss: 0.2646810710430145\n",
      "Step 639/1000, Loss: 0.21935215592384338, Validation Loss: 0.2646302878856659\n",
      "Step 640/1000, Loss: 0.21930459141731262, Validation Loss: 0.26457953453063965\n",
      "Step 641/1000, Loss: 0.21925702691078186, Validation Loss: 0.2645287811756134\n",
      "Step 642/1000, Loss: 0.21920952200889587, Validation Loss: 0.26447805762290955\n",
      "Step 643/1000, Loss: 0.21916204690933228, Validation Loss: 0.2644273340702057\n",
      "Step 644/1000, Loss: 0.21911455690860748, Validation Loss: 0.264376699924469\n",
      "Step 645/1000, Loss: 0.21906708180904388, Validation Loss: 0.2643260359764099\n",
      "Step 646/1000, Loss: 0.2190195918083191, Validation Loss: 0.264275461435318\n",
      "Step 647/1000, Loss: 0.21897216141223907, Validation Loss: 0.2642247676849365\n",
      "Step 648/1000, Loss: 0.21892468631267548, Validation Loss: 0.2641741931438446\n",
      "Step 649/1000, Loss: 0.21887728571891785, Validation Loss: 0.2641236484050751\n",
      "Step 650/1000, Loss: 0.21882988512516022, Validation Loss: 0.26407313346862793\n",
      "Step 651/1000, Loss: 0.21878249943256378, Validation Loss: 0.2640226483345032\n",
      "Step 652/1000, Loss: 0.21873514354228973, Validation Loss: 0.2639721632003784\n",
      "Step 653/1000, Loss: 0.21868778765201569, Validation Loss: 0.2639216184616089\n",
      "Step 654/1000, Loss: 0.21864041686058044, Validation Loss: 0.2638711929321289\n",
      "Step 655/1000, Loss: 0.21859309077262878, Validation Loss: 0.2638207674026489\n",
      "Step 656/1000, Loss: 0.2185457944869995, Validation Loss: 0.26377037167549133\n",
      "Step 657/1000, Loss: 0.21849851310253143, Validation Loss: 0.26371994614601135\n",
      "Step 658/1000, Loss: 0.21845124661922455, Validation Loss: 0.26366958022117615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 659/1000, Loss: 0.21840399503707886, Validation Loss: 0.2636192739009857\n",
      "Step 660/1000, Loss: 0.21835672855377197, Validation Loss: 0.2635689675807953\n",
      "Step 661/1000, Loss: 0.21830949187278748, Validation Loss: 0.26351866126060486\n",
      "Step 662/1000, Loss: 0.21826228499412537, Validation Loss: 0.2634683847427368\n",
      "Step 663/1000, Loss: 0.21821506321430206, Validation Loss: 0.26341813802719116\n",
      "Step 664/1000, Loss: 0.21816790103912354, Validation Loss: 0.2633679509162903\n",
      "Step 665/1000, Loss: 0.2181207686662674, Validation Loss: 0.263317734003067\n",
      "Step 666/1000, Loss: 0.21807359158992767, Validation Loss: 0.2632676362991333\n",
      "Step 667/1000, Loss: 0.21802644431591034, Validation Loss: 0.2632174789905548\n",
      "Step 668/1000, Loss: 0.2179793119430542, Validation Loss: 0.2631673514842987\n",
      "Step 669/1000, Loss: 0.21793219447135925, Validation Loss: 0.2631171643733978\n",
      "Step 670/1000, Loss: 0.2178850620985031, Validation Loss: 0.2630670666694641\n",
      "Step 671/1000, Loss: 0.21783795952796936, Validation Loss: 0.2630169689655304\n",
      "Step 672/1000, Loss: 0.2177909016609192, Validation Loss: 0.2629668712615967\n",
      "Step 673/1000, Loss: 0.21774382889270782, Validation Loss: 0.26291680335998535\n",
      "Step 674/1000, Loss: 0.21769681572914124, Validation Loss: 0.262866735458374\n",
      "Step 675/1000, Loss: 0.21764975786209106, Validation Loss: 0.26281675696372986\n",
      "Step 676/1000, Loss: 0.21760275959968567, Validation Loss: 0.2627667784690857\n",
      "Step 677/1000, Loss: 0.21755576133728027, Validation Loss: 0.26271679997444153\n",
      "Step 678/1000, Loss: 0.21750874817371368, Validation Loss: 0.26266688108444214\n",
      "Step 679/1000, Loss: 0.21746176481246948, Validation Loss: 0.26261696219444275\n",
      "Step 680/1000, Loss: 0.21741478145122528, Validation Loss: 0.26256707310676575\n",
      "Step 681/1000, Loss: 0.21736784279346466, Validation Loss: 0.26251718401908875\n",
      "Step 682/1000, Loss: 0.21732090413570404, Validation Loss: 0.2624673545360565\n",
      "Step 683/1000, Loss: 0.2172739952802658, Validation Loss: 0.2624174654483795\n",
      "Step 684/1000, Loss: 0.21722707152366638, Validation Loss: 0.2623676657676697\n",
      "Step 685/1000, Loss: 0.21718016266822815, Validation Loss: 0.26231783628463745\n",
      "Step 686/1000, Loss: 0.2171332687139511, Validation Loss: 0.26226806640625\n",
      "Step 687/1000, Loss: 0.21708638966083527, Validation Loss: 0.26221829652786255\n",
      "Step 688/1000, Loss: 0.2170395404100418, Validation Loss: 0.2621685266494751\n",
      "Step 689/1000, Loss: 0.21699273586273193, Validation Loss: 0.2621188461780548\n",
      "Step 690/1000, Loss: 0.21694590151309967, Validation Loss: 0.2620691657066345\n",
      "Step 691/1000, Loss: 0.21689915657043457, Validation Loss: 0.2620195150375366\n",
      "Step 692/1000, Loss: 0.2168523520231247, Validation Loss: 0.2619698643684387\n",
      "Step 693/1000, Loss: 0.216805562376976, Validation Loss: 0.2619202733039856\n",
      "Step 694/1000, Loss: 0.2167588174343109, Validation Loss: 0.2618706226348877\n",
      "Step 695/1000, Loss: 0.21671204268932343, Validation Loss: 0.26182109117507935\n",
      "Step 696/1000, Loss: 0.2166653424501419, Validation Loss: 0.2617715895175934\n",
      "Step 697/1000, Loss: 0.2166185826063156, Validation Loss: 0.26172202825546265\n",
      "Step 698/1000, Loss: 0.21657192707061768, Validation Loss: 0.26167258620262146\n",
      "Step 699/1000, Loss: 0.21652524173259735, Validation Loss: 0.2616230845451355\n",
      "Step 700/1000, Loss: 0.21647855639457703, Validation Loss: 0.2615736126899719\n",
      "Step 701/1000, Loss: 0.21643194556236267, Validation Loss: 0.26152411103248596\n",
      "Step 702/1000, Loss: 0.21638526022434235, Validation Loss: 0.26149943470954895\n",
      "Step 703/1000, Loss: 0.21636195480823517, Validation Loss: 0.26147472858428955\n",
      "Step 704/1000, Loss: 0.2163386195898056, Validation Loss: 0.26145005226135254\n",
      "Step 705/1000, Loss: 0.21631531417369843, Validation Loss: 0.2614253759384155\n",
      "Step 706/1000, Loss: 0.21629203855991364, Validation Loss: 0.26140066981315613\n",
      "Step 707/1000, Loss: 0.21626873314380646, Validation Loss: 0.26137596368789673\n",
      "Step 708/1000, Loss: 0.21624542772769928, Validation Loss: 0.26135125756263733\n",
      "Step 709/1000, Loss: 0.2162221223115921, Validation Loss: 0.2613266110420227\n",
      "Step 710/1000, Loss: 0.2161988615989685, Validation Loss: 0.2613019347190857\n",
      "Step 711/1000, Loss: 0.21617552638053894, Validation Loss: 0.2612772583961487\n",
      "Step 712/1000, Loss: 0.21615226566791534, Validation Loss: 0.26125261187553406\n",
      "Step 713/1000, Loss: 0.21612900495529175, Validation Loss: 0.26122793555259705\n",
      "Step 714/1000, Loss: 0.21610571444034576, Validation Loss: 0.2612032890319824\n",
      "Step 715/1000, Loss: 0.21608248353004456, Validation Loss: 0.2611786425113678\n",
      "Step 716/1000, Loss: 0.21605919301509857, Validation Loss: 0.26115402579307556\n",
      "Step 717/1000, Loss: 0.21603594720363617, Validation Loss: 0.2611294090747833\n",
      "Step 718/1000, Loss: 0.21601268649101257, Validation Loss: 0.2611047923564911\n",
      "Step 719/1000, Loss: 0.21598942577838898, Validation Loss: 0.26108020544052124\n",
      "Step 720/1000, Loss: 0.21596617996692657, Validation Loss: 0.2610556483268738\n",
      "Step 721/1000, Loss: 0.21594294905662537, Validation Loss: 0.26103103160858154\n",
      "Step 722/1000, Loss: 0.21591970324516296, Validation Loss: 0.2610063850879669\n",
      "Step 723/1000, Loss: 0.21589645743370056, Validation Loss: 0.26098179817199707\n",
      "Step 724/1000, Loss: 0.21587324142456055, Validation Loss: 0.2609572112560272\n",
      "Step 725/1000, Loss: 0.21585004031658173, Validation Loss: 0.2609326243400574\n",
      "Step 726/1000, Loss: 0.21582680940628052, Validation Loss: 0.26090800762176514\n",
      "Step 727/1000, Loss: 0.2158035784959793, Validation Loss: 0.26088348031044006\n",
      "Step 728/1000, Loss: 0.21578039228916168, Validation Loss: 0.2608588635921478\n",
      "Step 729/1000, Loss: 0.21575716137886047, Validation Loss: 0.260834276676178\n",
      "Step 730/1000, Loss: 0.21573393046855927, Validation Loss: 0.2608097493648529\n",
      "Step 731/1000, Loss: 0.21571074426174164, Validation Loss: 0.26078516244888306\n",
      "Step 732/1000, Loss: 0.215687558054924, Validation Loss: 0.260760635137558\n",
      "Step 733/1000, Loss: 0.2156643271446228, Validation Loss: 0.26073604822158813\n",
      "Step 734/1000, Loss: 0.2156411111354828, Validation Loss: 0.26071152091026306\n",
      "Step 735/1000, Loss: 0.21561791002750397, Validation Loss: 0.2606870234012604\n",
      "Step 736/1000, Loss: 0.21559470891952515, Validation Loss: 0.2606625258922577\n",
      "Step 737/1000, Loss: 0.21557149291038513, Validation Loss: 0.2606379985809326\n",
      "Step 738/1000, Loss: 0.2155483067035675, Validation Loss: 0.26061350107192993\n",
      "Step 739/1000, Loss: 0.21552512049674988, Validation Loss: 0.26058894395828247\n",
      "Step 740/1000, Loss: 0.21550188958644867, Validation Loss: 0.2605644166469574\n",
      "Step 741/1000, Loss: 0.21547871828079224, Validation Loss: 0.2605398893356323\n",
      "Step 742/1000, Loss: 0.215455561876297, Validation Loss: 0.26051539182662964\n",
      "Step 743/1000, Loss: 0.21543234586715698, Validation Loss: 0.2604908347129822\n",
      "Step 744/1000, Loss: 0.21540920436382294, Validation Loss: 0.2604663670063019\n",
      "Step 745/1000, Loss: 0.21538598835468292, Validation Loss: 0.2604418992996216\n",
      "Step 746/1000, Loss: 0.2153628170490265, Validation Loss: 0.2604173719882965\n",
      "Step 747/1000, Loss: 0.21533964574337006, Validation Loss: 0.2603929340839386\n",
      "Step 748/1000, Loss: 0.2153165191411972, Validation Loss: 0.2603684365749359\n",
      "Step 749/1000, Loss: 0.21529333293437958, Validation Loss: 0.260343998670578\n",
      "Step 750/1000, Loss: 0.21527016162872314, Validation Loss: 0.2603195607662201\n",
      "Step 751/1000, Loss: 0.2152469903230667, Validation Loss: 0.2602950632572174\n",
      "Step 752/1000, Loss: 0.21522381901741028, Validation Loss: 0.2602706253528595\n",
      "Step 753/1000, Loss: 0.21520067751407623, Validation Loss: 0.2602461278438568\n",
      "Step 754/1000, Loss: 0.2151775360107422, Validation Loss: 0.2602216601371765\n",
      "Step 755/1000, Loss: 0.21515439450740814, Validation Loss: 0.2601972222328186\n",
      "Step 756/1000, Loss: 0.2151312381029129, Validation Loss: 0.2601727843284607\n",
      "Step 757/1000, Loss: 0.21510812640190125, Validation Loss: 0.2601483166217804\n",
      "Step 758/1000, Loss: 0.2150849848985672, Validation Loss: 0.2601238787174225\n",
      "Step 759/1000, Loss: 0.21506182849407196, Validation Loss: 0.2600994110107422\n",
      "Step 760/1000, Loss: 0.21503868699073792, Validation Loss: 0.2600749731063843\n",
      "Step 761/1000, Loss: 0.21501557528972626, Validation Loss: 0.26005056500434875\n",
      "Step 762/1000, Loss: 0.21499241888523102, Validation Loss: 0.26002615690231323\n",
      "Step 763/1000, Loss: 0.21496933698654175, Validation Loss: 0.2600017189979553\n",
      "Step 764/1000, Loss: 0.2149461805820465, Validation Loss: 0.2599772810935974\n",
      "Step 765/1000, Loss: 0.21492305397987366, Validation Loss: 0.2599529027938843\n",
      "Step 766/1000, Loss: 0.214899942278862, Validation Loss: 0.25992852449417114\n",
      "Step 767/1000, Loss: 0.21487683057785034, Validation Loss: 0.25990408658981323\n",
      "Step 768/1000, Loss: 0.21485371887683868, Validation Loss: 0.2598797082901001\n",
      "Step 769/1000, Loss: 0.21483056247234344, Validation Loss: 0.2598553001880646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 770/1000, Loss: 0.21480749547481537, Validation Loss: 0.25983095169067383\n",
      "Step 771/1000, Loss: 0.2147843837738037, Validation Loss: 0.2598065733909607\n",
      "Step 772/1000, Loss: 0.21476125717163086, Validation Loss: 0.25978219509124756\n",
      "Step 773/1000, Loss: 0.2147381752729416, Validation Loss: 0.2597578167915344\n",
      "Step 774/1000, Loss: 0.21471509337425232, Validation Loss: 0.2597334384918213\n",
      "Step 775/1000, Loss: 0.21469199657440186, Validation Loss: 0.25970908999443054\n",
      "Step 776/1000, Loss: 0.2146688997745514, Validation Loss: 0.2596847116947174\n",
      "Step 777/1000, Loss: 0.21464583277702332, Validation Loss: 0.2596603333950043\n",
      "Step 778/1000, Loss: 0.21462272107601166, Validation Loss: 0.2596360146999359\n",
      "Step 779/1000, Loss: 0.21459966897964478, Validation Loss: 0.2596116364002228\n",
      "Step 780/1000, Loss: 0.21457655727863312, Validation Loss: 0.2595873177051544\n",
      "Step 781/1000, Loss: 0.21455349028110504, Validation Loss: 0.2595629096031189\n",
      "Step 782/1000, Loss: 0.21453039348125458, Validation Loss: 0.2595386505126953\n",
      "Step 783/1000, Loss: 0.2145073413848877, Validation Loss: 0.25951430201530457\n",
      "Step 784/1000, Loss: 0.2144842892885208, Validation Loss: 0.25948992371559143\n",
      "Step 785/1000, Loss: 0.21446119248867035, Validation Loss: 0.2594655752182007\n",
      "Step 786/1000, Loss: 0.21443809568881989, Validation Loss: 0.25944119691848755\n",
      "Step 787/1000, Loss: 0.214415043592453, Validation Loss: 0.2594168782234192\n",
      "Step 788/1000, Loss: 0.21439197659492493, Validation Loss: 0.25939247012138367\n",
      "Step 789/1000, Loss: 0.21436892449855804, Validation Loss: 0.2593681812286377\n",
      "Step 790/1000, Loss: 0.21434587240219116, Validation Loss: 0.25934386253356934\n",
      "Step 791/1000, Loss: 0.21432280540466309, Validation Loss: 0.2593194842338562\n",
      "Step 792/1000, Loss: 0.2142997682094574, Validation Loss: 0.2592952251434326\n",
      "Step 793/1000, Loss: 0.21427671611309052, Validation Loss: 0.25927096605300903\n",
      "Step 794/1000, Loss: 0.21425367891788483, Validation Loss: 0.2592466473579407\n",
      "Step 795/1000, Loss: 0.21423064172267914, Validation Loss: 0.2592223286628723\n",
      "Step 796/1000, Loss: 0.21420758962631226, Validation Loss: 0.25919806957244873\n",
      "Step 797/1000, Loss: 0.21418455243110657, Validation Loss: 0.25917378067970276\n",
      "Step 798/1000, Loss: 0.21416153013706207, Validation Loss: 0.2591494917869568\n",
      "Step 799/1000, Loss: 0.21413850784301758, Validation Loss: 0.2591252326965332\n",
      "Step 800/1000, Loss: 0.2141154706478119, Validation Loss: 0.2591009736061096\n",
      "Step 801/1000, Loss: 0.2140924632549286, Validation Loss: 0.25907671451568604\n",
      "Step 802/1000, Loss: 0.2140694260597229, Validation Loss: 0.25906458497047424\n",
      "Step 803/1000, Loss: 0.21405793726444244, Validation Loss: 0.25905242562294006\n",
      "Step 804/1000, Loss: 0.214046448469162, Validation Loss: 0.25904029607772827\n",
      "Step 805/1000, Loss: 0.21403492987155914, Validation Loss: 0.2590281665325165\n",
      "Step 806/1000, Loss: 0.21402345597743988, Validation Loss: 0.25901609659194946\n",
      "Step 807/1000, Loss: 0.21401193737983704, Validation Loss: 0.2590039372444153\n",
      "Step 808/1000, Loss: 0.2140004187822342, Validation Loss: 0.2589917778968811\n",
      "Step 809/1000, Loss: 0.21398892998695374, Validation Loss: 0.2589796781539917\n",
      "Step 810/1000, Loss: 0.2139774113893509, Validation Loss: 0.2589675784111023\n",
      "Step 811/1000, Loss: 0.21396590769290924, Validation Loss: 0.2589554488658905\n",
      "Step 812/1000, Loss: 0.2139544039964676, Validation Loss: 0.2589433491230011\n",
      "Step 813/1000, Loss: 0.21394288539886475, Validation Loss: 0.2589312195777893\n",
      "Step 814/1000, Loss: 0.2139313966035843, Validation Loss: 0.2589191496372223\n",
      "Step 815/1000, Loss: 0.21391989290714264, Validation Loss: 0.2589070498943329\n",
      "Step 816/1000, Loss: 0.2139083743095398, Validation Loss: 0.2588949501514435\n",
      "Step 817/1000, Loss: 0.21389688551425934, Validation Loss: 0.2588828206062317\n",
      "Step 818/1000, Loss: 0.21388541162014008, Validation Loss: 0.2588707208633423\n",
      "Step 819/1000, Loss: 0.21387390792369843, Validation Loss: 0.2588585615158081\n",
      "Step 820/1000, Loss: 0.21386241912841797, Validation Loss: 0.2588464617729187\n",
      "Step 821/1000, Loss: 0.2138509452342987, Validation Loss: 0.2588343620300293\n",
      "Step 822/1000, Loss: 0.21383944153785706, Validation Loss: 0.2588222920894623\n",
      "Step 823/1000, Loss: 0.2138279229402542, Validation Loss: 0.2588101625442505\n",
      "Step 824/1000, Loss: 0.21381643414497375, Validation Loss: 0.25879812240600586\n",
      "Step 825/1000, Loss: 0.2138049453496933, Validation Loss: 0.25878605246543884\n",
      "Step 826/1000, Loss: 0.21379345655441284, Validation Loss: 0.2587739825248718\n",
      "Step 827/1000, Loss: 0.2137819528579712, Validation Loss: 0.25876182317733765\n",
      "Step 828/1000, Loss: 0.21377043426036835, Validation Loss: 0.258749783039093\n",
      "Step 829/1000, Loss: 0.21375896036624908, Validation Loss: 0.258737713098526\n",
      "Step 830/1000, Loss: 0.21374748647212982, Validation Loss: 0.2587256133556366\n",
      "Step 831/1000, Loss: 0.21373598277568817, Validation Loss: 0.2587135434150696\n",
      "Step 832/1000, Loss: 0.21372446417808533, Validation Loss: 0.2587013840675354\n",
      "Step 833/1000, Loss: 0.21371299028396606, Validation Loss: 0.2586892545223236\n",
      "Step 834/1000, Loss: 0.21370148658752441, Validation Loss: 0.2586771249771118\n",
      "Step 835/1000, Loss: 0.21368998289108276, Validation Loss: 0.25866496562957764\n",
      "Step 836/1000, Loss: 0.2136785238981247, Validation Loss: 0.25865286588668823\n",
      "Step 837/1000, Loss: 0.21366702020168304, Validation Loss: 0.25864076614379883\n",
      "Step 838/1000, Loss: 0.2136555016040802, Validation Loss: 0.25862863659858704\n",
      "Step 839/1000, Loss: 0.21364401280879974, Validation Loss: 0.25861653685569763\n",
      "Step 840/1000, Loss: 0.21363255381584167, Validation Loss: 0.25860440731048584\n",
      "Step 841/1000, Loss: 0.21362106502056122, Validation Loss: 0.25859227776527405\n",
      "Step 842/1000, Loss: 0.21360957622528076, Validation Loss: 0.25858017802238464\n",
      "Step 843/1000, Loss: 0.2135980725288391, Validation Loss: 0.25856804847717285\n",
      "Step 844/1000, Loss: 0.21358659863471985, Validation Loss: 0.25855597853660583\n",
      "Step 845/1000, Loss: 0.2135750949382782, Validation Loss: 0.25854387879371643\n",
      "Step 846/1000, Loss: 0.21356360614299774, Validation Loss: 0.258531779050827\n",
      "Step 847/1000, Loss: 0.2135521024465561, Validation Loss: 0.25851961970329285\n",
      "Step 848/1000, Loss: 0.21354062855243683, Validation Loss: 0.25850751996040344\n",
      "Step 849/1000, Loss: 0.21352912485599518, Validation Loss: 0.25849542021751404\n",
      "Step 850/1000, Loss: 0.2135176658630371, Validation Loss: 0.25848332047462463\n",
      "Step 851/1000, Loss: 0.21350617706775665, Validation Loss: 0.25847122073173523\n",
      "Step 852/1000, Loss: 0.2134946882724762, Validation Loss: 0.25845909118652344\n",
      "Step 853/1000, Loss: 0.21348321437835693, Validation Loss: 0.2584470510482788\n",
      "Step 854/1000, Loss: 0.21347174048423767, Validation Loss: 0.25843489170074463\n",
      "Step 855/1000, Loss: 0.2134602665901184, Validation Loss: 0.2584227919578552\n",
      "Step 856/1000, Loss: 0.21344877779483795, Validation Loss: 0.2584106922149658\n",
      "Step 857/1000, Loss: 0.21343731880187988, Validation Loss: 0.25839856266975403\n",
      "Step 858/1000, Loss: 0.21342584490776062, Validation Loss: 0.258386492729187\n",
      "Step 859/1000, Loss: 0.21341434121131897, Validation Loss: 0.2583743929862976\n",
      "Step 860/1000, Loss: 0.2134028971195221, Validation Loss: 0.2583622634410858\n",
      "Step 861/1000, Loss: 0.21339139342308044, Validation Loss: 0.2583501935005188\n",
      "Step 862/1000, Loss: 0.2133798897266388, Validation Loss: 0.2583381235599518\n",
      "Step 863/1000, Loss: 0.21336843073368073, Validation Loss: 0.25832599401474\n",
      "Step 864/1000, Loss: 0.21335694193840027, Validation Loss: 0.258313924074173\n",
      "Step 865/1000, Loss: 0.213345468044281, Validation Loss: 0.2583017945289612\n",
      "Step 866/1000, Loss: 0.21333397924900055, Validation Loss: 0.2582896947860718\n",
      "Step 867/1000, Loss: 0.21332252025604248, Validation Loss: 0.25827765464782715\n",
      "Step 868/1000, Loss: 0.21331103146076202, Validation Loss: 0.25826552510261536\n",
      "Step 869/1000, Loss: 0.21329952776432037, Validation Loss: 0.25825345516204834\n",
      "Step 870/1000, Loss: 0.2132880538702011, Validation Loss: 0.2582413852214813\n",
      "Step 871/1000, Loss: 0.21327659487724304, Validation Loss: 0.25822925567626953\n",
      "Step 872/1000, Loss: 0.2132650911808014, Validation Loss: 0.2582172155380249\n",
      "Step 873/1000, Loss: 0.21325361728668213, Validation Loss: 0.2582050859928131\n",
      "Step 874/1000, Loss: 0.21324215829372406, Validation Loss: 0.2581930458545685\n",
      "Step 875/1000, Loss: 0.2132306545972824, Validation Loss: 0.2581809461116791\n",
      "Step 876/1000, Loss: 0.21321919560432434, Validation Loss: 0.25816890597343445\n",
      "Step 877/1000, Loss: 0.21320775151252747, Validation Loss: 0.25815683603286743\n",
      "Step 878/1000, Loss: 0.21319624781608582, Validation Loss: 0.2581447958946228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 879/1000, Loss: 0.21318477392196655, Validation Loss: 0.2581326961517334\n",
      "Step 880/1000, Loss: 0.21317331492900848, Validation Loss: 0.258120596408844\n",
      "Step 881/1000, Loss: 0.21316181123256683, Validation Loss: 0.258108526468277\n",
      "Step 882/1000, Loss: 0.21315032243728638, Validation Loss: 0.25809645652770996\n",
      "Step 883/1000, Loss: 0.21313883364200592, Validation Loss: 0.25808435678482056\n",
      "Step 884/1000, Loss: 0.21312737464904785, Validation Loss: 0.2580723464488983\n",
      "Step 885/1000, Loss: 0.21311591565608978, Validation Loss: 0.2580602467060089\n",
      "Step 886/1000, Loss: 0.21310444176197052, Validation Loss: 0.2580481469631195\n",
      "Step 887/1000, Loss: 0.21309296786785126, Validation Loss: 0.2580361068248749\n",
      "Step 888/1000, Loss: 0.2130814492702484, Validation Loss: 0.2580240070819855\n",
      "Step 889/1000, Loss: 0.21306999027729034, Validation Loss: 0.25801193714141846\n",
      "Step 890/1000, Loss: 0.21305853128433228, Validation Loss: 0.25799989700317383\n",
      "Step 891/1000, Loss: 0.21304701268672943, Validation Loss: 0.2579878568649292\n",
      "Step 892/1000, Loss: 0.21303556859493256, Validation Loss: 0.2579757869243622\n",
      "Step 893/1000, Loss: 0.2130240947008133, Validation Loss: 0.25796371698379517\n",
      "Step 894/1000, Loss: 0.21301259100437164, Validation Loss: 0.25795167684555054\n",
      "Step 895/1000, Loss: 0.21300114691257477, Validation Loss: 0.25793957710266113\n",
      "Step 896/1000, Loss: 0.2129896879196167, Validation Loss: 0.2579275667667389\n",
      "Step 897/1000, Loss: 0.21297819912433624, Validation Loss: 0.2579154074192047\n",
      "Step 898/1000, Loss: 0.21296672523021698, Validation Loss: 0.2579033076763153\n",
      "Step 899/1000, Loss: 0.21295525133609772, Validation Loss: 0.2578912079334259\n",
      "Step 900/1000, Loss: 0.21294376254081726, Validation Loss: 0.2578791379928589\n",
      "Step 901/1000, Loss: 0.2129323035478592, Validation Loss: 0.2578669786453247\n",
      "Step 902/1000, Loss: 0.21292082965373993, Validation Loss: 0.2578609585762024\n",
      "Step 903/1000, Loss: 0.2129150927066803, Validation Loss: 0.2578548789024353\n",
      "Step 904/1000, Loss: 0.21290934085845947, Validation Loss: 0.2578487992286682\n",
      "Step 905/1000, Loss: 0.21290357410907745, Validation Loss: 0.2578427195549011\n",
      "Step 906/1000, Loss: 0.21289782226085663, Validation Loss: 0.2578366696834564\n",
      "Step 907/1000, Loss: 0.21289211511611938, Validation Loss: 0.25783059000968933\n",
      "Step 908/1000, Loss: 0.21288636326789856, Validation Loss: 0.25782451033592224\n",
      "Step 909/1000, Loss: 0.21288062632083893, Validation Loss: 0.25781843066215515\n",
      "Step 910/1000, Loss: 0.2128748595714569, Validation Loss: 0.25781235098838806\n",
      "Step 911/1000, Loss: 0.21286910772323608, Validation Loss: 0.2578062415122986\n",
      "Step 912/1000, Loss: 0.21286337077617645, Validation Loss: 0.2578001618385315\n",
      "Step 913/1000, Loss: 0.21285761892795563, Validation Loss: 0.2577941119670868\n",
      "Step 914/1000, Loss: 0.2128518670797348, Validation Loss: 0.2577880322933197\n",
      "Step 915/1000, Loss: 0.21284613013267517, Validation Loss: 0.2577819526195526\n",
      "Step 916/1000, Loss: 0.21284036338329315, Validation Loss: 0.2577758729457855\n",
      "Step 917/1000, Loss: 0.21283462643623352, Validation Loss: 0.2577698230743408\n",
      "Step 918/1000, Loss: 0.2128288894891739, Validation Loss: 0.25776374340057373\n",
      "Step 919/1000, Loss: 0.21282313764095306, Validation Loss: 0.25775763392448425\n",
      "Step 920/1000, Loss: 0.21281737089157104, Validation Loss: 0.2577515244483948\n",
      "Step 921/1000, Loss: 0.21281161904335022, Validation Loss: 0.2577454745769501\n",
      "Step 922/1000, Loss: 0.2128058820962906, Validation Loss: 0.25773942470550537\n",
      "Step 923/1000, Loss: 0.21280016005039215, Validation Loss: 0.2577333450317383\n",
      "Step 924/1000, Loss: 0.21279436349868774, Validation Loss: 0.2577272355556488\n",
      "Step 925/1000, Loss: 0.2127886265516281, Validation Loss: 0.2577211856842041\n",
      "Step 926/1000, Loss: 0.21278288960456848, Validation Loss: 0.257715106010437\n",
      "Step 927/1000, Loss: 0.21277712285518646, Validation Loss: 0.2577090263366699\n",
      "Step 928/1000, Loss: 0.21277137100696564, Validation Loss: 0.25770291686058044\n",
      "Step 929/1000, Loss: 0.21276560425758362, Validation Loss: 0.25769689679145813\n",
      "Step 930/1000, Loss: 0.21275991201400757, Validation Loss: 0.25769081711769104\n",
      "Step 931/1000, Loss: 0.21275413036346436, Validation Loss: 0.25768473744392395\n",
      "Step 932/1000, Loss: 0.21274837851524353, Validation Loss: 0.25767868757247925\n",
      "Step 933/1000, Loss: 0.2127426117658615, Validation Loss: 0.25767263770103455\n",
      "Step 934/1000, Loss: 0.21273687481880188, Validation Loss: 0.25766652822494507\n",
      "Step 935/1000, Loss: 0.21273115277290344, Validation Loss: 0.257660448551178\n",
      "Step 936/1000, Loss: 0.21272535622119904, Validation Loss: 0.2576543986797333\n",
      "Step 937/1000, Loss: 0.2127196192741394, Validation Loss: 0.2576483190059662\n",
      "Step 938/1000, Loss: 0.21271388232707977, Validation Loss: 0.2576422393321991\n",
      "Step 939/1000, Loss: 0.21270811557769775, Validation Loss: 0.257636159658432\n",
      "Step 940/1000, Loss: 0.21270236372947693, Validation Loss: 0.2576300799846649\n",
      "Step 941/1000, Loss: 0.2126966267824173, Validation Loss: 0.2576240301132202\n",
      "Step 942/1000, Loss: 0.21269090473651886, Validation Loss: 0.2576179802417755\n",
      "Step 943/1000, Loss: 0.21268513798713684, Validation Loss: 0.2576119005680084\n",
      "Step 944/1000, Loss: 0.21267937123775482, Validation Loss: 0.25760582089424133\n",
      "Step 945/1000, Loss: 0.2126736342906952, Validation Loss: 0.257599800825119\n",
      "Step 946/1000, Loss: 0.21266788244247437, Validation Loss: 0.25759369134902954\n",
      "Step 947/1000, Loss: 0.21266214549541473, Validation Loss: 0.25758764147758484\n",
      "Step 948/1000, Loss: 0.2126563936471939, Validation Loss: 0.25758153200149536\n",
      "Step 949/1000, Loss: 0.2126506268978119, Validation Loss: 0.25757551193237305\n",
      "Step 950/1000, Loss: 0.21264491975307465, Validation Loss: 0.25756946206092834\n",
      "Step 951/1000, Loss: 0.21263915300369263, Validation Loss: 0.25756341218948364\n",
      "Step 952/1000, Loss: 0.2126334011554718, Validation Loss: 0.25755733251571655\n",
      "Step 953/1000, Loss: 0.2126276195049286, Validation Loss: 0.25755131244659424\n",
      "Step 954/1000, Loss: 0.21262191236019135, Validation Loss: 0.25754523277282715\n",
      "Step 955/1000, Loss: 0.21261614561080933, Validation Loss: 0.25753915309906006\n",
      "Step 956/1000, Loss: 0.2126103639602661, Validation Loss: 0.25753307342529297\n",
      "Step 957/1000, Loss: 0.2126046121120453, Validation Loss: 0.25752705335617065\n",
      "Step 958/1000, Loss: 0.21259889006614685, Validation Loss: 0.25752100348472595\n",
      "Step 959/1000, Loss: 0.21259312331676483, Validation Loss: 0.25751495361328125\n",
      "Step 960/1000, Loss: 0.212587371468544, Validation Loss: 0.2575088441371918\n",
      "Step 961/1000, Loss: 0.212581604719162, Validation Loss: 0.25750282406806946\n",
      "Step 962/1000, Loss: 0.21257586777210236, Validation Loss: 0.25749680399894714\n",
      "Step 963/1000, Loss: 0.21257011592388153, Validation Loss: 0.25749072432518005\n",
      "Step 964/1000, Loss: 0.2125643491744995, Validation Loss: 0.25748464465141296\n",
      "Step 965/1000, Loss: 0.2125585824251175, Validation Loss: 0.25747862458229065\n",
      "Step 966/1000, Loss: 0.21255283057689667, Validation Loss: 0.25747254490852356\n",
      "Step 967/1000, Loss: 0.21254709362983704, Validation Loss: 0.25746649503707886\n",
      "Step 968/1000, Loss: 0.2125413417816162, Validation Loss: 0.25746041536331177\n",
      "Step 969/1000, Loss: 0.2125355452299118, Validation Loss: 0.25745439529418945\n",
      "Step 970/1000, Loss: 0.21252980828285217, Validation Loss: 0.25744831562042236\n",
      "Step 971/1000, Loss: 0.21252407133579254, Validation Loss: 0.25744229555130005\n",
      "Step 972/1000, Loss: 0.21251827478408813, Validation Loss: 0.25743618607521057\n",
      "Step 973/1000, Loss: 0.21251250803470612, Validation Loss: 0.25743016600608826\n",
      "Step 974/1000, Loss: 0.21250677108764648, Validation Loss: 0.25742408633232117\n",
      "Step 975/1000, Loss: 0.21250101923942566, Validation Loss: 0.25741806626319885\n",
      "Step 976/1000, Loss: 0.21249523758888245, Validation Loss: 0.25741198658943176\n",
      "Step 977/1000, Loss: 0.21248950064182281, Validation Loss: 0.25740596652030945\n",
      "Step 978/1000, Loss: 0.2124837338924408, Validation Loss: 0.25739988684654236\n",
      "Step 979/1000, Loss: 0.21247798204421997, Validation Loss: 0.25739380717277527\n",
      "Step 980/1000, Loss: 0.21247220039367676, Validation Loss: 0.25738775730133057\n",
      "Step 981/1000, Loss: 0.21246644854545593, Validation Loss: 0.2573816776275635\n",
      "Step 982/1000, Loss: 0.2124606817960739, Validation Loss: 0.25737565755844116\n",
      "Step 983/1000, Loss: 0.2124549299478531, Validation Loss: 0.2573695778846741\n",
      "Step 984/1000, Loss: 0.21244916319847107, Validation Loss: 0.257363498210907\n",
      "Step 985/1000, Loss: 0.21244339644908905, Validation Loss: 0.25735747814178467\n",
      "Step 986/1000, Loss: 0.21243764460086823, Validation Loss: 0.2573513984680176\n",
      "Step 987/1000, Loss: 0.2124318778514862, Validation Loss: 0.25734537839889526\n",
      "Step 988/1000, Loss: 0.2124261111021042, Validation Loss: 0.2573392987251282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 989/1000, Loss: 0.21242035925388336, Validation Loss: 0.25733324885368347\n",
      "Step 990/1000, Loss: 0.21241459250450134, Validation Loss: 0.2573271691799164\n",
      "Step 991/1000, Loss: 0.21240882575511932, Validation Loss: 0.25732114911079407\n",
      "Step 992/1000, Loss: 0.2124030739068985, Validation Loss: 0.25731509923934937\n",
      "Step 993/1000, Loss: 0.21239730715751648, Validation Loss: 0.25730904936790466\n",
      "Step 994/1000, Loss: 0.21239157021045685, Validation Loss: 0.25730299949645996\n",
      "Step 995/1000, Loss: 0.21238580346107483, Validation Loss: 0.25729694962501526\n",
      "Step 996/1000, Loss: 0.2123800814151764, Validation Loss: 0.25729089975357056\n",
      "Step 997/1000, Loss: 0.21237431466579437, Validation Loss: 0.25728487968444824\n",
      "Step 998/1000, Loss: 0.21236859261989594, Validation Loss: 0.2572788596153259\n",
      "Step 999/1000, Loss: 0.2123628556728363, Validation Loss: 0.25727272033691406\n",
      "Step 1000/1000, Loss: 0.21235708892345428, Validation Loss: 0.25726667046546936\n",
      "Step 1/1000, Loss: 0.3244418799877167, Validation Loss: 0.6409236192703247\n",
      "Step 2/1000, Loss: 0.3236101567745209, Validation Loss: 0.641655683517456\n",
      "Step 3/1000, Loss: 0.3227902352809906, Validation Loss: 0.6419258713722229\n",
      "Step 4/1000, Loss: 0.32198211550712585, Validation Loss: 0.6418284177780151\n",
      "Step 5/1000, Loss: 0.3211854100227356, Validation Loss: 0.6415178179740906\n",
      "Step 6/1000, Loss: 0.32039979100227356, Validation Loss: 0.6410838961601257\n",
      "Step 7/1000, Loss: 0.31962525844573975, Validation Loss: 0.6405792236328125\n",
      "Step 8/1000, Loss: 0.31886184215545654, Validation Loss: 0.6400423049926758\n",
      "Step 9/1000, Loss: 0.3181094825267792, Validation Loss: 0.6395136117935181\n",
      "Step 10/1000, Loss: 0.3173682391643524, Validation Loss: 0.6390392184257507\n",
      "Step 11/1000, Loss: 0.31663787364959717, Validation Loss: 0.6386640667915344\n",
      "Step 12/1000, Loss: 0.31591835618019104, Validation Loss: 0.6384146213531494\n",
      "Step 13/1000, Loss: 0.3152094781398773, Validation Loss: 0.638290286064148\n",
      "Step 14/1000, Loss: 0.3145109713077545, Validation Loss: 0.6382731795310974\n",
      "Step 15/1000, Loss: 0.3138226866722107, Validation Loss: 0.6383394002914429\n",
      "Step 16/1000, Loss: 0.3131442964076996, Validation Loss: 0.6384665966033936\n",
      "Step 17/1000, Loss: 0.31247562170028687, Validation Loss: 0.6386364102363586\n",
      "Step 18/1000, Loss: 0.3118163049221039, Validation Loss: 0.6388339996337891\n",
      "Step 19/1000, Loss: 0.3111661672592163, Validation Loss: 0.6390484571456909\n",
      "Step 20/1000, Loss: 0.31052494049072266, Validation Loss: 0.6392709016799927\n",
      "Step 21/1000, Loss: 0.30989235639572144, Validation Loss: 0.6394951343536377\n",
      "Step 22/1000, Loss: 0.30926817655563354, Validation Loss: 0.639716386795044\n",
      "Step 23/1000, Loss: 0.3086521327495575, Validation Loss: 0.6399313807487488\n",
      "Step 24/1000, Loss: 0.30804404616355896, Validation Loss: 0.6401379704475403\n",
      "Step 25/1000, Loss: 0.3074435889720917, Validation Loss: 0.640334963798523\n",
      "Step 26/1000, Loss: 0.3068506717681885, Validation Loss: 0.6405213475227356\n",
      "Step 27/1000, Loss: 0.3062649965286255, Validation Loss: 0.6406980156898499\n",
      "Step 28/1000, Loss: 0.30568641424179077, Validation Loss: 0.6408647894859314\n",
      "Step 29/1000, Loss: 0.30511465668678284, Validation Loss: 0.6410224437713623\n",
      "Step 30/1000, Loss: 0.3045494854450226, Validation Loss: 0.6411719918251038\n",
      "Step 31/1000, Loss: 0.30399084091186523, Validation Loss: 0.6413150429725647\n",
      "Step 32/1000, Loss: 0.30343836545944214, Validation Loss: 0.641452431678772\n",
      "Step 33/1000, Loss: 0.3028920292854309, Validation Loss: 0.6415861248970032\n",
      "Step 34/1000, Loss: 0.30235153436660767, Validation Loss: 0.6417173743247986\n",
      "Step 35/1000, Loss: 0.30181682109832764, Validation Loss: 0.6418477296829224\n",
      "Step 36/1000, Loss: 0.3012876808643341, Validation Loss: 0.641978919506073\n",
      "Step 37/1000, Loss: 0.30076390504837036, Validation Loss: 0.642112135887146\n",
      "Step 38/1000, Loss: 0.30024540424346924, Validation Loss: 0.6422489881515503\n",
      "Step 39/1000, Loss: 0.2997320890426636, Validation Loss: 0.6423913836479187\n",
      "Step 40/1000, Loss: 0.2992236614227295, Validation Loss: 0.6425403356552124\n",
      "Step 41/1000, Loss: 0.29872018098831177, Validation Loss: 0.6426976919174194\n",
      "Step 42/1000, Loss: 0.2982213497161865, Validation Loss: 0.6428644061088562\n",
      "Step 43/1000, Loss: 0.29772719740867615, Validation Loss: 0.6430416703224182\n",
      "Step 44/1000, Loss: 0.29723745584487915, Validation Loss: 0.6432307362556458\n",
      "Step 45/1000, Loss: 0.29675212502479553, Validation Loss: 0.6434324979782104\n",
      "Step 46/1000, Loss: 0.29627111554145813, Validation Loss: 0.6436477303504944\n",
      "Step 47/1000, Loss: 0.29579421877861023, Validation Loss: 0.643877387046814\n",
      "Step 48/1000, Loss: 0.29532134532928467, Validation Loss: 0.6441217660903931\n",
      "Step 49/1000, Loss: 0.29485249519348145, Validation Loss: 0.6443816423416138\n",
      "Step 50/1000, Loss: 0.2943875193595886, Validation Loss: 0.6446570158004761\n",
      "Step 51/1000, Loss: 0.2939262092113495, Validation Loss: 0.6449480056762695\n",
      "Step 52/1000, Loss: 0.2934686541557312, Validation Loss: 0.645254909992218\n",
      "Step 53/1000, Loss: 0.29301461577415466, Validation Loss: 0.6455773115158081\n",
      "Step 54/1000, Loss: 0.2925640940666199, Validation Loss: 0.6459150910377502\n",
      "Step 55/1000, Loss: 0.2921169698238373, Validation Loss: 0.6462674736976624\n",
      "Step 56/1000, Loss: 0.2916731536388397, Validation Loss: 0.6466342806816101\n",
      "Step 57/1000, Loss: 0.29123255610466003, Validation Loss: 0.6470143795013428\n",
      "Step 58/1000, Loss: 0.29079514741897583, Validation Loss: 0.6474073529243469\n",
      "Step 59/1000, Loss: 0.29036077857017517, Validation Loss: 0.6478120684623718\n",
      "Step 60/1000, Loss: 0.2899293899536133, Validation Loss: 0.6482271552085876\n",
      "Step 61/1000, Loss: 0.2895009517669678, Validation Loss: 0.6486519575119019\n",
      "Step 62/1000, Loss: 0.2890753149986267, Validation Loss: 0.6490849256515503\n",
      "Step 63/1000, Loss: 0.2886524796485901, Validation Loss: 0.6495249271392822\n",
      "Step 64/1000, Loss: 0.28823238611221313, Validation Loss: 0.6499704122543335\n",
      "Step 65/1000, Loss: 0.2878148555755615, Validation Loss: 0.650420069694519\n",
      "Step 66/1000, Loss: 0.28739994764328003, Validation Loss: 0.6508723497390747\n",
      "Step 67/1000, Loss: 0.2869875431060791, Validation Loss: 0.6513255834579468\n",
      "Step 68/1000, Loss: 0.2865775525569916, Validation Loss: 0.6517786383628845\n",
      "Step 69/1000, Loss: 0.28616994619369507, Validation Loss: 0.6522299647331238\n",
      "Step 70/1000, Loss: 0.2857646942138672, Validation Loss: 0.6526780724525452\n",
      "Step 71/1000, Loss: 0.2853616774082184, Validation Loss: 0.6531217098236084\n",
      "Step 72/1000, Loss: 0.28496089577674866, Validation Loss: 0.6535595059394836\n",
      "Step 73/1000, Loss: 0.28456228971481323, Validation Loss: 0.6539903283119202\n",
      "Step 74/1000, Loss: 0.28416574001312256, Validation Loss: 0.6544127464294434\n",
      "Step 75/1000, Loss: 0.283771276473999, Validation Loss: 0.6548258066177368\n",
      "Step 76/1000, Loss: 0.28337880969047546, Validation Loss: 0.6552287936210632\n",
      "Step 77/1000, Loss: 0.2829882502555847, Validation Loss: 0.6556207537651062\n",
      "Step 78/1000, Loss: 0.28259962797164917, Validation Loss: 0.6560009717941284\n",
      "Step 79/1000, Loss: 0.28221288323402405, Validation Loss: 0.6563687920570374\n",
      "Step 80/1000, Loss: 0.28182798624038696, Validation Loss: 0.6567240953445435\n",
      "Step 81/1000, Loss: 0.281444787979126, Validation Loss: 0.6570662260055542\n",
      "Step 82/1000, Loss: 0.28106340765953064, Validation Loss: 0.6573954820632935\n",
      "Step 83/1000, Loss: 0.28068363666534424, Validation Loss: 0.6577116847038269\n",
      "Step 84/1000, Loss: 0.28030553460121155, Validation Loss: 0.6580148935317993\n",
      "Step 85/1000, Loss: 0.2799290716648102, Validation Loss: 0.6583055257797241\n",
      "Step 86/1000, Loss: 0.27955418825149536, Validation Loss: 0.6585835814476013\n",
      "Step 87/1000, Loss: 0.27918076515197754, Validation Loss: 0.6588497757911682\n",
      "Step 88/1000, Loss: 0.27880895137786865, Validation Loss: 0.6591044664382935\n",
      "Step 89/1000, Loss: 0.2784384787082672, Validation Loss: 0.6593487858772278\n",
      "Step 90/1000, Loss: 0.27806952595710754, Validation Loss: 0.6595830917358398\n",
      "Step 91/1000, Loss: 0.2777019739151001, Validation Loss: 0.6598083972930908\n",
      "Step 92/1000, Loss: 0.2773357331752777, Validation Loss: 0.6600252985954285\n",
      "Step 93/1000, Loss: 0.27697086334228516, Validation Loss: 0.6602344512939453\n",
      "Step 94/1000, Loss: 0.27660733461380005, Validation Loss: 0.6604369282722473\n",
      "Step 95/1000, Loss: 0.27624502778053284, Validation Loss: 0.6606336236000061\n",
      "Step 96/1000, Loss: 0.2758840024471283, Validation Loss: 0.6608253121376038\n",
      "Step 97/1000, Loss: 0.27552419900894165, Validation Loss: 0.6610128283500671\n",
      "Step 98/1000, Loss: 0.2751655578613281, Validation Loss: 0.6611967086791992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 99/1000, Loss: 0.2748080790042877, Validation Loss: 0.661378026008606\n",
      "Step 100/1000, Loss: 0.2744518220424652, Validation Loss: 0.6615572571754456\n",
      "Step 101/1000, Loss: 0.27409660816192627, Validation Loss: 0.6617351174354553\n",
      "Step 102/1000, Loss: 0.27374252676963806, Validation Loss: 0.6618231534957886\n",
      "Step 103/1000, Loss: 0.27356594800949097, Validation Loss: 0.6619102358818054\n",
      "Step 104/1000, Loss: 0.27338963747024536, Validation Loss: 0.6619965434074402\n",
      "Step 105/1000, Loss: 0.2732137441635132, Validation Loss: 0.6620824933052063\n",
      "Step 106/1000, Loss: 0.2730380892753601, Validation Loss: 0.6621678471565247\n",
      "Step 107/1000, Loss: 0.2728627324104309, Validation Loss: 0.6622534394264221\n",
      "Step 108/1000, Loss: 0.2726876735687256, Validation Loss: 0.6623389720916748\n",
      "Step 109/1000, Loss: 0.27251285314559937, Validation Loss: 0.6624243259429932\n",
      "Step 110/1000, Loss: 0.27233824133872986, Validation Loss: 0.6625102758407593\n",
      "Step 111/1000, Loss: 0.27216389775276184, Validation Loss: 0.662596583366394\n",
      "Step 112/1000, Loss: 0.27198976278305054, Validation Loss: 0.6626830697059631\n",
      "Step 113/1000, Loss: 0.2718159258365631, Validation Loss: 0.6627703309059143\n",
      "Step 114/1000, Loss: 0.27164217829704285, Validation Loss: 0.6628580093383789\n",
      "Early stopping triggered\n",
      "Step 1/1000, Loss: 1.034821629524231, Validation Loss: 1.190604567527771\n",
      "Step 2/1000, Loss: 1.033720850944519, Validation Loss: 1.1896613836288452\n",
      "Step 3/1000, Loss: 1.0326217412948608, Validation Loss: 1.1887199878692627\n",
      "Step 4/1000, Loss: 1.0315238237380981, Validation Loss: 1.1877801418304443\n",
      "Step 5/1000, Loss: 1.0304269790649414, Validation Loss: 1.1868417263031006\n",
      "Step 6/1000, Loss: 1.0293309688568115, Validation Loss: 1.1859043836593628\n",
      "Step 7/1000, Loss: 1.028235673904419, Validation Loss: 1.184968113899231\n",
      "Step 8/1000, Loss: 1.0271406173706055, Validation Loss: 1.1840324401855469\n",
      "Step 9/1000, Loss: 1.0260459184646606, Validation Loss: 1.183097243309021\n",
      "Step 10/1000, Loss: 1.0249511003494263, Validation Loss: 1.1821622848510742\n",
      "Step 11/1000, Loss: 1.0238559246063232, Validation Loss: 1.181227445602417\n",
      "Step 12/1000, Loss: 1.022760272026062, Validation Loss: 1.1802926063537598\n",
      "Step 13/1000, Loss: 1.021664023399353, Validation Loss: 1.179357647895813\n",
      "Step 14/1000, Loss: 1.0205665826797485, Validation Loss: 1.178422212600708\n",
      "Step 15/1000, Loss: 1.0194677114486694, Validation Loss: 1.1774866580963135\n",
      "Step 16/1000, Loss: 1.0183675289154053, Validation Loss: 1.1765505075454712\n",
      "Step 17/1000, Loss: 1.0172655582427979, Validation Loss: 1.1756138801574707\n",
      "Step 18/1000, Loss: 1.0161614418029785, Validation Loss: 1.174676775932312\n",
      "Step 19/1000, Loss: 1.0150551795959473, Validation Loss: 1.1737383604049683\n",
      "Step 20/1000, Loss: 1.0139460563659668, Validation Loss: 1.1727992296218872\n",
      "Step 21/1000, Loss: 1.0128343105316162, Validation Loss: 1.1718590259552002\n",
      "Step 22/1000, Loss: 1.0117194652557373, Validation Loss: 1.1709175109863281\n",
      "Step 23/1000, Loss: 1.0106010437011719, Validation Loss: 1.169974446296692\n",
      "Step 24/1000, Loss: 1.0094789266586304, Validation Loss: 1.169029712677002\n",
      "Step 25/1000, Loss: 1.0083528757095337, Validation Loss: 1.1680833101272583\n",
      "Step 26/1000, Loss: 1.0072225332260132, Validation Loss: 1.1671351194381714\n",
      "Step 27/1000, Loss: 1.0060876607894897, Validation Loss: 1.166184902191162\n",
      "Step 28/1000, Loss: 1.0049481391906738, Validation Loss: 1.165232539176941\n",
      "Step 29/1000, Loss: 1.0038037300109863, Validation Loss: 1.1642777919769287\n",
      "Step 30/1000, Loss: 1.002653956413269, Validation Loss: 1.1633208990097046\n",
      "Step 31/1000, Loss: 1.0014989376068115, Validation Loss: 1.1623613834381104\n",
      "Step 32/1000, Loss: 1.0003383159637451, Validation Loss: 1.161399245262146\n",
      "Step 33/1000, Loss: 0.999172031879425, Validation Loss: 1.160434365272522\n",
      "Step 34/1000, Loss: 0.9979996681213379, Validation Loss: 1.1594663858413696\n",
      "Step 35/1000, Loss: 0.9968212246894836, Validation Loss: 1.1584954261779785\n",
      "Step 36/1000, Loss: 0.9956362843513489, Validation Loss: 1.1575212478637695\n",
      "Step 37/1000, Loss: 0.9944448471069336, Validation Loss: 1.156543493270874\n",
      "Step 38/1000, Loss: 0.9932467937469482, Validation Loss: 1.155562400817871\n",
      "Step 39/1000, Loss: 0.9920419454574585, Validation Loss: 1.154577374458313\n",
      "Step 40/1000, Loss: 0.9908298850059509, Validation Loss: 1.1535882949829102\n",
      "Step 41/1000, Loss: 0.9896107912063599, Validation Loss: 1.152595043182373\n",
      "Step 42/1000, Loss: 0.9883843064308167, Validation Loss: 1.151597499847412\n",
      "Step 43/1000, Loss: 0.9871501326560974, Validation Loss: 1.1505955457687378\n",
      "Step 44/1000, Loss: 0.9859083890914917, Validation Loss: 1.1495888233184814\n",
      "Step 45/1000, Loss: 0.9846588969230652, Validation Loss: 1.1485769748687744\n",
      "Step 46/1000, Loss: 0.9834015369415283, Validation Loss: 1.1475600004196167\n",
      "Step 47/1000, Loss: 0.9821360111236572, Validation Loss: 1.1465377807617188\n",
      "Step 48/1000, Loss: 0.9808622598648071, Validation Loss: 1.145509958267212\n",
      "Step 49/1000, Loss: 0.9795799851417542, Validation Loss: 1.1444765329360962\n",
      "Step 50/1000, Loss: 0.9782893061637878, Validation Loss: 1.1434370279312134\n",
      "Step 51/1000, Loss: 0.9769899845123291, Validation Loss: 1.1423914432525635\n",
      "Step 52/1000, Loss: 0.9756819605827332, Validation Loss: 1.141339659690857\n",
      "Step 53/1000, Loss: 0.974364697933197, Validation Loss: 1.1402814388275146\n",
      "Step 54/1000, Loss: 0.9730386137962341, Validation Loss: 1.139216423034668\n",
      "Step 55/1000, Loss: 0.9717031717300415, Validation Loss: 1.138144612312317\n",
      "Step 56/1000, Loss: 0.9703587293624878, Validation Loss: 1.1370657682418823\n",
      "Step 57/1000, Loss: 0.9690045118331909, Validation Loss: 1.1359796524047852\n",
      "Step 58/1000, Loss: 0.9676409959793091, Validation Loss: 1.1348860263824463\n",
      "Step 59/1000, Loss: 0.9662678837776184, Validation Loss: 1.133784532546997\n",
      "Step 60/1000, Loss: 0.9648849964141846, Validation Loss: 1.1326755285263062\n",
      "Step 61/1000, Loss: 0.963492214679718, Validation Loss: 1.1315581798553467\n",
      "Step 62/1000, Loss: 0.9620896577835083, Validation Loss: 1.1304326057434082\n",
      "Step 63/1000, Loss: 0.9606770873069763, Validation Loss: 1.129298448562622\n",
      "Step 64/1000, Loss: 0.9592544436454773, Validation Loss: 1.1281557083129883\n",
      "Step 65/1000, Loss: 0.957821786403656, Validation Loss: 1.1270040273666382\n",
      "Step 66/1000, Loss: 0.9563788771629333, Validation Loss: 1.1258431673049927\n",
      "Step 67/1000, Loss: 0.9549257159233093, Validation Loss: 1.124672770500183\n",
      "Step 68/1000, Loss: 0.9534621834754944, Validation Loss: 1.1234930753707886\n",
      "Step 69/1000, Loss: 0.9519883394241333, Validation Loss: 1.1223033666610718\n",
      "Step 70/1000, Loss: 0.9505041837692261, Validation Loss: 1.1211036443710327\n",
      "Step 71/1000, Loss: 0.9490095973014832, Validation Loss: 1.1198937892913818\n",
      "Step 72/1000, Loss: 0.9475043416023254, Validation Loss: 1.1186732053756714\n",
      "Step 73/1000, Loss: 0.9459885358810425, Validation Loss: 1.1174421310424805\n",
      "Step 74/1000, Loss: 0.9444622993469238, Validation Loss: 1.1162002086639404\n",
      "Step 75/1000, Loss: 0.9429252743721008, Validation Loss: 1.114946961402893\n",
      "Step 76/1000, Loss: 0.9413776397705078, Validation Loss: 1.113682508468628\n",
      "Step 77/1000, Loss: 0.9398192763328552, Validation Loss: 1.1124064922332764\n",
      "Step 78/1000, Loss: 0.9382502436637878, Validation Loss: 1.1111186742782593\n",
      "Step 79/1000, Loss: 0.9366704821586609, Validation Loss: 1.109818935394287\n",
      "Step 80/1000, Loss: 0.9350798726081848, Validation Loss: 1.1085071563720703\n",
      "Step 81/1000, Loss: 0.9334783554077148, Validation Loss: 1.1071829795837402\n",
      "Step 82/1000, Loss: 0.9318659901618958, Validation Loss: 1.1058461666107178\n",
      "Step 83/1000, Loss: 0.9302428364753723, Validation Loss: 1.1044968366622925\n",
      "Step 84/1000, Loss: 0.9286087155342102, Validation Loss: 1.1031345129013062\n",
      "Step 85/1000, Loss: 0.9269636273384094, Validation Loss: 1.1017593145370483\n",
      "Step 86/1000, Loss: 0.9253076910972595, Validation Loss: 1.1003708839416504\n",
      "Step 87/1000, Loss: 0.9236408472061157, Validation Loss: 1.0989692211151123\n",
      "Step 88/1000, Loss: 0.921963095664978, Validation Loss: 1.0975542068481445\n",
      "Step 89/1000, Loss: 0.9202742576599121, Validation Loss: 1.0961253643035889\n",
      "Step 90/1000, Loss: 0.9185745120048523, Validation Loss: 1.094683051109314\n",
      "Step 91/1000, Loss: 0.9168638586997986, Validation Loss: 1.0932271480560303\n",
      "Step 92/1000, Loss: 0.9151421189308167, Validation Loss: 1.0917571783065796\n",
      "Step 93/1000, Loss: 0.913409411907196, Validation Loss: 1.0902734994888306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 94/1000, Loss: 0.9116658568382263, Validation Loss: 1.088775634765625\n",
      "Step 95/1000, Loss: 0.9099112749099731, Validation Loss: 1.087263822555542\n",
      "Step 96/1000, Loss: 0.9081454873085022, Validation Loss: 1.085737943649292\n",
      "Step 97/1000, Loss: 0.9063689708709717, Validation Loss: 1.0841976404190063\n",
      "Step 98/1000, Loss: 0.9045811891555786, Validation Loss: 1.0826433897018433\n",
      "Step 99/1000, Loss: 0.9027824997901917, Validation Loss: 1.0810747146606445\n",
      "Step 100/1000, Loss: 0.9009726643562317, Validation Loss: 1.0794919729232788\n",
      "Step 101/1000, Loss: 0.8991515636444092, Validation Loss: 1.0778948068618774\n",
      "Step 102/1000, Loss: 0.8973191976547241, Validation Loss: 1.077091097831726\n",
      "Step 103/1000, Loss: 0.8963989019393921, Validation Loss: 1.0762850046157837\n",
      "Step 104/1000, Loss: 0.8954761624336243, Validation Loss: 1.075476050376892\n",
      "Step 105/1000, Loss: 0.8945510983467102, Validation Loss: 1.0746644735336304\n",
      "Step 106/1000, Loss: 0.8936238884925842, Validation Loss: 1.073850393295288\n",
      "Step 107/1000, Loss: 0.8926945328712463, Validation Loss: 1.0730338096618652\n",
      "Step 108/1000, Loss: 0.8917630314826965, Validation Loss: 1.0722148418426514\n",
      "Step 109/1000, Loss: 0.8908297419548035, Validation Loss: 1.071393609046936\n",
      "Step 110/1000, Loss: 0.8898943662643433, Validation Loss: 1.0705697536468506\n",
      "Step 111/1000, Loss: 0.8889573812484741, Validation Loss: 1.0697438716888428\n",
      "Step 112/1000, Loss: 0.8880184292793274, Validation Loss: 1.068915605545044\n",
      "Step 113/1000, Loss: 0.8870775699615479, Validation Loss: 1.0680848360061646\n",
      "Step 114/1000, Loss: 0.8861349821090698, Validation Loss: 1.0672520399093628\n",
      "Step 115/1000, Loss: 0.8851907253265381, Validation Loss: 1.0664167404174805\n",
      "Step 116/1000, Loss: 0.8842445611953735, Validation Loss: 1.0655795335769653\n",
      "Step 117/1000, Loss: 0.8832966089248657, Validation Loss: 1.0647398233413696\n",
      "Step 118/1000, Loss: 0.8823468685150146, Validation Loss: 1.063897967338562\n",
      "Step 119/1000, Loss: 0.8813953399658203, Validation Loss: 1.0630537271499634\n",
      "Step 120/1000, Loss: 0.8804421424865723, Validation Loss: 1.0622073411941528\n",
      "Step 121/1000, Loss: 0.8794869780540466, Validation Loss: 1.06135892868042\n",
      "Step 122/1000, Loss: 0.878529965877533, Validation Loss: 1.0605080127716064\n",
      "Step 123/1000, Loss: 0.877571165561676, Validation Loss: 1.0596551895141602\n",
      "Step 124/1000, Loss: 0.876610517501831, Validation Loss: 1.0587999820709229\n",
      "Step 125/1000, Loss: 0.8756477236747742, Validation Loss: 1.0579426288604736\n",
      "Step 126/1000, Loss: 0.8746830224990845, Validation Loss: 1.0570831298828125\n",
      "Step 127/1000, Loss: 0.8737164735794067, Validation Loss: 1.0562212467193604\n",
      "Step 128/1000, Loss: 0.8727478981018066, Validation Loss: 1.0553572177886963\n",
      "Step 129/1000, Loss: 0.8717772364616394, Validation Loss: 1.0544910430908203\n",
      "Step 130/1000, Loss: 0.870804488658905, Validation Loss: 1.0536227226257324\n",
      "Step 131/1000, Loss: 0.869829535484314, Validation Loss: 1.052751898765564\n",
      "Step 132/1000, Loss: 0.8688526153564453, Validation Loss: 1.0518791675567627\n",
      "Step 133/1000, Loss: 0.86787348985672, Validation Loss: 1.05100417137146\n",
      "Step 134/1000, Loss: 0.8668920993804932, Validation Loss: 1.0501267910003662\n",
      "Step 135/1000, Loss: 0.8659085631370544, Validation Loss: 1.049247145652771\n",
      "Step 136/1000, Loss: 0.8649225234985352, Validation Loss: 1.0483653545379639\n",
      "Step 137/1000, Loss: 0.863934338092804, Validation Loss: 1.0474812984466553\n",
      "Step 138/1000, Loss: 0.862943708896637, Validation Loss: 1.0465947389602661\n",
      "Step 139/1000, Loss: 0.8619508147239685, Validation Loss: 1.045706033706665\n",
      "Step 140/1000, Loss: 0.8609554171562195, Validation Loss: 1.0448150634765625\n",
      "Step 141/1000, Loss: 0.8599575161933899, Validation Loss: 1.0439215898513794\n",
      "Step 142/1000, Loss: 0.8589571118354797, Validation Loss: 1.0430259704589844\n",
      "Step 143/1000, Loss: 0.857954204082489, Validation Loss: 1.0421278476715088\n",
      "Step 144/1000, Loss: 0.8569486141204834, Validation Loss: 1.0412273406982422\n",
      "Step 145/1000, Loss: 0.8559404611587524, Validation Loss: 1.0403244495391846\n",
      "Step 146/1000, Loss: 0.8549295663833618, Validation Loss: 1.0394190549850464\n",
      "Step 147/1000, Loss: 0.8539161682128906, Validation Loss: 1.038511037826538\n",
      "Step 148/1000, Loss: 0.8529000282287598, Validation Loss: 1.0376007556915283\n",
      "Step 149/1000, Loss: 0.8518810868263245, Validation Loss: 1.036687970161438\n",
      "Step 150/1000, Loss: 0.8508593440055847, Validation Loss: 1.0357725620269775\n",
      "Step 151/1000, Loss: 0.8498347401618958, Validation Loss: 1.0348544120788574\n",
      "Step 152/1000, Loss: 0.8488073348999023, Validation Loss: 1.0339338779449463\n",
      "Step 153/1000, Loss: 0.8477770686149597, Validation Loss: 1.0330106019973755\n",
      "Step 154/1000, Loss: 0.8467438220977783, Validation Loss: 1.0320847034454346\n",
      "Step 155/1000, Loss: 0.8457075953483582, Validation Loss: 1.031156063079834\n",
      "Step 156/1000, Loss: 0.8446683883666992, Validation Loss: 1.0302248001098633\n",
      "Step 157/1000, Loss: 0.8436263799667358, Validation Loss: 1.0292904376983643\n",
      "Step 158/1000, Loss: 0.8425810933113098, Validation Loss: 1.0283536911010742\n",
      "Step 159/1000, Loss: 0.8415328860282898, Validation Loss: 1.0274138450622559\n",
      "Step 160/1000, Loss: 0.8404815793037415, Validation Loss: 1.0264712572097778\n",
      "Step 161/1000, Loss: 0.8394272327423096, Validation Loss: 1.025525689125061\n",
      "Step 162/1000, Loss: 0.8383696675300598, Validation Loss: 1.024577260017395\n",
      "Step 163/1000, Loss: 0.8373088836669922, Validation Loss: 1.0236258506774902\n",
      "Step 164/1000, Loss: 0.8362449407577515, Validation Loss: 1.0226715803146362\n",
      "Step 165/1000, Loss: 0.8351777791976929, Validation Loss: 1.0217140913009644\n",
      "Step 166/1000, Loss: 0.8341074585914612, Validation Loss: 1.0207536220550537\n",
      "Step 167/1000, Loss: 0.8330338597297668, Validation Loss: 1.0197901725769043\n",
      "Step 168/1000, Loss: 0.8319569826126099, Validation Loss: 1.0188236236572266\n",
      "Step 169/1000, Loss: 0.8308767676353455, Validation Loss: 1.0178539752960205\n",
      "Step 170/1000, Loss: 0.8297933340072632, Validation Loss: 1.016880989074707\n",
      "Step 171/1000, Loss: 0.828706681728363, Validation Loss: 1.0159049034118652\n",
      "Step 172/1000, Loss: 0.8276164531707764, Validation Loss: 1.0149257183074951\n",
      "Step 173/1000, Loss: 0.826522946357727, Validation Loss: 1.0139433145523071\n",
      "Step 174/1000, Loss: 0.8254261016845703, Validation Loss: 1.0129574537277222\n",
      "Step 175/1000, Loss: 0.8243258595466614, Validation Loss: 1.0119684934616089\n",
      "Step 176/1000, Loss: 0.8232221603393555, Validation Loss: 1.0109763145446777\n",
      "Step 177/1000, Loss: 0.8221150636672974, Validation Loss: 1.0099807977676392\n",
      "Step 178/1000, Loss: 0.8210045695304871, Validation Loss: 1.0089818239212036\n",
      "Step 179/1000, Loss: 0.8198906779289246, Validation Loss: 1.0079796314239502\n",
      "Step 180/1000, Loss: 0.8187733888626099, Validation Loss: 1.0069741010665894\n",
      "Step 181/1000, Loss: 0.8176525235176086, Validation Loss: 1.0059651136398315\n",
      "Step 182/1000, Loss: 0.8165280818939209, Validation Loss: 1.0049526691436768\n",
      "Step 183/1000, Loss: 0.8154004216194153, Validation Loss: 1.0039371252059937\n",
      "Step 184/1000, Loss: 0.8142690062522888, Validation Loss: 1.0029181241989136\n",
      "Step 185/1000, Loss: 0.8131343126296997, Validation Loss: 1.001895546913147\n",
      "Step 186/1000, Loss: 0.8119960427284241, Validation Loss: 1.0008693933486938\n",
      "Step 187/1000, Loss: 0.8108543157577515, Validation Loss: 0.9998401999473572\n",
      "Step 188/1000, Loss: 0.809708833694458, Validation Loss: 0.9988073110580444\n",
      "Step 189/1000, Loss: 0.8085600137710571, Validation Loss: 0.997771143913269\n",
      "Step 190/1000, Loss: 0.8074077367782593, Validation Loss: 0.9967315793037415\n",
      "Step 191/1000, Loss: 0.8062517642974854, Validation Loss: 0.9956884980201721\n",
      "Step 192/1000, Loss: 0.8050923943519592, Validation Loss: 0.9946420192718506\n",
      "Step 193/1000, Loss: 0.8039295077323914, Validation Loss: 0.9935920238494873\n",
      "Step 194/1000, Loss: 0.8027629852294922, Validation Loss: 0.9925385117530823\n",
      "Step 195/1000, Loss: 0.801593005657196, Validation Loss: 0.9914817214012146\n",
      "Step 196/1000, Loss: 0.8004194498062134, Validation Loss: 0.9904215335845947\n",
      "Step 197/1000, Loss: 0.7992423176765442, Validation Loss: 0.9893578290939331\n",
      "Step 198/1000, Loss: 0.798061728477478, Validation Loss: 0.9882907271385193\n",
      "Step 199/1000, Loss: 0.7968777418136597, Validation Loss: 0.9872201681137085\n",
      "Step 200/1000, Loss: 0.7956900596618652, Validation Loss: 0.9861462712287903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 201/1000, Loss: 0.7944989204406738, Validation Loss: 0.9850688576698303\n",
      "Step 202/1000, Loss: 0.7933042645454407, Validation Loss: 0.9845291376113892\n",
      "Step 203/1000, Loss: 0.7927058339118958, Validation Loss: 0.9839891195297241\n",
      "Step 204/1000, Loss: 0.7921068668365479, Validation Loss: 0.9834486246109009\n",
      "Step 205/1000, Loss: 0.7915077209472656, Validation Loss: 0.982907772064209\n",
      "Step 206/1000, Loss: 0.7909082174301147, Validation Loss: 0.9823666214942932\n",
      "Step 207/1000, Loss: 0.79030841588974, Validation Loss: 0.9818254113197327\n",
      "Step 208/1000, Loss: 0.7897083759307861, Validation Loss: 0.9812837243080139\n",
      "Step 209/1000, Loss: 0.7891080975532532, Validation Loss: 0.9807419180870056\n",
      "Step 210/1000, Loss: 0.7885076999664307, Validation Loss: 0.9801999926567078\n",
      "Step 211/1000, Loss: 0.7879071831703186, Validation Loss: 0.979657769203186\n",
      "Step 212/1000, Loss: 0.7873064279556274, Validation Loss: 0.9791153073310852\n",
      "Step 213/1000, Loss: 0.7867056131362915, Validation Loss: 0.9785727858543396\n",
      "Step 214/1000, Loss: 0.7861045598983765, Validation Loss: 0.9780299663543701\n",
      "Step 215/1000, Loss: 0.7855035662651062, Validation Loss: 0.9774870872497559\n",
      "Step 216/1000, Loss: 0.7849022746086121, Validation Loss: 0.9769439697265625\n",
      "Step 217/1000, Loss: 0.7843010425567627, Validation Loss: 0.9764007925987244\n",
      "Step 218/1000, Loss: 0.783699631690979, Validation Loss: 0.9758573174476624\n",
      "Step 219/1000, Loss: 0.7830982804298401, Validation Loss: 0.9753138422966003\n",
      "Step 220/1000, Loss: 0.7824966907501221, Validation Loss: 0.974770188331604\n",
      "Step 221/1000, Loss: 0.7818951606750488, Validation Loss: 0.9742262959480286\n",
      "Step 222/1000, Loss: 0.781293511390686, Validation Loss: 0.9736824035644531\n",
      "Step 223/1000, Loss: 0.7806916832923889, Validation Loss: 0.9731380939483643\n",
      "Step 224/1000, Loss: 0.780089795589447, Validation Loss: 0.9725938439369202\n",
      "Step 225/1000, Loss: 0.7794879674911499, Validation Loss: 0.9720492959022522\n",
      "Step 226/1000, Loss: 0.7788859009742737, Validation Loss: 0.971504807472229\n",
      "Step 227/1000, Loss: 0.7782838344573975, Validation Loss: 0.9709601402282715\n",
      "Step 228/1000, Loss: 0.7776817083358765, Validation Loss: 0.9704151153564453\n",
      "Step 229/1000, Loss: 0.7770794630050659, Validation Loss: 0.9698700308799744\n",
      "Step 230/1000, Loss: 0.7764770984649658, Validation Loss: 0.9693247079849243\n",
      "Step 231/1000, Loss: 0.7758746147155762, Validation Loss: 0.9687793850898743\n",
      "Step 232/1000, Loss: 0.7752721309661865, Validation Loss: 0.9682337045669556\n",
      "Step 233/1000, Loss: 0.7746694684028625, Validation Loss: 0.9676880240440369\n",
      "Step 234/1000, Loss: 0.7740667462348938, Validation Loss: 0.9671420454978943\n",
      "Step 235/1000, Loss: 0.7734638452529907, Validation Loss: 0.9665958881378174\n",
      "Step 236/1000, Loss: 0.7728609442710876, Validation Loss: 0.9660494923591614\n",
      "Step 237/1000, Loss: 0.7722578048706055, Validation Loss: 0.9655030369758606\n",
      "Step 238/1000, Loss: 0.7716546654701233, Validation Loss: 0.9649562239646912\n",
      "Step 239/1000, Loss: 0.771051287651062, Validation Loss: 0.9644095301628113\n",
      "Step 240/1000, Loss: 0.7704477906227112, Validation Loss: 0.9638622999191284\n",
      "Step 241/1000, Loss: 0.7698442339897156, Validation Loss: 0.9633150696754456\n",
      "Step 242/1000, Loss: 0.7692405581474304, Validation Loss: 0.9627675414085388\n",
      "Step 243/1000, Loss: 0.7686366438865662, Validation Loss: 0.9622198343276978\n",
      "Step 244/1000, Loss: 0.7680326700210571, Validation Loss: 0.9616719484329224\n",
      "Step 245/1000, Loss: 0.7674285173416138, Validation Loss: 0.9611237645149231\n",
      "Step 246/1000, Loss: 0.7668242454528809, Validation Loss: 0.9605754613876343\n",
      "Step 247/1000, Loss: 0.7662196755409241, Validation Loss: 0.9600269198417664\n",
      "Step 248/1000, Loss: 0.7656151652336121, Validation Loss: 0.9594780802726746\n",
      "Step 249/1000, Loss: 0.7650102972984314, Validation Loss: 0.9589290618896484\n",
      "Step 250/1000, Loss: 0.764405369758606, Validation Loss: 0.9583799242973328\n",
      "Step 251/1000, Loss: 0.763800323009491, Validation Loss: 0.9578304290771484\n",
      "Step 252/1000, Loss: 0.7631950974464417, Validation Loss: 0.9572807550430298\n",
      "Step 253/1000, Loss: 0.7625896334648132, Validation Loss: 0.9567307829856873\n",
      "Step 254/1000, Loss: 0.7619840502738953, Validation Loss: 0.9561806917190552\n",
      "Step 255/1000, Loss: 0.7613781094551086, Validation Loss: 0.9556302428245544\n",
      "Step 256/1000, Loss: 0.7607722282409668, Validation Loss: 0.9550794959068298\n",
      "Step 257/1000, Loss: 0.7601659893989563, Validation Loss: 0.9545286297798157\n",
      "Step 258/1000, Loss: 0.7595596313476562, Validation Loss: 0.9539774060249329\n",
      "Step 259/1000, Loss: 0.7589530944824219, Validation Loss: 0.9534260034561157\n",
      "Step 260/1000, Loss: 0.7583463788032532, Validation Loss: 0.9528743624687195\n",
      "Step 261/1000, Loss: 0.7577394247055054, Validation Loss: 0.9523225426673889\n",
      "Step 262/1000, Loss: 0.7571322917938232, Validation Loss: 0.9517702460289001\n",
      "Step 263/1000, Loss: 0.7565250396728516, Validation Loss: 0.9512178897857666\n",
      "Step 264/1000, Loss: 0.755917489528656, Validation Loss: 0.9506651759147644\n",
      "Step 265/1000, Loss: 0.7553096413612366, Validation Loss: 0.9501121044158936\n",
      "Step 266/1000, Loss: 0.7547017931938171, Validation Loss: 0.9495588541030884\n",
      "Step 267/1000, Loss: 0.7540937066078186, Validation Loss: 0.9490053057670593\n",
      "Step 268/1000, Loss: 0.7534852623939514, Validation Loss: 0.9484514594078064\n",
      "Step 269/1000, Loss: 0.7528766989707947, Validation Loss: 0.9478974938392639\n",
      "Step 270/1000, Loss: 0.7522680163383484, Validation Loss: 0.9473432302474976\n",
      "Step 271/1000, Loss: 0.7516588568687439, Validation Loss: 0.946788489818573\n",
      "Step 272/1000, Loss: 0.7510496973991394, Validation Loss: 0.9462334513664246\n",
      "Step 273/1000, Loss: 0.7504402995109558, Validation Loss: 0.9456784129142761\n",
      "Step 274/1000, Loss: 0.7498306035995483, Validation Loss: 0.9451228976249695\n",
      "Step 275/1000, Loss: 0.7492206692695618, Validation Loss: 0.944567084312439\n",
      "Step 276/1000, Loss: 0.7486105561256409, Validation Loss: 0.9440109133720398\n",
      "Step 277/1000, Loss: 0.7480002045631409, Validation Loss: 0.9434545636177063\n",
      "Step 278/1000, Loss: 0.7473896145820618, Validation Loss: 0.9428977966308594\n",
      "Step 279/1000, Loss: 0.7467789649963379, Validation Loss: 0.9423407316207886\n",
      "Step 280/1000, Loss: 0.7461678385734558, Validation Loss: 0.941783607006073\n",
      "Step 281/1000, Loss: 0.7455565929412842, Validation Loss: 0.9412258267402649\n",
      "Step 282/1000, Loss: 0.7449449896812439, Validation Loss: 0.9406679272651672\n",
      "Step 283/1000, Loss: 0.7443333268165588, Validation Loss: 0.9401096105575562\n",
      "Step 284/1000, Loss: 0.7437213659286499, Validation Loss: 0.9395511746406555\n",
      "Step 285/1000, Loss: 0.7431090474128723, Validation Loss: 0.9389922022819519\n",
      "Step 286/1000, Loss: 0.7424967885017395, Validation Loss: 0.938433051109314\n",
      "Step 287/1000, Loss: 0.7418841123580933, Validation Loss: 0.9378734827041626\n",
      "Step 288/1000, Loss: 0.7412709593772888, Validation Loss: 0.9373134970664978\n",
      "Step 289/1000, Loss: 0.7406579256057739, Validation Loss: 0.9367533922195435\n",
      "Step 290/1000, Loss: 0.7400445342063904, Validation Loss: 0.9361929297447205\n",
      "Step 291/1000, Loss: 0.739430844783783, Validation Loss: 0.9356319904327393\n",
      "Step 292/1000, Loss: 0.7388169169425964, Validation Loss: 0.9350707530975342\n",
      "Step 293/1000, Loss: 0.7382027506828308, Validation Loss: 0.9345092177391052\n",
      "Step 294/1000, Loss: 0.7375884056091309, Validation Loss: 0.9339473843574524\n",
      "Step 295/1000, Loss: 0.7369737029075623, Validation Loss: 0.9333851337432861\n",
      "Step 296/1000, Loss: 0.7363587021827698, Validation Loss: 0.9328223466873169\n",
      "Step 297/1000, Loss: 0.7357435822486877, Validation Loss: 0.9322595000267029\n",
      "Step 298/1000, Loss: 0.7351282238960266, Validation Loss: 0.9316962361335754\n",
      "Step 299/1000, Loss: 0.7345125079154968, Validation Loss: 0.9311324954032898\n",
      "Step 300/1000, Loss: 0.7338965535163879, Validation Loss: 0.9305684566497803\n",
      "Step 301/1000, Loss: 0.7332804203033447, Validation Loss: 0.9300040602684021\n",
      "Step 302/1000, Loss: 0.7326639890670776, Validation Loss: 0.9297218322753906\n",
      "Step 303/1000, Loss: 0.7323557138442993, Validation Loss: 0.9294394850730896\n",
      "Step 304/1000, Loss: 0.7320476770401001, Validation Loss: 0.9291573762893677\n",
      "Step 305/1000, Loss: 0.7317396998405457, Validation Loss: 0.9288753867149353\n",
      "Step 306/1000, Loss: 0.7314318418502808, Validation Loss: 0.9285932183265686\n",
      "Step 307/1000, Loss: 0.7311241626739502, Validation Loss: 0.928311288356781\n",
      "Step 308/1000, Loss: 0.7308164238929749, Validation Loss: 0.928029477596283\n",
      "Step 309/1000, Loss: 0.7305089831352234, Validation Loss: 0.9277475476264954\n",
      "Step 310/1000, Loss: 0.730201780796051, Validation Loss: 0.9274658560752869\n",
      "Step 311/1000, Loss: 0.7298945188522339, Validation Loss: 0.9271841049194336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 312/1000, Loss: 0.7295874953269958, Validation Loss: 0.9269024133682251\n",
      "Step 313/1000, Loss: 0.7292805910110474, Validation Loss: 0.9266209602355957\n",
      "Step 314/1000, Loss: 0.7289738059043884, Validation Loss: 0.926339328289032\n",
      "Step 315/1000, Loss: 0.728667140007019, Validation Loss: 0.9260578155517578\n",
      "Step 316/1000, Loss: 0.7283605933189392, Validation Loss: 0.9257764220237732\n",
      "Step 317/1000, Loss: 0.7280541658401489, Validation Loss: 0.9254951477050781\n",
      "Step 318/1000, Loss: 0.7277479767799377, Validation Loss: 0.9252138137817383\n",
      "Step 319/1000, Loss: 0.7274417877197266, Validation Loss: 0.9249324798583984\n",
      "Step 320/1000, Loss: 0.7271358370780945, Validation Loss: 0.9246513247489929\n",
      "Step 321/1000, Loss: 0.726830005645752, Validation Loss: 0.9243701696395874\n",
      "Step 322/1000, Loss: 0.726524293422699, Validation Loss: 0.9240890145301819\n",
      "Step 323/1000, Loss: 0.7262186408042908, Validation Loss: 0.9238079190254211\n",
      "Step 324/1000, Loss: 0.7259132266044617, Validation Loss: 0.9235268831253052\n",
      "Step 325/1000, Loss: 0.7256078124046326, Validation Loss: 0.923245906829834\n",
      "Step 326/1000, Loss: 0.7253025770187378, Validation Loss: 0.922964870929718\n",
      "Step 327/1000, Loss: 0.7249974012374878, Validation Loss: 0.9226839542388916\n",
      "Step 328/1000, Loss: 0.7246923446655273, Validation Loss: 0.92240309715271\n",
      "Step 329/1000, Loss: 0.7243874073028564, Validation Loss: 0.9221222400665283\n",
      "Step 330/1000, Loss: 0.7240827083587646, Validation Loss: 0.9218413233757019\n",
      "Step 331/1000, Loss: 0.7237780094146729, Validation Loss: 0.9215604662895203\n",
      "Step 332/1000, Loss: 0.7234734296798706, Validation Loss: 0.9212795495986938\n",
      "Step 333/1000, Loss: 0.7231689691543579, Validation Loss: 0.9209988117218018\n",
      "Step 334/1000, Loss: 0.72286456823349, Validation Loss: 0.9207181334495544\n",
      "Step 335/1000, Loss: 0.7225602865219116, Validation Loss: 0.9204370975494385\n",
      "Step 336/1000, Loss: 0.7222561240196228, Validation Loss: 0.9201564192771912\n",
      "Step 337/1000, Loss: 0.7219520807266235, Validation Loss: 0.9198757410049438\n",
      "Step 338/1000, Loss: 0.721648097038269, Validation Loss: 0.919594943523407\n",
      "Step 339/1000, Loss: 0.7213441729545593, Validation Loss: 0.9193140268325806\n",
      "Step 340/1000, Loss: 0.7210404872894287, Validation Loss: 0.9190332889556885\n",
      "Step 341/1000, Loss: 0.7207368016242981, Validation Loss: 0.9187525510787964\n",
      "Step 342/1000, Loss: 0.720433235168457, Validation Loss: 0.9184718132019043\n",
      "Step 343/1000, Loss: 0.720129668712616, Validation Loss: 0.9181910753250122\n",
      "Step 344/1000, Loss: 0.7198262810707092, Validation Loss: 0.9179102182388306\n",
      "Step 345/1000, Loss: 0.7195228934288025, Validation Loss: 0.9176293611526489\n",
      "Step 346/1000, Loss: 0.7192197442054749, Validation Loss: 0.9173485636711121\n",
      "Step 347/1000, Loss: 0.7189164161682129, Validation Loss: 0.9170677065849304\n",
      "Step 348/1000, Loss: 0.71861332654953, Validation Loss: 0.916786789894104\n",
      "Step 349/1000, Loss: 0.7183104157447815, Validation Loss: 0.9165059924125671\n",
      "Step 350/1000, Loss: 0.718007504940033, Validation Loss: 0.9162251353263855\n",
      "Step 351/1000, Loss: 0.7177045941352844, Validation Loss: 0.9159441590309143\n",
      "Step 352/1000, Loss: 0.7174018025398254, Validation Loss: 0.9156631231307983\n",
      "Step 353/1000, Loss: 0.717099130153656, Validation Loss: 0.9153822660446167\n",
      "Step 354/1000, Loss: 0.7167963981628418, Validation Loss: 0.915101170539856\n",
      "Step 355/1000, Loss: 0.7164938449859619, Validation Loss: 0.9148201942443848\n",
      "Step 356/1000, Loss: 0.7161914110183716, Validation Loss: 0.9145391583442688\n",
      "Step 357/1000, Loss: 0.715889036655426, Validation Loss: 0.9142580032348633\n",
      "Step 358/1000, Loss: 0.7155866622924805, Validation Loss: 0.9139769077301025\n",
      "Step 359/1000, Loss: 0.7152842283248901, Validation Loss: 0.913695752620697\n",
      "Step 360/1000, Loss: 0.7149820923805237, Validation Loss: 0.9134144186973572\n",
      "Step 361/1000, Loss: 0.7146799564361572, Validation Loss: 0.9131332039833069\n",
      "Step 362/1000, Loss: 0.7143778204917908, Validation Loss: 0.9128519296646118\n",
      "Step 363/1000, Loss: 0.7140757441520691, Validation Loss: 0.9125704765319824\n",
      "Step 364/1000, Loss: 0.7137736678123474, Validation Loss: 0.9122891426086426\n",
      "Step 365/1000, Loss: 0.7134718298912048, Validation Loss: 0.9120075702667236\n",
      "Step 366/1000, Loss: 0.7131698727607727, Validation Loss: 0.9117261171340942\n",
      "Step 367/1000, Loss: 0.7128680348396301, Validation Loss: 0.9114446640014648\n",
      "Step 368/1000, Loss: 0.7125663161277771, Validation Loss: 0.911162793636322\n",
      "Step 369/1000, Loss: 0.7122645378112793, Validation Loss: 0.9108811020851135\n",
      "Step 370/1000, Loss: 0.7119628190994263, Validation Loss: 0.9105995297431946\n",
      "Step 371/1000, Loss: 0.7116612195968628, Validation Loss: 0.9103176593780518\n",
      "Step 372/1000, Loss: 0.7113596200942993, Validation Loss: 0.9100357294082642\n",
      "Step 373/1000, Loss: 0.7110580801963806, Validation Loss: 0.9097538590431213\n",
      "Step 374/1000, Loss: 0.7107566595077515, Validation Loss: 0.909471869468689\n",
      "Step 375/1000, Loss: 0.7104551792144775, Validation Loss: 0.909189760684967\n",
      "Step 376/1000, Loss: 0.7101537585258484, Validation Loss: 0.9089076519012451\n",
      "Step 377/1000, Loss: 0.7098524570465088, Validation Loss: 0.9086253643035889\n",
      "Step 378/1000, Loss: 0.7095510363578796, Validation Loss: 0.9083430767059326\n",
      "Step 379/1000, Loss: 0.7092497944831848, Validation Loss: 0.9080606698989868\n",
      "Step 380/1000, Loss: 0.7089486122131348, Validation Loss: 0.907778263092041\n",
      "Step 381/1000, Loss: 0.7086474895477295, Validation Loss: 0.9074957966804504\n",
      "Step 382/1000, Loss: 0.7083462476730347, Validation Loss: 0.9072131514549255\n",
      "Step 383/1000, Loss: 0.7080451250076294, Validation Loss: 0.9069305062294006\n",
      "Step 384/1000, Loss: 0.7077440619468689, Validation Loss: 0.9066477417945862\n",
      "Step 385/1000, Loss: 0.7074430584907532, Validation Loss: 0.906364917755127\n",
      "Step 386/1000, Loss: 0.7071420550346375, Validation Loss: 0.9060819149017334\n",
      "Step 387/1000, Loss: 0.7068411111831665, Validation Loss: 0.9057989716529846\n",
      "Step 388/1000, Loss: 0.7065401673316956, Validation Loss: 0.9055158495903015\n",
      "Step 389/1000, Loss: 0.7062392830848694, Validation Loss: 0.9052326083183289\n",
      "Step 390/1000, Loss: 0.7059383392333984, Validation Loss: 0.9049493670463562\n",
      "Step 391/1000, Loss: 0.705637514591217, Validation Loss: 0.904666006565094\n",
      "Step 392/1000, Loss: 0.7053366899490356, Validation Loss: 0.9043826460838318\n",
      "Step 393/1000, Loss: 0.705035924911499, Validation Loss: 0.9040988683700562\n",
      "Step 394/1000, Loss: 0.7047351598739624, Validation Loss: 0.9038152694702148\n",
      "Step 395/1000, Loss: 0.7044344544410706, Validation Loss: 0.9035316109657288\n",
      "Step 396/1000, Loss: 0.7041336894035339, Validation Loss: 0.9032478332519531\n",
      "Step 397/1000, Loss: 0.7038330435752869, Validation Loss: 0.9029638767242432\n",
      "Step 398/1000, Loss: 0.7035323977470398, Validation Loss: 0.9026797413825989\n",
      "Step 399/1000, Loss: 0.7032318711280823, Validation Loss: 0.9023955464363098\n",
      "Step 400/1000, Loss: 0.7029311656951904, Validation Loss: 0.9021114110946655\n",
      "Step 401/1000, Loss: 0.7026305794715881, Validation Loss: 0.9018269777297974\n",
      "Step 402/1000, Loss: 0.7023300528526306, Validation Loss: 0.9016847610473633\n",
      "Step 403/1000, Loss: 0.7021797895431519, Validation Loss: 0.9015425443649292\n",
      "Step 404/1000, Loss: 0.7020296454429626, Validation Loss: 0.9014003276824951\n",
      "Step 405/1000, Loss: 0.7018795013427734, Validation Loss: 0.9012582302093506\n",
      "Step 406/1000, Loss: 0.701729416847229, Validation Loss: 0.9011160135269165\n",
      "Step 407/1000, Loss: 0.7015792727470398, Validation Loss: 0.9009737968444824\n",
      "Step 408/1000, Loss: 0.7014294266700745, Validation Loss: 0.9008316397666931\n",
      "Step 409/1000, Loss: 0.7012795209884644, Validation Loss: 0.900689423084259\n",
      "Step 410/1000, Loss: 0.7011296153068542, Validation Loss: 0.9005472660064697\n",
      "Step 411/1000, Loss: 0.7009797692298889, Validation Loss: 0.9004050493240356\n",
      "Step 412/1000, Loss: 0.7008300423622131, Validation Loss: 0.9002629518508911\n",
      "Step 413/1000, Loss: 0.7006803154945374, Validation Loss: 0.9001208543777466\n",
      "Step 414/1000, Loss: 0.7005306482315063, Validation Loss: 0.8999786972999573\n",
      "Step 415/1000, Loss: 0.7003811597824097, Validation Loss: 0.8998365998268127\n",
      "Step 416/1000, Loss: 0.7002315521240234, Validation Loss: 0.8996944427490234\n",
      "Step 417/1000, Loss: 0.7000821828842163, Validation Loss: 0.8995524048805237\n",
      "Step 418/1000, Loss: 0.6999326944351196, Validation Loss: 0.8994102478027344\n",
      "Step 419/1000, Loss: 0.6997832655906677, Validation Loss: 0.8992681503295898\n",
      "Step 420/1000, Loss: 0.6996339559555054, Validation Loss: 0.8991260528564453\n",
      "Step 421/1000, Loss: 0.6994845867156982, Validation Loss: 0.8989840149879456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 422/1000, Loss: 0.6993353962898254, Validation Loss: 0.8988419771194458\n",
      "Step 423/1000, Loss: 0.6991862654685974, Validation Loss: 0.8986998200416565\n",
      "Step 424/1000, Loss: 0.6990371346473694, Validation Loss: 0.8985577821731567\n",
      "Step 425/1000, Loss: 0.6988880634307861, Validation Loss: 0.8984158039093018\n",
      "Step 426/1000, Loss: 0.6987389922142029, Validation Loss: 0.898273766040802\n",
      "Step 427/1000, Loss: 0.698590099811554, Validation Loss: 0.8981317281723022\n",
      "Step 428/1000, Loss: 0.6984410285949707, Validation Loss: 0.8979896306991577\n",
      "Step 429/1000, Loss: 0.6982921957969666, Validation Loss: 0.8978476524353027\n",
      "Step 430/1000, Loss: 0.6981433033943176, Validation Loss: 0.8977056741714478\n",
      "Step 431/1000, Loss: 0.6979945302009583, Validation Loss: 0.8975638151168823\n",
      "Step 432/1000, Loss: 0.6978458166122437, Validation Loss: 0.8974217176437378\n",
      "Step 433/1000, Loss: 0.6976971626281738, Validation Loss: 0.8972797393798828\n",
      "Step 434/1000, Loss: 0.6975483894348145, Validation Loss: 0.8971378207206726\n",
      "Step 435/1000, Loss: 0.6973997950553894, Validation Loss: 0.8969958424568176\n",
      "Step 436/1000, Loss: 0.6972512602806091, Validation Loss: 0.8968538641929626\n",
      "Step 437/1000, Loss: 0.6971026062965393, Validation Loss: 0.8967118859291077\n",
      "Step 438/1000, Loss: 0.6969541907310486, Validation Loss: 0.8965697884559631\n",
      "Step 439/1000, Loss: 0.6968057155609131, Validation Loss: 0.8964279294013977\n",
      "Step 440/1000, Loss: 0.6966573596000671, Validation Loss: 0.8962859511375427\n",
      "Step 441/1000, Loss: 0.6965089440345764, Validation Loss: 0.8961440324783325\n",
      "Step 442/1000, Loss: 0.6963606476783752, Validation Loss: 0.8960020542144775\n",
      "Step 443/1000, Loss: 0.6962122917175293, Validation Loss: 0.8958600759506226\n",
      "Step 444/1000, Loss: 0.6960640549659729, Validation Loss: 0.8957181572914124\n",
      "Step 445/1000, Loss: 0.6959158182144165, Validation Loss: 0.8955761790275574\n",
      "Step 446/1000, Loss: 0.6957676410675049, Validation Loss: 0.8954343199729919\n",
      "Step 447/1000, Loss: 0.6956196427345276, Validation Loss: 0.895292341709137\n",
      "Step 448/1000, Loss: 0.695471465587616, Validation Loss: 0.895150363445282\n",
      "Step 449/1000, Loss: 0.6953233480453491, Validation Loss: 0.895008385181427\n",
      "Step 450/1000, Loss: 0.6951754093170166, Validation Loss: 0.8948664665222168\n",
      "Step 451/1000, Loss: 0.6950274109840393, Validation Loss: 0.8947244882583618\n",
      "Step 452/1000, Loss: 0.6948795318603516, Validation Loss: 0.8945825099945068\n",
      "Step 453/1000, Loss: 0.6947314739227295, Validation Loss: 0.8944405317306519\n",
      "Step 454/1000, Loss: 0.6945836544036865, Validation Loss: 0.8942987322807312\n",
      "Step 455/1000, Loss: 0.6944357752799988, Validation Loss: 0.8941567540168762\n",
      "Step 456/1000, Loss: 0.6942880153656006, Validation Loss: 0.8940147757530212\n",
      "Step 457/1000, Loss: 0.6941401958465576, Validation Loss: 0.8938727378845215\n",
      "Step 458/1000, Loss: 0.693992555141449, Validation Loss: 0.8937308192253113\n",
      "Step 459/1000, Loss: 0.6938447952270508, Validation Loss: 0.8935889005661011\n",
      "Step 460/1000, Loss: 0.6936972141265869, Validation Loss: 0.8934468626976013\n",
      "Step 461/1000, Loss: 0.6935495138168335, Validation Loss: 0.8933048248291016\n",
      "Step 462/1000, Loss: 0.6934018731117249, Validation Loss: 0.8931629657745361\n",
      "Step 463/1000, Loss: 0.693254292011261, Validation Loss: 0.8930209279060364\n",
      "Step 464/1000, Loss: 0.6931067109107971, Validation Loss: 0.892879068851471\n",
      "Step 465/1000, Loss: 0.692959189414978, Validation Loss: 0.8927369713783264\n",
      "Step 466/1000, Loss: 0.6928116679191589, Validation Loss: 0.8925949931144714\n",
      "Step 467/1000, Loss: 0.6926643252372742, Validation Loss: 0.8924529552459717\n",
      "Step 468/1000, Loss: 0.6925168037414551, Validation Loss: 0.8923109769821167\n",
      "Step 469/1000, Loss: 0.6923694610595703, Validation Loss: 0.8921689391136169\n",
      "Step 470/1000, Loss: 0.6922221183776855, Validation Loss: 0.892026960849762\n",
      "Step 471/1000, Loss: 0.6920748353004456, Validation Loss: 0.891884982585907\n",
      "Step 472/1000, Loss: 0.691927433013916, Validation Loss: 0.8917428255081177\n",
      "Step 473/1000, Loss: 0.6917802095413208, Validation Loss: 0.8916007876396179\n",
      "Step 474/1000, Loss: 0.6916329264640808, Validation Loss: 0.8914587497711182\n",
      "Step 475/1000, Loss: 0.6914856433868408, Validation Loss: 0.8913167715072632\n",
      "Step 476/1000, Loss: 0.6913385391235352, Validation Loss: 0.8911746144294739\n",
      "Step 477/1000, Loss: 0.6911913752555847, Validation Loss: 0.8910325765609741\n",
      "Step 478/1000, Loss: 0.6910443305969238, Validation Loss: 0.8908905386924744\n",
      "Step 479/1000, Loss: 0.6908971071243286, Validation Loss: 0.8907483816146851\n",
      "Step 480/1000, Loss: 0.6907500624656677, Validation Loss: 0.8906063437461853\n",
      "Step 481/1000, Loss: 0.6906030178070068, Validation Loss: 0.8904641270637512\n",
      "Step 482/1000, Loss: 0.6904559135437012, Validation Loss: 0.8903219699859619\n",
      "Step 483/1000, Loss: 0.6903088688850403, Validation Loss: 0.8901799321174622\n",
      "Step 484/1000, Loss: 0.690161943435669, Validation Loss: 0.8900377154350281\n",
      "Step 485/1000, Loss: 0.6900148987770081, Validation Loss: 0.8898956775665283\n",
      "Step 486/1000, Loss: 0.6898680329322815, Validation Loss: 0.889753520488739\n",
      "Step 487/1000, Loss: 0.6897211670875549, Validation Loss: 0.8896114230155945\n",
      "Step 488/1000, Loss: 0.6895743608474731, Validation Loss: 0.8894691467285156\n",
      "Step 489/1000, Loss: 0.6894274353981018, Validation Loss: 0.8893269896507263\n",
      "Step 490/1000, Loss: 0.68928062915802, Validation Loss: 0.8891847729682922\n",
      "Step 491/1000, Loss: 0.6891337633132935, Validation Loss: 0.8890425562858582\n",
      "Step 492/1000, Loss: 0.6889870166778564, Validation Loss: 0.8889003992080688\n",
      "Step 493/1000, Loss: 0.6888402104377747, Validation Loss: 0.8887581825256348\n",
      "Step 494/1000, Loss: 0.6886934638023376, Validation Loss: 0.8886159658432007\n",
      "Step 495/1000, Loss: 0.6885468363761902, Validation Loss: 0.8884736895561218\n",
      "Step 496/1000, Loss: 0.6884000897407532, Validation Loss: 0.8883314728736877\n",
      "Step 497/1000, Loss: 0.6882534623146057, Validation Loss: 0.8881891965866089\n",
      "Step 498/1000, Loss: 0.6881067752838135, Validation Loss: 0.8880469799041748\n",
      "Step 499/1000, Loss: 0.687960147857666, Validation Loss: 0.8879045844078064\n",
      "Step 500/1000, Loss: 0.6878135800361633, Validation Loss: 0.8877622485160828\n",
      "Step 501/1000, Loss: 0.6876668334007263, Validation Loss: 0.8876200318336487\n",
      "Step 502/1000, Loss: 0.6875203251838684, Validation Loss: 0.8875488638877869\n",
      "Step 503/1000, Loss: 0.6874471306800842, Validation Loss: 0.8874776363372803\n",
      "Step 504/1000, Loss: 0.6873738169670105, Validation Loss: 0.8874064683914185\n",
      "Step 505/1000, Loss: 0.6873005032539368, Validation Loss: 0.8873353600502014\n",
      "Step 506/1000, Loss: 0.6872273683547974, Validation Loss: 0.8872642517089844\n",
      "Step 507/1000, Loss: 0.6871541738510132, Validation Loss: 0.8871931433677673\n",
      "Step 508/1000, Loss: 0.687080979347229, Validation Loss: 0.8871219754219055\n",
      "Step 509/1000, Loss: 0.6870077848434448, Validation Loss: 0.887050986289978\n",
      "Step 510/1000, Loss: 0.6869347095489502, Validation Loss: 0.8869797587394714\n",
      "Step 511/1000, Loss: 0.6868615746498108, Validation Loss: 0.8869086503982544\n",
      "Step 512/1000, Loss: 0.6867884993553162, Validation Loss: 0.8868376612663269\n",
      "Step 513/1000, Loss: 0.6867153644561768, Validation Loss: 0.8867666125297546\n",
      "Step 514/1000, Loss: 0.6866423487663269, Validation Loss: 0.8866956830024719\n",
      "Step 515/1000, Loss: 0.6865692734718323, Validation Loss: 0.8866243958473206\n",
      "Step 516/1000, Loss: 0.6864963173866272, Validation Loss: 0.8865534663200378\n",
      "Step 517/1000, Loss: 0.6864231824874878, Validation Loss: 0.8864823579788208\n",
      "Step 518/1000, Loss: 0.6863501667976379, Validation Loss: 0.8864114880561829\n",
      "Step 519/1000, Loss: 0.6862772703170776, Validation Loss: 0.8863404393196106\n",
      "Step 520/1000, Loss: 0.6862043142318726, Validation Loss: 0.8862693309783936\n",
      "Step 521/1000, Loss: 0.6861312389373779, Validation Loss: 0.8861984014511108\n",
      "Step 522/1000, Loss: 0.6860583424568176, Validation Loss: 0.8861274719238281\n",
      "Step 523/1000, Loss: 0.6859854459762573, Validation Loss: 0.8860563635826111\n",
      "Step 524/1000, Loss: 0.685912549495697, Validation Loss: 0.8859853148460388\n",
      "Step 525/1000, Loss: 0.6858395934104919, Validation Loss: 0.8859144449234009\n",
      "Step 526/1000, Loss: 0.6857668161392212, Validation Loss: 0.8858435153961182\n",
      "Step 527/1000, Loss: 0.6856939196586609, Validation Loss: 0.8857725262641907\n",
      "Step 528/1000, Loss: 0.6856210827827454, Validation Loss: 0.885701596736908\n",
      "Step 529/1000, Loss: 0.6855482459068298, Validation Loss: 0.88563072681427\n",
      "Step 530/1000, Loss: 0.6854753494262695, Validation Loss: 0.8855597972869873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 531/1000, Loss: 0.6854026317596436, Validation Loss: 0.8854889273643494\n",
      "Step 532/1000, Loss: 0.6853298544883728, Validation Loss: 0.8854181170463562\n",
      "Step 533/1000, Loss: 0.6852571368217468, Validation Loss: 0.8853471875190735\n",
      "Step 534/1000, Loss: 0.6851843595504761, Validation Loss: 0.8852763772010803\n",
      "Step 535/1000, Loss: 0.6851118206977844, Validation Loss: 0.8852054476737976\n",
      "Step 536/1000, Loss: 0.6850389838218689, Validation Loss: 0.8851346373558044\n",
      "Step 537/1000, Loss: 0.6849663257598877, Validation Loss: 0.8850638270378113\n",
      "Step 538/1000, Loss: 0.6848936080932617, Validation Loss: 0.8849929571151733\n",
      "Step 539/1000, Loss: 0.6848209500312805, Validation Loss: 0.8849221467971802\n",
      "Step 540/1000, Loss: 0.6847482919692993, Validation Loss: 0.8848514556884766\n",
      "Step 541/1000, Loss: 0.6846756935119629, Validation Loss: 0.8847805261611938\n",
      "Step 542/1000, Loss: 0.6846030354499817, Validation Loss: 0.8847097158432007\n",
      "Step 543/1000, Loss: 0.6845304369926453, Validation Loss: 0.8846389055252075\n",
      "Step 544/1000, Loss: 0.6844578385353088, Validation Loss: 0.8845680952072144\n",
      "Step 545/1000, Loss: 0.6843852400779724, Validation Loss: 0.8844974040985107\n",
      "Step 546/1000, Loss: 0.6843127012252808, Validation Loss: 0.8844265937805176\n",
      "Step 547/1000, Loss: 0.6842402219772339, Validation Loss: 0.8843557834625244\n",
      "Step 548/1000, Loss: 0.6841676235198975, Validation Loss: 0.884285032749176\n",
      "Step 549/1000, Loss: 0.6840950846672058, Validation Loss: 0.8842142820358276\n",
      "Step 550/1000, Loss: 0.6840226054191589, Validation Loss: 0.884143590927124\n",
      "Step 551/1000, Loss: 0.6839500069618225, Validation Loss: 0.8840728402137756\n",
      "Step 552/1000, Loss: 0.6838776469230652, Validation Loss: 0.8840020895004272\n",
      "Step 553/1000, Loss: 0.6838051676750183, Validation Loss: 0.8839313387870789\n",
      "Step 554/1000, Loss: 0.6837326288223267, Validation Loss: 0.8838606476783752\n",
      "Step 555/1000, Loss: 0.6836602687835693, Validation Loss: 0.8837900161743164\n",
      "Step 556/1000, Loss: 0.6835877895355225, Validation Loss: 0.883719265460968\n",
      "Step 557/1000, Loss: 0.6835153698921204, Validation Loss: 0.8836485743522644\n",
      "Step 558/1000, Loss: 0.683443009853363, Validation Loss: 0.883577823638916\n",
      "Step 559/1000, Loss: 0.6833704710006714, Validation Loss: 0.8835071325302124\n",
      "Step 560/1000, Loss: 0.6832981109619141, Validation Loss: 0.883436381816864\n",
      "Step 561/1000, Loss: 0.6832257509231567, Validation Loss: 0.8833657503128052\n",
      "Step 562/1000, Loss: 0.6831533908843994, Validation Loss: 0.8832951188087463\n",
      "Step 563/1000, Loss: 0.6830809712409973, Validation Loss: 0.8832244873046875\n",
      "Step 564/1000, Loss: 0.6830086708068848, Validation Loss: 0.8831538558006287\n",
      "Step 565/1000, Loss: 0.6829363107681274, Validation Loss: 0.8830831050872803\n",
      "Step 566/1000, Loss: 0.6828639507293701, Validation Loss: 0.8830124735832214\n",
      "Step 567/1000, Loss: 0.6827917695045471, Validation Loss: 0.8829418420791626\n",
      "Step 568/1000, Loss: 0.6827194690704346, Validation Loss: 0.8828712701797485\n",
      "Step 569/1000, Loss: 0.6826471090316772, Validation Loss: 0.8828006982803345\n",
      "Step 570/1000, Loss: 0.6825749278068542, Validation Loss: 0.8827300071716309\n",
      "Step 571/1000, Loss: 0.6825026273727417, Validation Loss: 0.8826594352722168\n",
      "Step 572/1000, Loss: 0.6824304461479187, Validation Loss: 0.882588803768158\n",
      "Step 573/1000, Loss: 0.6823581457138062, Validation Loss: 0.8825182318687439\n",
      "Step 574/1000, Loss: 0.6822859644889832, Validation Loss: 0.8824475407600403\n",
      "Step 575/1000, Loss: 0.6822137236595154, Validation Loss: 0.882377028465271\n",
      "Step 576/1000, Loss: 0.6821416020393372, Validation Loss: 0.8823063969612122\n",
      "Step 577/1000, Loss: 0.6820693612098694, Validation Loss: 0.8822358250617981\n",
      "Step 578/1000, Loss: 0.6819972395896912, Validation Loss: 0.882165253162384\n",
      "Step 579/1000, Loss: 0.6819249987602234, Validation Loss: 0.8820946216583252\n",
      "Step 580/1000, Loss: 0.6818528175354004, Validation Loss: 0.8820239901542664\n",
      "Step 581/1000, Loss: 0.6817806959152222, Validation Loss: 0.8819535374641418\n",
      "Step 582/1000, Loss: 0.681708574295044, Validation Loss: 0.8818829655647278\n",
      "Step 583/1000, Loss: 0.6816363334655762, Validation Loss: 0.8818123936653137\n",
      "Step 584/1000, Loss: 0.681564211845398, Validation Loss: 0.8817417621612549\n",
      "Step 585/1000, Loss: 0.6814922094345093, Validation Loss: 0.8816712498664856\n",
      "Step 586/1000, Loss: 0.681420087814331, Validation Loss: 0.8816006183624268\n",
      "Step 587/1000, Loss: 0.6813479065895081, Validation Loss: 0.8815300464630127\n",
      "Step 588/1000, Loss: 0.6812757849693298, Validation Loss: 0.8814594745635986\n",
      "Step 589/1000, Loss: 0.6812038421630859, Validation Loss: 0.8813890218734741\n",
      "Step 590/1000, Loss: 0.6811317205429077, Validation Loss: 0.8813184499740601\n",
      "Step 591/1000, Loss: 0.6810596585273743, Validation Loss: 0.881247878074646\n",
      "Step 592/1000, Loss: 0.6809875965118408, Validation Loss: 0.8811774253845215\n",
      "Step 593/1000, Loss: 0.6809155941009521, Validation Loss: 0.8811068534851074\n",
      "Step 594/1000, Loss: 0.6808435916900635, Validation Loss: 0.8810363411903381\n",
      "Step 595/1000, Loss: 0.6807714700698853, Validation Loss: 0.8809658885002136\n",
      "Step 596/1000, Loss: 0.6806995272636414, Validation Loss: 0.8808953166007996\n",
      "Step 597/1000, Loss: 0.6806275248527527, Validation Loss: 0.8808247447013855\n",
      "Step 598/1000, Loss: 0.6805554628372192, Validation Loss: 0.880754292011261\n",
      "Step 599/1000, Loss: 0.6804835796356201, Validation Loss: 0.8806837797164917\n",
      "Step 600/1000, Loss: 0.6804115176200867, Validation Loss: 0.8806132078170776\n",
      "Step 601/1000, Loss: 0.6803396344184875, Validation Loss: 0.8805428743362427\n",
      "Step 602/1000, Loss: 0.6802676916122437, Validation Loss: 0.8805076479911804\n",
      "Step 603/1000, Loss: 0.6802316904067993, Validation Loss: 0.8804722428321838\n",
      "Step 604/1000, Loss: 0.680195689201355, Validation Loss: 0.8804370760917664\n",
      "Step 605/1000, Loss: 0.6801598072052002, Validation Loss: 0.8804019093513489\n",
      "Step 606/1000, Loss: 0.6801238059997559, Validation Loss: 0.8803666830062866\n",
      "Step 607/1000, Loss: 0.6800879240036011, Validation Loss: 0.8803315162658691\n",
      "Step 608/1000, Loss: 0.6800519824028015, Validation Loss: 0.8802961707115173\n",
      "Step 609/1000, Loss: 0.680016040802002, Validation Loss: 0.8802611231803894\n",
      "Step 610/1000, Loss: 0.6799800992012024, Validation Loss: 0.8802258372306824\n",
      "Step 611/1000, Loss: 0.6799442172050476, Validation Loss: 0.8801906108856201\n",
      "Step 612/1000, Loss: 0.679908275604248, Validation Loss: 0.8801553845405579\n",
      "Step 613/1000, Loss: 0.6798723936080933, Validation Loss: 0.8801202178001404\n",
      "Step 614/1000, Loss: 0.6798364520072937, Validation Loss: 0.8800851106643677\n",
      "Step 615/1000, Loss: 0.6798005700111389, Validation Loss: 0.8800498247146606\n",
      "Step 616/1000, Loss: 0.6797646880149841, Validation Loss: 0.8800146579742432\n",
      "Step 617/1000, Loss: 0.6797287464141846, Validation Loss: 0.8799793720245361\n",
      "Step 618/1000, Loss: 0.6796927452087402, Validation Loss: 0.8799443244934082\n",
      "Step 619/1000, Loss: 0.6796569228172302, Validation Loss: 0.8799091577529907\n",
      "Step 620/1000, Loss: 0.6796211004257202, Validation Loss: 0.8798739314079285\n",
      "Step 621/1000, Loss: 0.6795852184295654, Validation Loss: 0.8798388242721558\n",
      "Step 622/1000, Loss: 0.6795493960380554, Validation Loss: 0.8798036575317383\n",
      "Step 623/1000, Loss: 0.6795134544372559, Validation Loss: 0.8797684907913208\n",
      "Step 624/1000, Loss: 0.6794776916503906, Validation Loss: 0.8797333836555481\n",
      "Step 625/1000, Loss: 0.6794418692588806, Validation Loss: 0.8796983361244202\n",
      "Step 626/1000, Loss: 0.6794059872627258, Validation Loss: 0.8796630501747131\n",
      "Step 627/1000, Loss: 0.6793700456619263, Validation Loss: 0.8796278834342957\n",
      "Step 628/1000, Loss: 0.6793342232704163, Validation Loss: 0.8795927166938782\n",
      "Step 629/1000, Loss: 0.6792984008789062, Validation Loss: 0.8795576691627502\n",
      "Step 630/1000, Loss: 0.6792627573013306, Validation Loss: 0.8795225620269775\n",
      "Step 631/1000, Loss: 0.679226815700531, Validation Loss: 0.8794874548912048\n",
      "Step 632/1000, Loss: 0.679190993309021, Validation Loss: 0.8794522881507874\n",
      "Step 633/1000, Loss: 0.6791552305221558, Validation Loss: 0.8794172406196594\n",
      "Step 634/1000, Loss: 0.6791194081306458, Validation Loss: 0.8793820738792419\n",
      "Step 635/1000, Loss: 0.6790835857391357, Validation Loss: 0.879347026348114\n",
      "Step 636/1000, Loss: 0.6790478229522705, Validation Loss: 0.8793118596076965\n",
      "Step 637/1000, Loss: 0.6790120005607605, Validation Loss: 0.879276692867279\n",
      "Step 638/1000, Loss: 0.6789762377738953, Validation Loss: 0.8792417645454407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 639/1000, Loss: 0.6789404153823853, Validation Loss: 0.8792067766189575\n",
      "Step 640/1000, Loss: 0.6789047718048096, Validation Loss: 0.8791716694831848\n",
      "Step 641/1000, Loss: 0.6788690090179443, Validation Loss: 0.8791364431381226\n",
      "Step 642/1000, Loss: 0.6788331866264343, Validation Loss: 0.8791014552116394\n",
      "Step 643/1000, Loss: 0.6787973046302795, Validation Loss: 0.8790664076805115\n",
      "Step 644/1000, Loss: 0.6787616014480591, Validation Loss: 0.879031240940094\n",
      "Step 645/1000, Loss: 0.6787258386611938, Validation Loss: 0.8789962530136108\n",
      "Step 646/1000, Loss: 0.6786900758743286, Validation Loss: 0.8789611458778381\n",
      "Step 647/1000, Loss: 0.6786543130874634, Validation Loss: 0.8789262175559998\n",
      "Step 648/1000, Loss: 0.6786186695098877, Validation Loss: 0.8788910508155823\n",
      "Step 649/1000, Loss: 0.6785829067230225, Validation Loss: 0.8788561224937439\n",
      "Step 650/1000, Loss: 0.6785471439361572, Validation Loss: 0.8788209557533264\n",
      "Step 651/1000, Loss: 0.6785114407539368, Validation Loss: 0.8787859082221985\n",
      "Step 652/1000, Loss: 0.6784757375717163, Validation Loss: 0.8787508606910706\n",
      "Step 653/1000, Loss: 0.6784399747848511, Validation Loss: 0.8787158131599426\n",
      "Step 654/1000, Loss: 0.6784042716026306, Validation Loss: 0.8786808252334595\n",
      "Step 655/1000, Loss: 0.6783686280250549, Validation Loss: 0.8786457777023315\n",
      "Step 656/1000, Loss: 0.6783328652381897, Validation Loss: 0.8786108493804932\n",
      "Step 657/1000, Loss: 0.6782971024513245, Validation Loss: 0.8785756826400757\n",
      "Step 658/1000, Loss: 0.6782614588737488, Validation Loss: 0.8785407543182373\n",
      "Step 659/1000, Loss: 0.6782257556915283, Validation Loss: 0.8785056471824646\n",
      "Step 660/1000, Loss: 0.6781901121139526, Validation Loss: 0.8784707188606262\n",
      "Step 661/1000, Loss: 0.6781544089317322, Validation Loss: 0.8784357905387878\n",
      "Step 662/1000, Loss: 0.6781187057495117, Validation Loss: 0.8784006834030151\n",
      "Step 663/1000, Loss: 0.678083062171936, Validation Loss: 0.878365695476532\n",
      "Step 664/1000, Loss: 0.6780472993850708, Validation Loss: 0.8783307671546936\n",
      "Step 665/1000, Loss: 0.6780117154121399, Validation Loss: 0.8782958388328552\n",
      "Step 666/1000, Loss: 0.6779760122299194, Validation Loss: 0.8782607913017273\n",
      "Step 667/1000, Loss: 0.6779404282569885, Validation Loss: 0.8782258033752441\n",
      "Step 668/1000, Loss: 0.6779047846794128, Validation Loss: 0.8781908750534058\n",
      "Step 669/1000, Loss: 0.6778690814971924, Validation Loss: 0.8781559467315674\n",
      "Step 670/1000, Loss: 0.6778334975242615, Validation Loss: 0.878121018409729\n",
      "Step 671/1000, Loss: 0.6777979135513306, Validation Loss: 0.8780859708786011\n",
      "Step 672/1000, Loss: 0.6777622699737549, Validation Loss: 0.8780511617660522\n",
      "Step 673/1000, Loss: 0.6777266263961792, Validation Loss: 0.8780162334442139\n",
      "Step 674/1000, Loss: 0.6776910424232483, Validation Loss: 0.8779811859130859\n",
      "Step 675/1000, Loss: 0.6776553988456726, Validation Loss: 0.8779463171958923\n",
      "Step 676/1000, Loss: 0.6776197552680969, Validation Loss: 0.8779112696647644\n",
      "Step 677/1000, Loss: 0.6775840520858765, Validation Loss: 0.8778764009475708\n",
      "Step 678/1000, Loss: 0.6775485277175903, Validation Loss: 0.877841591835022\n",
      "Step 679/1000, Loss: 0.6775129437446594, Validation Loss: 0.877806544303894\n",
      "Step 680/1000, Loss: 0.6774773597717285, Validation Loss: 0.8777716755867004\n",
      "Step 681/1000, Loss: 0.6774418354034424, Validation Loss: 0.8777367472648621\n",
      "Step 682/1000, Loss: 0.6774062514305115, Validation Loss: 0.8777017593383789\n",
      "Step 683/1000, Loss: 0.6773706674575806, Validation Loss: 0.8776669502258301\n",
      "Step 684/1000, Loss: 0.6773350238800049, Validation Loss: 0.8776319622993469\n",
      "Step 685/1000, Loss: 0.677299439907074, Validation Loss: 0.8775970339775085\n",
      "Step 686/1000, Loss: 0.6772639155387878, Validation Loss: 0.8775621652603149\n",
      "Step 687/1000, Loss: 0.6772283911705017, Validation Loss: 0.8775272965431213\n",
      "Step 688/1000, Loss: 0.6771928071975708, Validation Loss: 0.877492368221283\n",
      "Step 689/1000, Loss: 0.6771572232246399, Validation Loss: 0.8774574398994446\n",
      "Step 690/1000, Loss: 0.6771216988563538, Validation Loss: 0.8774225115776062\n",
      "Step 691/1000, Loss: 0.6770861148834229, Validation Loss: 0.8773875832557678\n",
      "Step 692/1000, Loss: 0.6770505309104919, Validation Loss: 0.8773527145385742\n",
      "Step 693/1000, Loss: 0.6770150065422058, Validation Loss: 0.8773178458213806\n",
      "Step 694/1000, Loss: 0.6769794821739197, Validation Loss: 0.8772828578948975\n",
      "Step 695/1000, Loss: 0.6769437789916992, Validation Loss: 0.8772480487823486\n",
      "Step 696/1000, Loss: 0.6769083738327026, Validation Loss: 0.8772131204605103\n",
      "Step 697/1000, Loss: 0.676872730255127, Validation Loss: 0.8771781921386719\n",
      "Step 698/1000, Loss: 0.6768372058868408, Validation Loss: 0.877143383026123\n",
      "Step 699/1000, Loss: 0.6768018007278442, Validation Loss: 0.8771085739135742\n",
      "Step 700/1000, Loss: 0.6767662167549133, Validation Loss: 0.8770734667778015\n",
      "Step 701/1000, Loss: 0.6767306923866272, Validation Loss: 0.8770386576652527\n",
      "Step 702/1000, Loss: 0.6766951680183411, Validation Loss: 0.877021312713623\n",
      "Step 703/1000, Loss: 0.676677405834198, Validation Loss: 0.8770038485527039\n",
      "Step 704/1000, Loss: 0.6766596436500549, Validation Loss: 0.8769863247871399\n",
      "Step 705/1000, Loss: 0.6766418814659119, Validation Loss: 0.8769689202308655\n",
      "Step 706/1000, Loss: 0.6766241192817688, Validation Loss: 0.8769515156745911\n",
      "Step 707/1000, Loss: 0.676606297492981, Validation Loss: 0.8769340515136719\n",
      "Step 708/1000, Loss: 0.6765885353088379, Validation Loss: 0.8769166469573975\n",
      "Step 709/1000, Loss: 0.6765708327293396, Validation Loss: 0.876899242401123\n",
      "Step 710/1000, Loss: 0.6765530705451965, Validation Loss: 0.8768817782402039\n",
      "Step 711/1000, Loss: 0.6765354871749878, Validation Loss: 0.876864492893219\n",
      "Step 712/1000, Loss: 0.6765176057815552, Validation Loss: 0.8768470287322998\n",
      "Step 713/1000, Loss: 0.6764998435974121, Validation Loss: 0.8768296837806702\n",
      "Step 714/1000, Loss: 0.676482081413269, Validation Loss: 0.876812219619751\n",
      "Step 715/1000, Loss: 0.6764644384384155, Validation Loss: 0.8767948150634766\n",
      "Step 716/1000, Loss: 0.6764466762542725, Validation Loss: 0.8767774701118469\n",
      "Step 717/1000, Loss: 0.6764289736747742, Validation Loss: 0.876759946346283\n",
      "Step 718/1000, Loss: 0.6764111518859863, Validation Loss: 0.8767426013946533\n",
      "Step 719/1000, Loss: 0.676393449306488, Validation Loss: 0.8767252564430237\n",
      "Step 720/1000, Loss: 0.676375687122345, Validation Loss: 0.8767076730728149\n",
      "Step 721/1000, Loss: 0.6763579249382019, Validation Loss: 0.8766903877258301\n",
      "Step 722/1000, Loss: 0.6763403415679932, Validation Loss: 0.8766729831695557\n",
      "Step 723/1000, Loss: 0.6763225793838501, Validation Loss: 0.8766555190086365\n",
      "Step 724/1000, Loss: 0.676304817199707, Validation Loss: 0.8766381740570068\n",
      "Step 725/1000, Loss: 0.676287055015564, Validation Loss: 0.8766207695007324\n",
      "Step 726/1000, Loss: 0.6762693524360657, Validation Loss: 0.8766034841537476\n",
      "Step 727/1000, Loss: 0.6762517094612122, Validation Loss: 0.8765859603881836\n",
      "Step 728/1000, Loss: 0.6762339472770691, Validation Loss: 0.876568615436554\n",
      "Step 729/1000, Loss: 0.676216185092926, Validation Loss: 0.8765512704849243\n",
      "Step 730/1000, Loss: 0.6761985421180725, Validation Loss: 0.8765338063240051\n",
      "Step 731/1000, Loss: 0.6761807799339294, Validation Loss: 0.8765164017677307\n",
      "Step 732/1000, Loss: 0.6761631369590759, Validation Loss: 0.8764989972114563\n",
      "Step 733/1000, Loss: 0.6761453747749329, Validation Loss: 0.8764817118644714\n",
      "Step 734/1000, Loss: 0.6761277914047241, Validation Loss: 0.876464307308197\n",
      "Step 735/1000, Loss: 0.6761099100112915, Validation Loss: 0.8764469623565674\n",
      "Step 736/1000, Loss: 0.676092267036438, Validation Loss: 0.8764294981956482\n",
      "Step 737/1000, Loss: 0.6760745644569397, Validation Loss: 0.8764120936393738\n",
      "Step 738/1000, Loss: 0.6760568022727966, Validation Loss: 0.8763947486877441\n",
      "Step 739/1000, Loss: 0.6760390996932983, Validation Loss: 0.8763774037361145\n",
      "Step 740/1000, Loss: 0.6760215163230896, Validation Loss: 0.8763599395751953\n",
      "Step 741/1000, Loss: 0.6760037541389465, Validation Loss: 0.8763425946235657\n",
      "Step 742/1000, Loss: 0.6759860515594482, Validation Loss: 0.876325249671936\n",
      "Step 743/1000, Loss: 0.6759682893753052, Validation Loss: 0.8763077855110168\n",
      "Step 744/1000, Loss: 0.6759506464004517, Validation Loss: 0.876290500164032\n",
      "Step 745/1000, Loss: 0.6759330034255981, Validation Loss: 0.8762731552124023\n",
      "Step 746/1000, Loss: 0.6759152412414551, Validation Loss: 0.8762558102607727\n",
      "Step 747/1000, Loss: 0.6758975982666016, Validation Loss: 0.8762384653091431\n",
      "Step 748/1000, Loss: 0.675879955291748, Validation Loss: 0.8762210011482239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 749/1000, Loss: 0.675862193107605, Validation Loss: 0.876203715801239\n",
      "Step 750/1000, Loss: 0.6758446097373962, Validation Loss: 0.8761863112449646\n",
      "Step 751/1000, Loss: 0.6758268475532532, Validation Loss: 0.8761690258979797\n",
      "Step 752/1000, Loss: 0.6758091449737549, Validation Loss: 0.8761516213417053\n",
      "Step 753/1000, Loss: 0.6757915019989014, Validation Loss: 0.8761343955993652\n",
      "Step 754/1000, Loss: 0.6757738590240479, Validation Loss: 0.876116931438446\n",
      "Step 755/1000, Loss: 0.6757562160491943, Validation Loss: 0.8760996460914612\n",
      "Step 756/1000, Loss: 0.6757384538650513, Validation Loss: 0.8760822415351868\n",
      "Step 757/1000, Loss: 0.6757208108901978, Validation Loss: 0.8760648369789124\n",
      "Step 758/1000, Loss: 0.675703227519989, Validation Loss: 0.8760476112365723\n",
      "Step 759/1000, Loss: 0.675685465335846, Validation Loss: 0.8760302662849426\n",
      "Step 760/1000, Loss: 0.6756677627563477, Validation Loss: 0.8760128021240234\n",
      "Step 761/1000, Loss: 0.6756500005722046, Validation Loss: 0.8759955167770386\n",
      "Step 762/1000, Loss: 0.6756324172019958, Validation Loss: 0.8759781718254089\n",
      "Step 763/1000, Loss: 0.6756148338317871, Validation Loss: 0.8759608268737793\n",
      "Step 764/1000, Loss: 0.6755971312522888, Validation Loss: 0.8759434819221497\n",
      "Step 765/1000, Loss: 0.6755794286727905, Validation Loss: 0.8759261965751648\n",
      "Step 766/1000, Loss: 0.675561785697937, Validation Loss: 0.8759087920188904\n",
      "Step 767/1000, Loss: 0.6755440831184387, Validation Loss: 0.8758915066719055\n",
      "Step 768/1000, Loss: 0.67552649974823, Validation Loss: 0.8758741617202759\n",
      "Step 769/1000, Loss: 0.6755087971687317, Validation Loss: 0.8758568167686462\n",
      "Step 770/1000, Loss: 0.6754910945892334, Validation Loss: 0.8758395314216614\n",
      "Step 771/1000, Loss: 0.6754734516143799, Validation Loss: 0.8758221864700317\n",
      "Step 772/1000, Loss: 0.6754558086395264, Validation Loss: 0.8758047819137573\n",
      "Step 773/1000, Loss: 0.6754382252693176, Validation Loss: 0.8757874965667725\n",
      "Step 774/1000, Loss: 0.6754204630851746, Validation Loss: 0.8757701516151428\n",
      "Step 775/1000, Loss: 0.6754028797149658, Validation Loss: 0.875752866268158\n",
      "Step 776/1000, Loss: 0.6753851771354675, Validation Loss: 0.8757355213165283\n",
      "Step 777/1000, Loss: 0.6753675937652588, Validation Loss: 0.8757181763648987\n",
      "Step 778/1000, Loss: 0.6753498911857605, Validation Loss: 0.875700831413269\n",
      "Step 779/1000, Loss: 0.6753323078155518, Validation Loss: 0.8756835460662842\n",
      "Step 780/1000, Loss: 0.6753146052360535, Validation Loss: 0.8756661415100098\n",
      "Step 781/1000, Loss: 0.6752970218658447, Validation Loss: 0.8756489157676697\n",
      "Step 782/1000, Loss: 0.6752793192863464, Validation Loss: 0.8756315112113953\n",
      "Step 783/1000, Loss: 0.6752617359161377, Validation Loss: 0.8756142854690552\n",
      "Step 784/1000, Loss: 0.6752440333366394, Validation Loss: 0.8755968809127808\n",
      "Step 785/1000, Loss: 0.6752263307571411, Validation Loss: 0.8755795955657959\n",
      "Step 786/1000, Loss: 0.6752087473869324, Validation Loss: 0.875562310218811\n",
      "Step 787/1000, Loss: 0.6751911640167236, Validation Loss: 0.8755449652671814\n",
      "Step 788/1000, Loss: 0.6751735210418701, Validation Loss: 0.8755276203155518\n",
      "Step 789/1000, Loss: 0.6751558780670166, Validation Loss: 0.8755102753639221\n",
      "Step 790/1000, Loss: 0.6751382350921631, Validation Loss: 0.8754929900169373\n",
      "Step 791/1000, Loss: 0.6751206517219543, Validation Loss: 0.8754756450653076\n",
      "Step 792/1000, Loss: 0.6751030087471008, Validation Loss: 0.8754583597183228\n",
      "Step 793/1000, Loss: 0.6750853657722473, Validation Loss: 0.8754411339759827\n",
      "Step 794/1000, Loss: 0.6750677227973938, Validation Loss: 0.8754238486289978\n",
      "Step 795/1000, Loss: 0.6750500798225403, Validation Loss: 0.8754065036773682\n",
      "Step 796/1000, Loss: 0.6750325560569763, Validation Loss: 0.8753891587257385\n",
      "Step 797/1000, Loss: 0.6750149130821228, Validation Loss: 0.8753718733787537\n",
      "Step 798/1000, Loss: 0.6749972701072693, Validation Loss: 0.8753545880317688\n",
      "Step 799/1000, Loss: 0.6749796271324158, Validation Loss: 0.8753373026847839\n",
      "Step 800/1000, Loss: 0.674962043762207, Validation Loss: 0.8753200769424438\n",
      "Step 801/1000, Loss: 0.6749444603919983, Validation Loss: 0.875302791595459\n",
      "Step 802/1000, Loss: 0.6749268174171448, Validation Loss: 0.875294029712677\n",
      "Step 803/1000, Loss: 0.6749180555343628, Validation Loss: 0.8752853274345398\n",
      "Step 804/1000, Loss: 0.6749091744422913, Validation Loss: 0.8752766847610474\n",
      "Step 805/1000, Loss: 0.6749002933502197, Validation Loss: 0.8752681016921997\n",
      "Step 806/1000, Loss: 0.6748915314674377, Validation Loss: 0.8752593994140625\n",
      "Step 807/1000, Loss: 0.6748827695846558, Validation Loss: 0.8752506375312805\n",
      "Step 808/1000, Loss: 0.674873948097229, Validation Loss: 0.8752420544624329\n",
      "Step 809/1000, Loss: 0.6748651266098022, Validation Loss: 0.8752334713935852\n",
      "Step 810/1000, Loss: 0.6748562455177307, Validation Loss: 0.875224769115448\n",
      "Step 811/1000, Loss: 0.674847424030304, Validation Loss: 0.8752161264419556\n",
      "Step 812/1000, Loss: 0.6748386025428772, Validation Loss: 0.8752074241638184\n",
      "Step 813/1000, Loss: 0.6748297214508057, Validation Loss: 0.8751988410949707\n",
      "Step 814/1000, Loss: 0.6748209595680237, Validation Loss: 0.8751901388168335\n",
      "Step 815/1000, Loss: 0.6748121976852417, Validation Loss: 0.8751814961433411\n",
      "Step 816/1000, Loss: 0.6748033761978149, Validation Loss: 0.8751727938652039\n",
      "Step 817/1000, Loss: 0.674794614315033, Validation Loss: 0.8751642107963562\n",
      "Step 818/1000, Loss: 0.674785852432251, Validation Loss: 0.8751556873321533\n",
      "Step 819/1000, Loss: 0.674777090549469, Validation Loss: 0.8751469850540161\n",
      "Step 820/1000, Loss: 0.6747682094573975, Validation Loss: 0.8751383423805237\n",
      "Step 821/1000, Loss: 0.6747594475746155, Validation Loss: 0.875129759311676\n",
      "Step 822/1000, Loss: 0.6747506856918335, Validation Loss: 0.8751210570335388\n",
      "Step 823/1000, Loss: 0.6747418642044067, Validation Loss: 0.8751124739646912\n",
      "Step 824/1000, Loss: 0.6747331023216248, Validation Loss: 0.8751038908958435\n",
      "Step 825/1000, Loss: 0.674724280834198, Validation Loss: 0.8750951290130615\n",
      "Step 826/1000, Loss: 0.674715518951416, Validation Loss: 0.8750866055488586\n",
      "Step 827/1000, Loss: 0.6747066378593445, Validation Loss: 0.8750778436660767\n",
      "Step 828/1000, Loss: 0.6746978759765625, Validation Loss: 0.875069260597229\n",
      "Step 829/1000, Loss: 0.6746891140937805, Validation Loss: 0.8750606775283813\n",
      "Step 830/1000, Loss: 0.6746802926063538, Validation Loss: 0.8750520348548889\n",
      "Step 831/1000, Loss: 0.674671471118927, Validation Loss: 0.8750433921813965\n",
      "Step 832/1000, Loss: 0.6746627688407898, Validation Loss: 0.8750346899032593\n",
      "Step 833/1000, Loss: 0.674653947353363, Validation Loss: 0.8750261664390564\n",
      "Step 834/1000, Loss: 0.674645185470581, Validation Loss: 0.8750175833702087\n",
      "Step 835/1000, Loss: 0.6746364235877991, Validation Loss: 0.8750090003013611\n",
      "Step 836/1000, Loss: 0.6746275424957275, Validation Loss: 0.8750001788139343\n",
      "Step 837/1000, Loss: 0.6746187806129456, Validation Loss: 0.8749916553497314\n",
      "Step 838/1000, Loss: 0.6746100187301636, Validation Loss: 0.874983012676239\n",
      "Step 839/1000, Loss: 0.6746012568473816, Validation Loss: 0.8749743700027466\n",
      "Step 840/1000, Loss: 0.6745924353599548, Validation Loss: 0.8749656677246094\n",
      "Step 841/1000, Loss: 0.6745836734771729, Validation Loss: 0.8749570846557617\n",
      "Step 842/1000, Loss: 0.6745747923851013, Validation Loss: 0.8749485015869141\n",
      "Step 843/1000, Loss: 0.6745661497116089, Validation Loss: 0.8749399185180664\n",
      "Step 844/1000, Loss: 0.6745572686195374, Validation Loss: 0.8749312162399292\n",
      "Step 845/1000, Loss: 0.6745484471321106, Validation Loss: 0.8749225735664368\n",
      "Step 846/1000, Loss: 0.6745396852493286, Validation Loss: 0.8749139308929443\n",
      "Step 847/1000, Loss: 0.6745309233665466, Validation Loss: 0.8749052882194519\n",
      "Step 848/1000, Loss: 0.6745221614837646, Validation Loss: 0.8748966455459595\n",
      "Step 849/1000, Loss: 0.6745132207870483, Validation Loss: 0.8748881816864014\n",
      "Step 850/1000, Loss: 0.6745045781135559, Validation Loss: 0.8748794794082642\n",
      "Step 851/1000, Loss: 0.6744958162307739, Validation Loss: 0.874870777130127\n",
      "Step 852/1000, Loss: 0.6744870543479919, Validation Loss: 0.8748621940612793\n",
      "Step 853/1000, Loss: 0.6744781732559204, Validation Loss: 0.8748536109924316\n",
      "Step 854/1000, Loss: 0.6744694113731384, Validation Loss: 0.8748450875282288\n",
      "Step 855/1000, Loss: 0.6744606494903564, Validation Loss: 0.8748363256454468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 856/1000, Loss: 0.6744518280029297, Validation Loss: 0.8748279213905334\n",
      "Step 857/1000, Loss: 0.6744431257247925, Validation Loss: 0.8748191595077515\n",
      "Step 858/1000, Loss: 0.6744343042373657, Validation Loss: 0.874810516834259\n",
      "Step 859/1000, Loss: 0.6744254231452942, Validation Loss: 0.8748019337654114\n",
      "Step 860/1000, Loss: 0.6744167804718018, Validation Loss: 0.8747933506965637\n",
      "Step 861/1000, Loss: 0.6744078993797302, Validation Loss: 0.8747847676277161\n",
      "Step 862/1000, Loss: 0.6743991374969482, Validation Loss: 0.8747760653495789\n",
      "Step 863/1000, Loss: 0.6743903756141663, Validation Loss: 0.8747674226760864\n",
      "Step 864/1000, Loss: 0.6743817329406738, Validation Loss: 0.874758780002594\n",
      "Step 865/1000, Loss: 0.6743728518486023, Validation Loss: 0.8747502565383911\n",
      "Step 866/1000, Loss: 0.6743640303611755, Validation Loss: 0.8747416138648987\n",
      "Step 867/1000, Loss: 0.6743553280830383, Validation Loss: 0.874733030796051\n",
      "Step 868/1000, Loss: 0.6743465662002563, Validation Loss: 0.8747243881225586\n",
      "Step 869/1000, Loss: 0.6743377447128296, Validation Loss: 0.8747157454490662\n",
      "Step 870/1000, Loss: 0.6743290424346924, Validation Loss: 0.8747071623802185\n",
      "Step 871/1000, Loss: 0.6743202805519104, Validation Loss: 0.8746985197067261\n",
      "Step 872/1000, Loss: 0.6743114590644836, Validation Loss: 0.8746898770332336\n",
      "Step 873/1000, Loss: 0.6743026971817017, Validation Loss: 0.8746813535690308\n",
      "Step 874/1000, Loss: 0.6742938160896301, Validation Loss: 0.8746726512908936\n",
      "Step 875/1000, Loss: 0.6742851734161377, Validation Loss: 0.8746640682220459\n",
      "Step 876/1000, Loss: 0.6742762923240662, Validation Loss: 0.8746554255485535\n",
      "Step 877/1000, Loss: 0.6742676496505737, Validation Loss: 0.8746469020843506\n",
      "Step 878/1000, Loss: 0.6742588877677917, Validation Loss: 0.8746381998062134\n",
      "Step 879/1000, Loss: 0.6742500066757202, Validation Loss: 0.8746296167373657\n",
      "Step 880/1000, Loss: 0.6742411851882935, Validation Loss: 0.8746209144592285\n",
      "Step 881/1000, Loss: 0.6742324233055115, Validation Loss: 0.8746123313903809\n",
      "Step 882/1000, Loss: 0.6742237210273743, Validation Loss: 0.8746036291122437\n",
      "Step 883/1000, Loss: 0.6742148995399475, Validation Loss: 0.8745951652526855\n",
      "Step 884/1000, Loss: 0.6742061972618103, Validation Loss: 0.8745865225791931\n",
      "Step 885/1000, Loss: 0.6741974353790283, Validation Loss: 0.8745779395103455\n",
      "Step 886/1000, Loss: 0.6741886138916016, Validation Loss: 0.874569296836853\n",
      "Step 887/1000, Loss: 0.6741798520088196, Validation Loss: 0.8745606541633606\n",
      "Step 888/1000, Loss: 0.6741710901260376, Validation Loss: 0.8745520114898682\n",
      "Step 889/1000, Loss: 0.6741623282432556, Validation Loss: 0.8745434880256653\n",
      "Step 890/1000, Loss: 0.6741535663604736, Validation Loss: 0.8745347857475281\n",
      "Step 891/1000, Loss: 0.6741448044776917, Validation Loss: 0.8745262622833252\n",
      "Step 892/1000, Loss: 0.6741360425949097, Validation Loss: 0.8745176196098328\n",
      "Step 893/1000, Loss: 0.6741272807121277, Validation Loss: 0.8745089173316956\n",
      "Step 894/1000, Loss: 0.6741184592247009, Validation Loss: 0.8745003938674927\n",
      "Step 895/1000, Loss: 0.674109697341919, Validation Loss: 0.8744917511940002\n",
      "Step 896/1000, Loss: 0.674100935459137, Validation Loss: 0.8744832277297974\n",
      "Step 897/1000, Loss: 0.674092173576355, Validation Loss: 0.8744745254516602\n",
      "Step 898/1000, Loss: 0.674083411693573, Validation Loss: 0.8744659423828125\n",
      "Step 899/1000, Loss: 0.674074649810791, Validation Loss: 0.8744572401046753\n",
      "Step 900/1000, Loss: 0.674065887928009, Validation Loss: 0.8744486570358276\n",
      "Step 901/1000, Loss: 0.674057126045227, Validation Loss: 0.87444007396698\n",
      "Step 902/1000, Loss: 0.6740483641624451, Validation Loss: 0.8744357824325562\n",
      "Step 903/1000, Loss: 0.6740440130233765, Validation Loss: 0.8744315505027771\n",
      "Step 904/1000, Loss: 0.6740396022796631, Validation Loss: 0.8744271397590637\n",
      "Step 905/1000, Loss: 0.6740351915359497, Validation Loss: 0.8744229674339294\n",
      "Step 906/1000, Loss: 0.6740308403968811, Validation Loss: 0.8744186162948608\n",
      "Step 907/1000, Loss: 0.6740264892578125, Validation Loss: 0.8744144439697266\n",
      "Step 908/1000, Loss: 0.6740221381187439, Validation Loss: 0.874410092830658\n",
      "Step 909/1000, Loss: 0.6740177273750305, Validation Loss: 0.8744057416915894\n",
      "Step 910/1000, Loss: 0.6740133166313171, Validation Loss: 0.8744015097618103\n",
      "Step 911/1000, Loss: 0.6740090847015381, Validation Loss: 0.8743970990180969\n",
      "Step 912/1000, Loss: 0.6740046143531799, Validation Loss: 0.8743929266929626\n",
      "Step 913/1000, Loss: 0.6740003228187561, Validation Loss: 0.8743886947631836\n",
      "Step 914/1000, Loss: 0.6739957928657532, Validation Loss: 0.8743842840194702\n",
      "Step 915/1000, Loss: 0.6739915609359741, Validation Loss: 0.8743800520896912\n",
      "Step 916/1000, Loss: 0.673987090587616, Validation Loss: 0.8743757605552673\n",
      "Step 917/1000, Loss: 0.6739828586578369, Validation Loss: 0.8743714690208435\n",
      "Step 918/1000, Loss: 0.673978328704834, Validation Loss: 0.8743671178817749\n",
      "Step 919/1000, Loss: 0.6739739775657654, Validation Loss: 0.8743629455566406\n",
      "Step 920/1000, Loss: 0.6739696860313416, Validation Loss: 0.8743586540222168\n",
      "Step 921/1000, Loss: 0.673965334892273, Validation Loss: 0.8743543028831482\n",
      "Step 922/1000, Loss: 0.6739609241485596, Validation Loss: 0.8743500709533691\n",
      "Step 923/1000, Loss: 0.673956573009491, Validation Loss: 0.8743457794189453\n",
      "Step 924/1000, Loss: 0.6739521622657776, Validation Loss: 0.8743414878845215\n",
      "Step 925/1000, Loss: 0.673947811126709, Validation Loss: 0.8743373155593872\n",
      "Step 926/1000, Loss: 0.6739433407783508, Validation Loss: 0.8743329644203186\n",
      "Step 927/1000, Loss: 0.673939049243927, Validation Loss: 0.8743286728858948\n",
      "Step 928/1000, Loss: 0.6739345788955688, Validation Loss: 0.8743244409561157\n",
      "Step 929/1000, Loss: 0.6739303469657898, Validation Loss: 0.8743200302124023\n",
      "Step 930/1000, Loss: 0.6739259362220764, Validation Loss: 0.8743157386779785\n",
      "Step 931/1000, Loss: 0.673921525478363, Validation Loss: 0.8743115067481995\n",
      "Step 932/1000, Loss: 0.6739171743392944, Validation Loss: 0.8743072152137756\n",
      "Step 933/1000, Loss: 0.6739128232002258, Validation Loss: 0.8743029236793518\n",
      "Step 934/1000, Loss: 0.6739084124565125, Validation Loss: 0.874298632144928\n",
      "Step 935/1000, Loss: 0.6739040613174438, Validation Loss: 0.8742944002151489\n",
      "Step 936/1000, Loss: 0.6738996505737305, Validation Loss: 0.8742900490760803\n",
      "Step 937/1000, Loss: 0.6738952994346619, Validation Loss: 0.874285876750946\n",
      "Step 938/1000, Loss: 0.6738909482955933, Validation Loss: 0.8742814660072327\n",
      "Step 939/1000, Loss: 0.6738864183425903, Validation Loss: 0.8742771744728088\n",
      "Step 940/1000, Loss: 0.6738821268081665, Validation Loss: 0.8742730021476746\n",
      "Step 941/1000, Loss: 0.6738777756690979, Validation Loss: 0.874268651008606\n",
      "Step 942/1000, Loss: 0.6738734245300293, Validation Loss: 0.8742642998695374\n",
      "Step 943/1000, Loss: 0.6738690137863159, Validation Loss: 0.8742600679397583\n",
      "Step 944/1000, Loss: 0.6738646030426025, Validation Loss: 0.8742557168006897\n",
      "Step 945/1000, Loss: 0.6738602519035339, Validation Loss: 0.8742514848709106\n",
      "Step 946/1000, Loss: 0.6738558411598206, Validation Loss: 0.8742473125457764\n",
      "Step 947/1000, Loss: 0.673851490020752, Validation Loss: 0.8742429614067078\n",
      "Step 948/1000, Loss: 0.6738470196723938, Validation Loss: 0.8742386102676392\n",
      "Step 949/1000, Loss: 0.67384272813797, Validation Loss: 0.8742343783378601\n",
      "Step 950/1000, Loss: 0.6738383769989014, Validation Loss: 0.8742300868034363\n",
      "Step 951/1000, Loss: 0.6738340258598328, Validation Loss: 0.8742257356643677\n",
      "Step 952/1000, Loss: 0.6738296151161194, Validation Loss: 0.8742214441299438\n",
      "Step 953/1000, Loss: 0.673825204372406, Validation Loss: 0.87421715259552\n",
      "Step 954/1000, Loss: 0.6738209128379822, Validation Loss: 0.8742129802703857\n",
      "Step 955/1000, Loss: 0.6738165020942688, Validation Loss: 0.8742086291313171\n",
      "Step 956/1000, Loss: 0.6738121509552002, Validation Loss: 0.8742044568061829\n",
      "Step 957/1000, Loss: 0.6738077998161316, Validation Loss: 0.8742000460624695\n",
      "Step 958/1000, Loss: 0.673803448677063, Validation Loss: 0.8741958737373352\n",
      "Step 959/1000, Loss: 0.6737990975379944, Validation Loss: 0.8741915225982666\n",
      "Step 960/1000, Loss: 0.6737948060035706, Validation Loss: 0.8741873502731323\n",
      "Step 961/1000, Loss: 0.6737903356552124, Validation Loss: 0.8741830587387085\n",
      "Step 962/1000, Loss: 0.673785924911499, Validation Loss: 0.8741787075996399\n",
      "Step 963/1000, Loss: 0.6737815737724304, Validation Loss: 0.8741744160652161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 964/1000, Loss: 0.6737772226333618, Validation Loss: 0.8741702437400818\n",
      "Step 965/1000, Loss: 0.673772931098938, Validation Loss: 0.8741657733917236\n",
      "Step 966/1000, Loss: 0.6737684607505798, Validation Loss: 0.8741617202758789\n",
      "Step 967/1000, Loss: 0.673764169216156, Validation Loss: 0.8741573691368103\n",
      "Step 968/1000, Loss: 0.6737598180770874, Validation Loss: 0.8741530776023865\n",
      "Step 969/1000, Loss: 0.6737554669380188, Validation Loss: 0.8741486668586731\n",
      "Step 970/1000, Loss: 0.6737511157989502, Validation Loss: 0.8741444945335388\n",
      "Step 971/1000, Loss: 0.6737467050552368, Validation Loss: 0.8741402626037598\n",
      "Step 972/1000, Loss: 0.6737423539161682, Validation Loss: 0.8741359710693359\n",
      "Step 973/1000, Loss: 0.6737379431724548, Validation Loss: 0.8741317391395569\n",
      "Step 974/1000, Loss: 0.6737335920333862, Validation Loss: 0.8741275668144226\n",
      "Step 975/1000, Loss: 0.6737292408943176, Validation Loss: 0.8741230964660645\n",
      "Step 976/1000, Loss: 0.673724889755249, Validation Loss: 0.8741188645362854\n",
      "Step 977/1000, Loss: 0.67372065782547, Validation Loss: 0.8741145133972168\n",
      "Step 978/1000, Loss: 0.673716127872467, Validation Loss: 0.8741104006767273\n",
      "Step 979/1000, Loss: 0.6737117767333984, Validation Loss: 0.8741059899330139\n",
      "Step 980/1000, Loss: 0.6737074255943298, Validation Loss: 0.8741018772125244\n",
      "Step 981/1000, Loss: 0.673703134059906, Validation Loss: 0.874097466468811\n",
      "Step 982/1000, Loss: 0.6736987829208374, Validation Loss: 0.874093234539032\n",
      "Step 983/1000, Loss: 0.673694372177124, Validation Loss: 0.8740890622138977\n",
      "Step 984/1000, Loss: 0.6736900210380554, Validation Loss: 0.8740847110748291\n",
      "Step 985/1000, Loss: 0.6736856698989868, Validation Loss: 0.8740804195404053\n",
      "Step 986/1000, Loss: 0.6736813187599182, Validation Loss: 0.8740761876106262\n",
      "Step 987/1000, Loss: 0.6736769676208496, Validation Loss: 0.8740718960762024\n",
      "Step 988/1000, Loss: 0.6736725568771362, Validation Loss: 0.8740676045417786\n",
      "Step 989/1000, Loss: 0.6736682057380676, Validation Loss: 0.8740633130073547\n",
      "Step 990/1000, Loss: 0.6736637949943542, Validation Loss: 0.8740591406822205\n",
      "Step 991/1000, Loss: 0.6736595034599304, Validation Loss: 0.8740547895431519\n",
      "Step 992/1000, Loss: 0.6736551523208618, Validation Loss: 0.874050498008728\n",
      "Step 993/1000, Loss: 0.6736506819725037, Validation Loss: 0.8740461468696594\n",
      "Step 994/1000, Loss: 0.6736463308334351, Validation Loss: 0.8740418553352356\n",
      "Step 995/1000, Loss: 0.6736419796943665, Validation Loss: 0.8740375638008118\n",
      "Step 996/1000, Loss: 0.6736376285552979, Validation Loss: 0.8740333318710327\n",
      "Step 997/1000, Loss: 0.6736332178115845, Validation Loss: 0.8740289211273193\n",
      "Step 998/1000, Loss: 0.6736288666725159, Validation Loss: 0.8740246891975403\n",
      "Step 999/1000, Loss: 0.6736245155334473, Validation Loss: 0.8740203976631165\n",
      "Step 1000/1000, Loss: 0.6736202239990234, Validation Loss: 0.8740161657333374\n",
      "Step 1/1000, Loss: 1.0001459121704102, Validation Loss: 1.1631892919540405\n",
      "Step 2/1000, Loss: 0.9988601207733154, Validation Loss: 1.1620981693267822\n",
      "Step 3/1000, Loss: 0.9975792765617371, Validation Loss: 1.1610110998153687\n",
      "Step 4/1000, Loss: 0.9963030815124512, Validation Loss: 1.159928560256958\n",
      "Step 5/1000, Loss: 0.9950308799743652, Validation Loss: 1.1588495969772339\n",
      "Step 6/1000, Loss: 0.9937623739242554, Validation Loss: 1.1577742099761963\n",
      "Step 7/1000, Loss: 0.9924970269203186, Validation Loss: 1.1567018032073975\n",
      "Step 8/1000, Loss: 0.9912340641021729, Validation Loss: 1.1556320190429688\n",
      "Step 9/1000, Loss: 0.9899725317955017, Validation Loss: 1.1545647382736206\n",
      "Step 10/1000, Loss: 0.9887117743492126, Validation Loss: 1.1534996032714844\n",
      "Step 11/1000, Loss: 0.9874507784843445, Validation Loss: 1.1524362564086914\n",
      "Step 12/1000, Loss: 0.986188530921936, Validation Loss: 1.151374340057373\n",
      "Step 13/1000, Loss: 0.98492431640625, Validation Loss: 1.1503137350082397\n",
      "Step 14/1000, Loss: 0.98365718126297, Validation Loss: 1.1492537260055542\n",
      "Step 15/1000, Loss: 0.9823861122131348, Validation Loss: 1.1481938362121582\n",
      "Step 16/1000, Loss: 0.9811108112335205, Validation Loss: 1.1471335887908936\n",
      "Step 17/1000, Loss: 0.9798301458358765, Validation Loss: 1.146072268486023\n",
      "Step 18/1000, Loss: 0.9785436391830444, Validation Loss: 1.1450092792510986\n",
      "Step 19/1000, Loss: 0.9772506356239319, Validation Loss: 1.1439436674118042\n",
      "Step 20/1000, Loss: 0.9759508371353149, Validation Loss: 1.1428749561309814\n",
      "Step 21/1000, Loss: 0.9746438264846802, Validation Loss: 1.1418025493621826\n",
      "Step 22/1000, Loss: 0.9733291268348694, Validation Loss: 1.1407254934310913\n",
      "Step 23/1000, Loss: 0.9720064401626587, Validation Loss: 1.139643669128418\n",
      "Step 24/1000, Loss: 0.9706756472587585, Validation Loss: 1.138556718826294\n",
      "Step 25/1000, Loss: 0.9693363308906555, Validation Loss: 1.1374644041061401\n",
      "Step 26/1000, Loss: 0.9679887294769287, Validation Loss: 1.136366844177246\n",
      "Step 27/1000, Loss: 0.9666323065757751, Validation Loss: 1.1352635622024536\n",
      "Step 28/1000, Loss: 0.9652674794197083, Validation Loss: 1.1341549158096313\n",
      "Step 29/1000, Loss: 0.9638937711715698, Validation Loss: 1.133040428161621\n",
      "Step 30/1000, Loss: 0.9625114798545837, Validation Loss: 1.131920576095581\n",
      "Step 31/1000, Loss: 0.9611206650733948, Validation Loss: 1.1307951211929321\n",
      "Step 32/1000, Loss: 0.9597212672233582, Validation Loss: 1.1296641826629639\n",
      "Step 33/1000, Loss: 0.9583137035369873, Validation Loss: 1.1285279989242554\n",
      "Step 34/1000, Loss: 0.9568976163864136, Validation Loss: 1.1273863315582275\n",
      "Step 35/1000, Loss: 0.9554736614227295, Validation Loss: 1.1262396574020386\n",
      "Step 36/1000, Loss: 0.9540417790412903, Validation Loss: 1.125087857246399\n",
      "Step 37/1000, Loss: 0.9526021480560303, Validation Loss: 1.1239310503005981\n",
      "Step 38/1000, Loss: 0.9511550664901733, Validation Loss: 1.1227694749832153\n",
      "Step 39/1000, Loss: 0.9497007727622986, Validation Loss: 1.1216033697128296\n",
      "Step 40/1000, Loss: 0.9482393860816956, Validation Loss: 1.120432734489441\n",
      "Step 41/1000, Loss: 0.9467712044715881, Validation Loss: 1.119257926940918\n",
      "Step 42/1000, Loss: 0.9452967047691345, Validation Loss: 1.1180790662765503\n",
      "Step 43/1000, Loss: 0.9438158869743347, Validation Loss: 1.116896390914917\n",
      "Step 44/1000, Loss: 0.9423290491104126, Validation Loss: 1.115709900856018\n",
      "Step 45/1000, Loss: 0.9408366680145264, Validation Loss: 1.1145199537277222\n",
      "Step 46/1000, Loss: 0.9393388628959656, Validation Loss: 1.1133267879486084\n",
      "Step 47/1000, Loss: 0.9378360509872437, Validation Loss: 1.1121305227279663\n",
      "Step 48/1000, Loss: 0.9363281726837158, Validation Loss: 1.110931396484375\n",
      "Step 49/1000, Loss: 0.9348158240318298, Validation Loss: 1.1097296476364136\n",
      "Step 50/1000, Loss: 0.93329918384552, Validation Loss: 1.108525276184082\n",
      "Step 51/1000, Loss: 0.9317786693572998, Validation Loss: 1.1073187589645386\n",
      "Step 52/1000, Loss: 0.9302542805671692, Validation Loss: 1.1061099767684937\n",
      "Step 53/1000, Loss: 0.9287265539169312, Validation Loss: 1.1048991680145264\n",
      "Step 54/1000, Loss: 0.9271955490112305, Validation Loss: 1.1036869287490845\n",
      "Step 55/1000, Loss: 0.9256615042686462, Validation Loss: 1.1024726629257202\n",
      "Step 56/1000, Loss: 0.9241248965263367, Validation Loss: 1.1012566089630127\n",
      "Step 57/1000, Loss: 0.9225857257843018, Validation Loss: 1.1000393629074097\n",
      "Step 58/1000, Loss: 0.9210442900657654, Validation Loss: 1.098820447921753\n",
      "Step 59/1000, Loss: 0.9195008277893066, Validation Loss: 1.0976003408432007\n",
      "Step 60/1000, Loss: 0.9179555773735046, Validation Loss: 1.0963785648345947\n",
      "Step 61/1000, Loss: 0.9164088368415833, Validation Loss: 1.0951555967330933\n",
      "Step 62/1000, Loss: 0.9148602485656738, Validation Loss: 1.0939311981201172\n",
      "Step 63/1000, Loss: 0.9133105278015137, Validation Loss: 1.092705249786377\n",
      "Step 64/1000, Loss: 0.911759614944458, Validation Loss: 1.0914781093597412\n",
      "Step 65/1000, Loss: 0.9102078080177307, Validation Loss: 1.0902494192123413\n",
      "Step 66/1000, Loss: 0.908655047416687, Validation Loss: 1.0890189409255981\n",
      "Step 67/1000, Loss: 0.9071013927459717, Validation Loss: 1.0877870321273804\n",
      "Step 68/1000, Loss: 0.9055473804473877, Validation Loss: 1.0865534543991089\n",
      "Step 69/1000, Loss: 0.9039927124977112, Validation Loss: 1.0853182077407837\n",
      "Step 70/1000, Loss: 0.9024375081062317, Validation Loss: 1.0840811729431152\n",
      "Step 71/1000, Loss: 0.9008819460868835, Validation Loss: 1.082842230796814\n",
      "Step 72/1000, Loss: 0.8993260264396667, Validation Loss: 1.0816015005111694\n",
      "Step 73/1000, Loss: 0.8977699279785156, Validation Loss: 1.080358862876892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 74/1000, Loss: 0.8962136507034302, Validation Loss: 1.0791144371032715\n",
      "Step 75/1000, Loss: 0.8946570158004761, Validation Loss: 1.0778679847717285\n",
      "Step 76/1000, Loss: 0.893100380897522, Validation Loss: 1.0766197443008423\n",
      "Step 77/1000, Loss: 0.8915436267852783, Validation Loss: 1.0753694772720337\n",
      "Step 78/1000, Loss: 0.8899868130683899, Validation Loss: 1.0741175413131714\n",
      "Step 79/1000, Loss: 0.8884298801422119, Validation Loss: 1.0728638172149658\n",
      "Step 80/1000, Loss: 0.8868727684020996, Validation Loss: 1.071608304977417\n",
      "Step 81/1000, Loss: 0.8853156566619873, Validation Loss: 1.0703511238098145\n",
      "Step 82/1000, Loss: 0.8837584257125854, Validation Loss: 1.0690923929214478\n",
      "Step 83/1000, Loss: 0.882201075553894, Validation Loss: 1.0678319931030273\n",
      "Step 84/1000, Loss: 0.8806437253952026, Validation Loss: 1.0665702819824219\n",
      "Step 85/1000, Loss: 0.8790860772132874, Validation Loss: 1.0653070211410522\n",
      "Step 86/1000, Loss: 0.8775282502174377, Validation Loss: 1.0640426874160767\n",
      "Step 87/1000, Loss: 0.8759703636169434, Validation Loss: 1.062777042388916\n",
      "Step 88/1000, Loss: 0.8744122385978699, Validation Loss: 1.0615103244781494\n",
      "Step 89/1000, Loss: 0.872853696346283, Validation Loss: 1.0602424144744873\n",
      "Step 90/1000, Loss: 0.8712949156761169, Validation Loss: 1.0589734315872192\n",
      "Step 91/1000, Loss: 0.8697357773780823, Validation Loss: 1.0577032566070557\n",
      "Step 92/1000, Loss: 0.8681759834289551, Validation Loss: 1.0564321279525757\n",
      "Step 93/1000, Loss: 0.8666160106658936, Validation Loss: 1.0551599264144897\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sympy import symbols, sympify\n",
    "from scipy.special import erfc\n",
    "from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# Parameters\n",
    "alpha = 2.5e-6\n",
    "T0 = 20\n",
    "T1 = 40\n",
    "L = 4\n",
    "dx = 0.1\n",
    "dt = 1800\n",
    "tMax = 86400\n",
    "W = 30\n",
    "\n",
    "# Time and space grid\n",
    "x = np.arange(0, L + dx, dx)\n",
    "t = np.arange(dt, tMax + dt, dt)\n",
    "\n",
    "# Initialize temperature data storage\n",
    "TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# Compute temperature distribution and store results\n",
    "for k in range(len(t)):\n",
    "    TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# Prepare the data\n",
    "x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Normalize the data\n",
    "x_mean = np.mean(x_data, axis=0)\n",
    "x_std = np.std(x_data, axis=0)\n",
    "y_mean = np.mean(y_data)\n",
    "y_std = np.std(y_data)\n",
    "\n",
    "x_data_normalized = (x_data - x_mean) / x_std\n",
    "y_data_normalized = (y_data - y_mean) / y_std\n",
    "# x_data_normalized = x_data\n",
    "# y_data_normalized = y_data\n",
    "\n",
    "# Convert to tensors\n",
    "x_tensor = torch.tensor(x_data_normalized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_data_normalized, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "train_size = int(0.8 * len(x_tensor))\n",
    "test_size = len(x_tensor) - train_size\n",
    "train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# Create the dataset dictionary as expected by KAN\n",
    "dataset = {\n",
    "    'train_input': train_input,\n",
    "    'train_label': train_label,\n",
    "    'test_input': test_input,\n",
    "    'test_label': test_label\n",
    "}\n",
    "\n",
    "# Add erfc to the symbolic library\n",
    "add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# Function to train KAN with early stopping and learning rate scheduling\n",
    "def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "    current_lr = initial_lr\n",
    "\n",
    "    for step in range(steps):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(dataset['train_input'])\n",
    "            loss = criterion(outputs, dataset['train_label'])\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(closure).item()\n",
    "\n",
    "        # Validation step\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(dataset['test_input'])\n",
    "            val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "        print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        if step % patience == 0 and step > 0:\n",
    "            current_lr = max(min_lr, current_lr * lr_decay)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "\n",
    "        # Check for NaN values\n",
    "        if np.isnan(loss) or np.isnan(val_loss):\n",
    "            print(\"NaN detected, stopping training\")\n",
    "            break\n",
    "\n",
    "    # Load the best model\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "# Initial training with a coarse grid\n",
    "initial_grid = 3\n",
    "model = KAN(width=[2, 2, 2, 1], grid=initial_grid, k=3, seed=0)\n",
    "train_with_early_stopping(model, dataset, steps=1000, patience=100, initial_lr=0.002)\n",
    "\n",
    "# Iteratively refine the grid and retrain the model\n",
    "grids = [5, 10, 20]\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for grid in grids:\n",
    "    new_model = KAN(width=[2, 2, 2, 1], grid=grid, k=3).initialize_from_another_model(model, dataset['train_input'])\n",
    "    train_with_early_stopping(new_model, dataset, steps=1000, patience=100, initial_lr=0.002)\n",
    "    model = new_model  # Update the model to the new refined grid model\n",
    "\n",
    "    # Collect training and test losses\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(dataset['train_input'])\n",
    "        train_loss = torch.nn.functional.mse_loss(train_outputs, dataset['train_label']).item()\n",
    "        test_outputs = model(dataset['test_input'])\n",
    "        test_loss = torch.nn.functional.mse_loss(test_outputs, dataset['test_label']).item()\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "# Automatically set activation functions to be symbolic\n",
    "lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "model.auto_symbolic(lib=lib)\n",
    "\n",
    "# Prune the model\n",
    "model = model.prune()\n",
    "\n",
    "# Obtain the symbolic formula and denormalize it\n",
    "symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# symbolic_formula_denormalized = sympify(symbolic_formula.replace('x_1', '(x_1*{:.6f}+{:.6f})'.format(x_std[0], x_mean[0]))\n",
    "#                                         .replace('x_2', '(x_2*{:.6f}+{:.6f})'.format(x_std[1], x_mean[1])))\n",
    "# symbolic_formula_denormalized = (symbolic_formula_denormalized * y_std + y_mean).simplify()\n",
    "\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula_denormalized)\n",
    "\n",
    "# Create output directory for plots\n",
    "outputDir = 'TemperaturePlots'\n",
    "if not os.path.exists(outputDir):\n",
    "    os.makedirs(outputDir)\n",
    "\n",
    "# Plot predicted temperature distribution\n",
    "x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "x_test_normalized = (x_test - x_mean) / x_std\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "predicted_temperature = model(x_test_tensor).detach().numpy().flatten() * y_std + y_mean\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, predicted_temperature, label='Predicted')\n",
    "plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "plt.xlabel('Position along the wall thickness (m)')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and test losses over different grid refinements\n",
    "plt.figure()\n",
    "plt.plot(grids, train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(grids, test_losses, marker='o', label='Test Loss')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Grid Size')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Training and Test Losses over Grid Refinements')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14422989",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Discovered Symbolic Formula (Normalized):\")\n",
    "model.symbolic_formula()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Discovered Symbolic Formula (Original):\")\n",
    "# print(original_formula)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
