{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f8a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 固定壁面温度的一维稳态计算\n",
    "\n",
    "# % 参数设置\n",
    "# L = 0.24;          % 墙体的厚度，单位为米 (240mm)\n",
    "# W = 1.0;           % 墙体的长度，单位为米 (1000mm)\n",
    "# alpha = 2.5*10^(-6);      % 热扩散率，单位为平方米每秒\n",
    "# T0 = 20;           % x=0处的温度，单位为摄氏度\n",
    "# TL = 40;           % x=L处的温度，单位为摄氏度\n",
    "# Ti = 25;           % 初始平均温度，单位为摄氏度\n",
    "\n",
    "# % 计算温度分布\n",
    "# x = linspace(0, L, 100); % 在墙的厚度方向生成100个点以计算温度分布\n",
    "# T = T0 + (TL - T0) * (x / L); % 线性温度分布计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553725ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6  # Thermal diffusivity of the soil in m^2/s\n",
    "# T0 = 20        # Initial surface temperature in Celsius\n",
    "# T1 = 40        # Surface temperature after change in Celsius\n",
    "# L = 4          # Calculation thickness in meters\n",
    "# dx = 0.1       # Spatial step in meters\n",
    "# dt = 1800      # Time step in seconds\n",
    "# tMax = 86400   # Simulation duration in seconds (1 day)\n",
    "# W = 30         # Image width for plotting in meters\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)  # Space grid\n",
    "# t = np.arange(dt, tMax + dt, dt)  # Time grid, start from dt to avoid divide by zero\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Normalize the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# x_mean = np.mean(x_data, axis=0)\n",
    "# x_std = np.std(x_data, axis=0) + 1e-8  # Add small constant to avoid division by zero\n",
    "# x_data_norm = (x_data - x_mean) / x_std\n",
    "\n",
    "# y_mean = np.mean(y_data)\n",
    "# y_std = np.std(y_data) + 1e-8  # Add small constant to avoid division by zero\n",
    "# y_data_norm = (y_data - y_mean) / y_std\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data_norm, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data_norm, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Initialize KAN with adjusted parameters\n",
    "# model = KAN(width=[2, 10, 1], grid=20, k=3, seed=0)  # Input layer now has 2 neurons\n",
    "\n",
    "# # Plot KAN at initialization\n",
    "# model(dataset['train_input'])\n",
    "# model.plot(beta=100)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Train the model with early stopping and learning rate scheduling\n",
    "# train_with_early_stopping(model, dataset, steps=500, patience=100, initial_lr=0.0025)\n",
    "\n",
    "# # Plot trained KAN\n",
    "# model.plot()\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "# # # Continue training to almost machine precision with conservative settings\n",
    "# # train_with_early_stopping(model, dataset, steps=200, patience=50, initial_lr=0.00001)\n",
    "\n",
    "# # Obtain the symbolic formula in terms of normalized data\n",
    "# symbolic_formula_normalized = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula (Normalized):\")\n",
    "# print(symbolic_formula_normalized)\n",
    "\n",
    "# # Reverse normalization for symbolic formula using sympy\n",
    "# x_sym, t_sym = symbols('x t')\n",
    "# normalized_formula = sympify(symbolic_formula_normalized)\n",
    "\n",
    "# # Replace normalized variables with original scale variables\n",
    "# original_formula = normalized_formula * (y_std / x_std[0]) + (y_mean - y_std / x_std[0] * x_mean[0])\n",
    "\n",
    "# print(\"Discovered Symbolic Formula (Original):\")\n",
    "# print(original_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_norm = (x_test - x_mean) / x_std\n",
    "# x_test_tensor = torch.tensor(x_test_norm, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "# predicted_temperature = predicted_temperature * y_std + y_mean  # Denormalize the prediction\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00562165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6  # Thermal diffusivity of the soil in m^2/s\n",
    "# T0 = 20        # Initial surface temperature in Celsius\n",
    "# T1 = 40        # Surface temperature after change in Celsius\n",
    "# L = 4          # Calculation thickness in meters\n",
    "# dx = 0.1       # Spatial step in meters\n",
    "# dt = 1800      # Time step in seconds\n",
    "# tMax = 86400   # Simulation duration in seconds (1 day)\n",
    "# W = 30         # Image width for plotting in meters\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)  # Space grid\n",
    "# t = np.arange(dt, tMax + dt, dt)  # Time grid, start from dt to avoid divide by zero\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Prepare the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Initialize KAN with adjusted parameters\n",
    "# model = KAN(width=[2, 20, 1], grid=20, k=3, seed=0)  # Input layer now has 2 neurons\n",
    "\n",
    "# # Plot KAN at initialization\n",
    "# model(dataset['train_input'])\n",
    "# model.plot(beta=100)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Train the model with early stopping and learning rate scheduling\n",
    "# train_with_early_stopping(model, dataset, steps=500, patience=100, initial_lr=0.0025)\n",
    "\n",
    "# # Plot trained KAN\n",
    "# model.plot()\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "# # # Continue training to almost machine precision with conservative settings\n",
    "# # train_with_early_stopping(model, dataset, steps=200, patience=50, initial_lr=0.00001)\n",
    "\n",
    "# # Obtain the symbolic formula\n",
    "# symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af2582d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6\n",
    "# T0 = 20\n",
    "# T1 = 40\n",
    "# L = 4\n",
    "# dx = 0.1\n",
    "# dt = 1800\n",
    "# tMax = 86400\n",
    "# W = 30\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)\n",
    "# t = np.arange(dt, tMax + dt, dt)\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Prepare the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Initial training with a coarse grid\n",
    "# initial_grid = 3\n",
    "# model = KAN(width=[2, 2, 2, 1], grid=initial_grid, k=3, seed=0)\n",
    "# train_with_early_stopping(model, dataset, steps=1000, patience=100, initial_lr=0.002)\n",
    "\n",
    "# # Iteratively refine the grid and retrain the model\n",
    "# grids = [5, 10, 20, 50, 100]\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "\n",
    "# for grid in grids:\n",
    "#     new_model = KAN(width=[2, 2, 2, 1], grid=grid, k=3).initialize_from_another_model(model, dataset['train_input'])\n",
    "#     train_with_early_stopping(new_model, dataset, steps=500, patience=100, initial_lr=0.00005)\n",
    "#     model = new_model  # Update the model to the new refined grid model\n",
    "\n",
    "#     # Collect training and test losses\n",
    "#     with torch.no_grad():\n",
    "#         train_outputs = model(dataset['train_input'])\n",
    "#         train_loss = torch.nn.functional.mse_loss(train_outputs, dataset['train_label']).item()\n",
    "#         test_outputs = model(dataset['test_input'])\n",
    "#         test_loss = torch.nn.functional.mse_loss(test_outputs, dataset['test_label']).item()\n",
    "#         train_losses.append(train_loss)\n",
    "#         test_losses.append(test_loss)\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "\n",
    "# # Obtain the symbolic formula\n",
    "# symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training and test losses over different grid refinements\n",
    "# plt.figure()\n",
    "# plt.plot(grids, train_losses, marker='o', label='Train Loss')\n",
    "# plt.plot(grids, test_losses, marker='o', label='Test Loss')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel('Grid Size')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.title('Training and Test Losses over Grid Refinements')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40a6bb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1000, Loss: 448167936.0, Validation Loss: 1641033216.0\n",
      "Step 2/1000, Loss: 437588864.0, Validation Loss: 1602142336.0\n",
      "Step 3/1000, Loss: 427218176.0, Validation Loss: 1564020352.0\n",
      "Step 4/1000, Loss: 417052768.0, Validation Loss: 1526656768.0\n",
      "Step 5/1000, Loss: 407089472.0, Validation Loss: 1490038528.0\n",
      "Step 6/1000, Loss: 397325024.0, Validation Loss: 1454154624.0\n",
      "Step 7/1000, Loss: 387756352.0, Validation Loss: 1418992640.0\n",
      "Step 8/1000, Loss: 378380128.0, Validation Loss: 1384542336.0\n",
      "Step 9/1000, Loss: 369193696.0, Validation Loss: 1350790656.0\n",
      "Step 10/1000, Loss: 360193632.0, Validation Loss: 1317728128.0\n",
      "Step 11/1000, Loss: 351377248.0, Validation Loss: 1285341824.0\n",
      "Step 12/1000, Loss: 342741312.0, Validation Loss: 1253622656.0\n",
      "Step 13/1000, Loss: 334283104.0, Validation Loss: 1222557824.0\n",
      "Step 14/1000, Loss: 325999552.0, Validation Loss: 1192137984.0\n",
      "Step 15/1000, Loss: 317887904.0, Validation Loss: 1162351872.0\n",
      "Step 16/1000, Loss: 309945280.0, Validation Loss: 1133189760.0\n",
      "Step 17/1000, Loss: 302168960.0, Validation Loss: 1104639744.0\n",
      "Step 18/1000, Loss: 294555968.0, Validation Loss: 1076693376.0\n",
      "Step 19/1000, Loss: 287103808.0, Validation Loss: 1049338816.0\n",
      "Step 20/1000, Loss: 279809632.0, Validation Loss: 1022567424.0\n",
      "Step 21/1000, Loss: 272670816.0, Validation Loss: 996368256.0\n",
      "Step 22/1000, Loss: 265684720.0, Validation Loss: 970732352.0\n",
      "Step 23/1000, Loss: 258848688.0, Validation Loss: 945649280.0\n",
      "Step 24/1000, Loss: 252160192.0, Validation Loss: 921110336.0\n",
      "Step 25/1000, Loss: 245616672.0, Validation Loss: 897105152.0\n",
      "Step 26/1000, Loss: 239215600.0, Validation Loss: 873625408.0\n",
      "Step 27/1000, Loss: 232954512.0, Validation Loss: 850660992.0\n",
      "Step 28/1000, Loss: 226830960.0, Validation Loss: 828203520.0\n",
      "Step 29/1000, Loss: 220842544.0, Validation Loss: 806243520.0\n",
      "Step 30/1000, Loss: 214986768.0, Validation Loss: 784772544.0\n",
      "Step 31/1000, Loss: 209261408.0, Validation Loss: 763781568.0\n",
      "Step 32/1000, Loss: 203664064.0, Validation Loss: 743262336.0\n",
      "Step 33/1000, Loss: 198192480.0, Validation Loss: 723205760.0\n",
      "Step 34/1000, Loss: 192844320.0, Validation Loss: 703603904.0\n",
      "Step 35/1000, Loss: 187617392.0, Validation Loss: 684448320.0\n",
      "Step 36/1000, Loss: 182509440.0, Validation Loss: 665730880.0\n",
      "Step 37/1000, Loss: 177518336.0, Validation Loss: 647443456.0\n",
      "Step 38/1000, Loss: 172641888.0, Validation Loss: 629578112.0\n",
      "Step 39/1000, Loss: 167878016.0, Validation Loss: 612127040.0\n",
      "Step 40/1000, Loss: 163224560.0, Validation Loss: 595082368.0\n",
      "Step 41/1000, Loss: 158679536.0, Validation Loss: 578436544.0\n",
      "Step 42/1000, Loss: 154240832.0, Validation Loss: 562182080.0\n",
      "Step 43/1000, Loss: 149906512.0, Validation Loss: 546311424.0\n",
      "Step 44/1000, Loss: 145674544.0, Validation Loss: 530817632.0\n",
      "Step 45/1000, Loss: 141543024.0, Validation Loss: 515693056.0\n",
      "Step 46/1000, Loss: 137509984.0, Validation Loss: 500930848.0\n",
      "Step 47/1000, Loss: 133573560.0, Validation Loss: 486523808.0\n",
      "Step 48/1000, Loss: 129731864.0, Validation Loss: 472465120.0\n",
      "Step 49/1000, Loss: 125983048.0, Validation Loss: 458747968.0\n",
      "Step 50/1000, Loss: 122325304.0, Validation Loss: 445365728.0\n",
      "Step 51/1000, Loss: 118756888.0, Validation Loss: 432311552.0\n",
      "Step 52/1000, Loss: 115275936.0, Validation Loss: 419579424.0\n",
      "Step 53/1000, Loss: 111880840.0, Validation Loss: 407162272.0\n",
      "Step 54/1000, Loss: 108569776.0, Validation Loss: 395054240.0\n",
      "Step 55/1000, Loss: 105341096.0, Validation Loss: 383248960.0\n",
      "Step 56/1000, Loss: 102193160.0, Validation Loss: 371740320.0\n",
      "Step 57/1000, Loss: 99124352.0, Validation Loss: 360522240.0\n",
      "Step 58/1000, Loss: 96133008.0, Validation Loss: 349588992.0\n",
      "Step 59/1000, Loss: 93217600.0, Validation Loss: 338934400.0\n",
      "Step 60/1000, Loss: 90376520.0, Validation Loss: 328552960.0\n",
      "Step 61/1000, Loss: 87608248.0, Validation Loss: 318438752.0\n",
      "Step 62/1000, Loss: 84911296.0, Validation Loss: 308586496.0\n",
      "Step 63/1000, Loss: 82284152.0, Validation Loss: 298990592.0\n",
      "Step 64/1000, Loss: 79725352.0, Validation Loss: 289645536.0\n",
      "Step 65/1000, Loss: 77233472.0, Validation Loss: 280546112.0\n",
      "Step 66/1000, Loss: 74807072.0, Validation Loss: 271687008.0\n",
      "Step 67/1000, Loss: 72444768.0, Validation Loss: 263063152.0\n",
      "Step 68/1000, Loss: 70145184.0, Validation Loss: 254669488.0\n",
      "Step 69/1000, Loss: 67906992.0, Validation Loss: 246500960.0\n",
      "Step 70/1000, Loss: 65728828.0, Validation Loss: 238552784.0\n",
      "Step 71/1000, Loss: 63609428.0, Validation Loss: 230819936.0\n",
      "Step 72/1000, Loss: 61547448.0, Validation Loss: 223297904.0\n",
      "Step 73/1000, Loss: 59541672.0, Validation Loss: 215981920.0\n",
      "Step 74/1000, Loss: 57590848.0, Validation Loss: 208867488.0\n",
      "Step 75/1000, Loss: 55693764.0, Validation Loss: 201949936.0\n",
      "Step 76/1000, Loss: 53849192.0, Validation Loss: 195225056.0\n",
      "Step 77/1000, Loss: 52055976.0, Validation Loss: 188688256.0\n",
      "Step 78/1000, Loss: 50312932.0, Validation Loss: 182335408.0\n",
      "Step 79/1000, Loss: 48618932.0, Validation Loss: 176162304.0\n",
      "Step 80/1000, Loss: 46972856.0, Validation Loss: 170164768.0\n",
      "Step 81/1000, Loss: 45373604.0, Validation Loss: 164338800.0\n",
      "Step 82/1000, Loss: 43820100.0, Validation Loss: 158680416.0\n",
      "Step 83/1000, Loss: 42311276.0, Validation Loss: 153185664.0\n",
      "Step 84/1000, Loss: 40846096.0, Validation Loss: 147850704.0\n",
      "Step 85/1000, Loss: 39423520.0, Validation Loss: 142671744.0\n",
      "Step 86/1000, Loss: 38042548.0, Validation Loss: 137645168.0\n",
      "Step 87/1000, Loss: 36702204.0, Validation Loss: 132767216.0\n",
      "Step 88/1000, Loss: 35401504.0, Validation Loss: 128034400.0\n",
      "Step 89/1000, Loss: 34139492.0, Validation Loss: 123443192.0\n",
      "Step 90/1000, Loss: 32915238.0, Validation Loss: 118990120.0\n",
      "Step 91/1000, Loss: 31727826.0, Validation Loss: 114671800.0\n",
      "Step 92/1000, Loss: 30576346.0, Validation Loss: 110484976.0\n",
      "Step 93/1000, Loss: 29459930.0, Validation Loss: 106426344.0\n",
      "Step 94/1000, Loss: 28377694.0, Validation Loss: 102492728.0\n",
      "Step 95/1000, Loss: 27328798.0, Validation Loss: 98680976.0\n",
      "Step 96/1000, Loss: 26312394.0, Validation Loss: 94988072.0\n",
      "Step 97/1000, Loss: 25327684.0, Validation Loss: 91410952.0\n",
      "Step 98/1000, Loss: 24373844.0, Validation Loss: 87946680.0\n",
      "Step 99/1000, Loss: 23450100.0, Validation Loss: 84592360.0\n",
      "Step 100/1000, Loss: 22555672.0, Validation Loss: 81345152.0\n",
      "Step 101/1000, Loss: 21689812.0, Validation Loss: 78202264.0\n",
      "Step 102/1000, Loss: 20851764.0, Validation Loss: 76669376.0\n",
      "Step 103/1000, Loss: 20443020.0, Validation Loss: 75162048.0\n",
      "Step 104/1000, Loss: 20041090.0, Validation Loss: 73679832.0\n",
      "Step 105/1000, Loss: 19645866.0, Validation Loss: 72222368.0\n",
      "Step 106/1000, Loss: 19257232.0, Validation Loss: 70789224.0\n",
      "Step 107/1000, Loss: 18875092.0, Validation Loss: 69380088.0\n",
      "Step 108/1000, Loss: 18499342.0, Validation Loss: 67994528.0\n",
      "Step 109/1000, Loss: 18129892.0, Validation Loss: 66632236.0\n",
      "Step 110/1000, Loss: 17766636.0, Validation Loss: 65292848.0\n",
      "Step 111/1000, Loss: 17409492.0, Validation Loss: 63976028.0\n",
      "Step 112/1000, Loss: 17058362.0, Validation Loss: 62681436.0\n",
      "Step 113/1000, Loss: 16713166.0, Validation Loss: 61408760.0\n",
      "Step 114/1000, Loss: 16373809.0, Validation Loss: 60157688.0\n",
      "Step 115/1000, Loss: 16040214.0, Validation Loss: 58927896.0\n",
      "Step 116/1000, Loss: 15712292.0, Validation Loss: 57719064.0\n",
      "Step 117/1000, Loss: 15389963.0, Validation Loss: 56530928.0\n",
      "Step 118/1000, Loss: 15073150.0, Validation Loss: 55363156.0\n",
      "Step 119/1000, Loss: 14761768.0, Validation Loss: 54215468.0\n",
      "Step 120/1000, Loss: 14455742.0, Validation Loss: 53087580.0\n",
      "Step 121/1000, Loss: 14154994.0, Validation Loss: 51979212.0\n",
      "Step 122/1000, Loss: 13859451.0, Validation Loss: 50890088.0\n",
      "Step 123/1000, Loss: 13569038.0, Validation Loss: 49819908.0\n",
      "Step 124/1000, Loss: 13283682.0, Validation Loss: 48768436.0\n",
      "Step 125/1000, Loss: 13003310.0, Validation Loss: 47735396.0\n",
      "Step 126/1000, Loss: 12727854.0, Validation Loss: 46720504.0\n",
      "Step 127/1000, Loss: 12457238.0, Validation Loss: 45723512.0\n",
      "Step 128/1000, Loss: 12191395.0, Validation Loss: 44744160.0\n",
      "Step 129/1000, Loss: 11930257.0, Validation Loss: 43782208.0\n",
      "Step 130/1000, Loss: 11673755.0, Validation Loss: 42837384.0\n",
      "Step 131/1000, Loss: 11421822.0, Validation Loss: 41909456.0\n",
      "Step 132/1000, Loss: 11174394.0, Validation Loss: 40998160.0\n",
      "Step 133/1000, Loss: 10931404.0, Validation Loss: 40103288.0\n",
      "Step 134/1000, Loss: 10692788.0, Validation Loss: 39224564.0\n",
      "Step 135/1000, Loss: 10458483.0, Validation Loss: 38361772.0\n",
      "Step 136/1000, Loss: 10228424.0, Validation Loss: 37514676.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 137/1000, Loss: 10002550.0, Validation Loss: 36683028.0\n",
      "Step 138/1000, Loss: 9780799.0, Validation Loss: 35866628.0\n",
      "Step 139/1000, Loss: 9563109.0, Validation Loss: 35065228.0\n",
      "Step 140/1000, Loss: 9349421.0, Validation Loss: 34278624.0\n",
      "Step 141/1000, Loss: 9139676.0, Validation Loss: 33506574.0\n",
      "Step 142/1000, Loss: 8933816.0, Validation Loss: 32748872.0\n",
      "Step 143/1000, Loss: 8731780.0, Validation Loss: 32005310.0\n",
      "Step 144/1000, Loss: 8533514.0, Validation Loss: 31275658.0\n",
      "Step 145/1000, Loss: 8338955.5, Validation Loss: 30559716.0\n",
      "Step 146/1000, Loss: 8148055.0, Validation Loss: 29857264.0\n",
      "Step 147/1000, Loss: 7960754.5, Validation Loss: 29168118.0\n",
      "Step 148/1000, Loss: 7776997.5, Validation Loss: 28492060.0\n",
      "Step 149/1000, Loss: 7596732.0, Validation Loss: 27828892.0\n",
      "Step 150/1000, Loss: 7419903.5, Validation Loss: 27178410.0\n",
      "Step 151/1000, Loss: 7246459.5, Validation Loss: 26540428.0\n",
      "Step 152/1000, Loss: 7076346.0, Validation Loss: 25914748.0\n",
      "Step 153/1000, Loss: 6909513.0, Validation Loss: 25301172.0\n",
      "Step 154/1000, Loss: 6745909.5, Validation Loss: 24699520.0\n",
      "Step 155/1000, Loss: 6585484.0, Validation Loss: 24109594.0\n",
      "Step 156/1000, Loss: 6428187.5, Validation Loss: 23531218.0\n",
      "Step 157/1000, Loss: 6273968.5, Validation Loss: 22964216.0\n",
      "Step 158/1000, Loss: 6122782.5, Validation Loss: 22408386.0\n",
      "Step 159/1000, Loss: 5974575.5, Validation Loss: 21863570.0\n",
      "Step 160/1000, Loss: 5829306.0, Validation Loss: 21329580.0\n",
      "Step 161/1000, Loss: 5686922.5, Validation Loss: 20806254.0\n",
      "Step 162/1000, Loss: 5547382.5, Validation Loss: 20293402.0\n",
      "Step 163/1000, Loss: 5410637.5, Validation Loss: 19790866.0\n",
      "Step 164/1000, Loss: 5276642.0, Validation Loss: 19298478.0\n",
      "Step 165/1000, Loss: 5145353.0, Validation Loss: 18816072.0\n",
      "Step 166/1000, Loss: 5016723.5, Validation Loss: 18343486.0\n",
      "Step 167/1000, Loss: 4890713.5, Validation Loss: 17880548.0\n",
      "Step 168/1000, Loss: 4767276.5, Validation Loss: 17427114.0\n",
      "Step 169/1000, Loss: 4646373.5, Validation Loss: 16983008.0\n",
      "Step 170/1000, Loss: 4527960.0, Validation Loss: 16548090.0\n",
      "Step 171/1000, Loss: 4411994.0, Validation Loss: 16122203.0\n",
      "Step 172/1000, Loss: 4298437.0, Validation Loss: 15705188.0\n",
      "Step 173/1000, Loss: 4187246.0, Validation Loss: 15296904.0\n",
      "Step 174/1000, Loss: 4078383.25, Validation Loss: 14897203.0\n",
      "Step 175/1000, Loss: 3971807.75, Validation Loss: 14505936.0\n",
      "Step 176/1000, Loss: 3867481.25, Validation Loss: 14122956.0\n",
      "Step 177/1000, Loss: 3765365.75, Validation Loss: 13748123.0\n",
      "Step 178/1000, Loss: 3665422.75, Validation Loss: 13381301.0\n",
      "Step 179/1000, Loss: 3567615.0, Validation Loss: 13022346.0\n",
      "Step 180/1000, Loss: 3471905.25, Validation Loss: 12671127.0\n",
      "Step 181/1000, Loss: 3378258.0, Validation Loss: 12327507.0\n",
      "Step 182/1000, Loss: 3286637.0, Validation Loss: 11991349.0\n",
      "Step 183/1000, Loss: 3197006.0, Validation Loss: 11662526.0\n",
      "Step 184/1000, Loss: 3109331.0, Validation Loss: 11340913.0\n",
      "Step 185/1000, Loss: 3023578.0, Validation Loss: 11026375.0\n",
      "Step 186/1000, Loss: 2939712.0, Validation Loss: 10718787.0\n",
      "Step 187/1000, Loss: 2857699.75, Validation Loss: 10418029.0\n",
      "Step 188/1000, Loss: 2777508.0, Validation Loss: 10123973.0\n",
      "Step 189/1000, Loss: 2699103.75, Validation Loss: 9836506.0\n",
      "Step 190/1000, Loss: 2622455.75, Validation Loss: 9555501.0\n",
      "Step 191/1000, Loss: 2547531.5, Validation Loss: 9280847.0\n",
      "Step 192/1000, Loss: 2474299.75, Validation Loss: 9012423.0\n",
      "Step 193/1000, Loss: 2402730.25, Validation Loss: 8750121.0\n",
      "Step 194/1000, Loss: 2332793.25, Validation Loss: 8493822.0\n",
      "Step 195/1000, Loss: 2264456.25, Validation Loss: 8243421.0\n",
      "Step 196/1000, Loss: 2197692.5, Validation Loss: 7998807.0\n",
      "Step 197/1000, Loss: 2132471.25, Validation Loss: 7759871.5\n",
      "Step 198/1000, Loss: 2068764.25, Validation Loss: 7526506.5\n",
      "Step 199/1000, Loss: 2006543.0, Validation Loss: 7298613.5\n",
      "Step 200/1000, Loss: 1945780.75, Validation Loss: 7076084.5\n",
      "Step 201/1000, Loss: 1886448.0, Validation Loss: 6858817.5\n",
      "Step 202/1000, Loss: 1828519.375, Validation Loss: 6752156.5\n",
      "Step 203/1000, Loss: 1800081.125, Validation Loss: 6646826.0\n",
      "Step 204/1000, Loss: 1771996.875, Validation Loss: 6542807.5\n",
      "Step 205/1000, Loss: 1744263.0, Validation Loss: 6440079.0\n",
      "Step 206/1000, Loss: 1716873.125, Validation Loss: 6338627.0\n",
      "Step 207/1000, Loss: 1689823.75, Validation Loss: 6238431.0\n",
      "Step 208/1000, Loss: 1663109.0, Validation Loss: 6139478.0\n",
      "Step 209/1000, Loss: 1636725.875, Validation Loss: 6041750.0\n",
      "Step 210/1000, Loss: 1610669.25, Validation Loss: 5945234.0\n",
      "Step 211/1000, Loss: 1584935.625, Validation Loss: 5849914.0\n",
      "Step 212/1000, Loss: 1559521.25, Validation Loss: 5755774.5\n",
      "Step 213/1000, Loss: 1534421.75, Validation Loss: 5662804.5\n",
      "Step 214/1000, Loss: 1509633.625, Validation Loss: 5570990.0\n",
      "Step 215/1000, Loss: 1485153.625, Validation Loss: 5480317.5\n",
      "Step 216/1000, Loss: 1460978.125, Validation Loss: 5390772.5\n",
      "Step 217/1000, Loss: 1437104.0, Validation Loss: 5302346.5\n",
      "Step 218/1000, Loss: 1413527.875, Validation Loss: 5215025.5\n",
      "Step 219/1000, Loss: 1390246.0, Validation Loss: 5128797.0\n",
      "Step 220/1000, Loss: 1367256.0, Validation Loss: 5043650.0\n",
      "Step 221/1000, Loss: 1344553.625, Validation Loss: 4959573.0\n",
      "Step 222/1000, Loss: 1322137.25, Validation Loss: 4876553.5\n",
      "Step 223/1000, Loss: 1300002.75, Validation Loss: 4794583.0\n",
      "Step 224/1000, Loss: 1278148.0, Validation Loss: 4713649.5\n",
      "Step 225/1000, Loss: 1256569.375, Validation Loss: 4633742.0\n",
      "Step 226/1000, Loss: 1235264.375, Validation Loss: 4554849.0\n",
      "Step 227/1000, Loss: 1214230.0, Validation Loss: 4476961.5\n",
      "Step 228/1000, Loss: 1193463.875, Validation Loss: 4400066.5\n",
      "Step 229/1000, Loss: 1172962.5, Validation Loss: 4324157.5\n",
      "Step 230/1000, Loss: 1152723.875, Validation Loss: 4249222.0\n",
      "Step 231/1000, Loss: 1132744.75, Validation Loss: 4175251.25\n",
      "Step 232/1000, Loss: 1113022.75, Validation Loss: 4102233.75\n",
      "Step 233/1000, Loss: 1093555.25, Validation Loss: 4030161.0\n",
      "Step 234/1000, Loss: 1074339.375, Validation Loss: 3959022.75\n",
      "Step 235/1000, Loss: 1055372.875, Validation Loss: 3888811.0\n",
      "Step 236/1000, Loss: 1036653.1875, Validation Loss: 3819514.25\n",
      "Step 237/1000, Loss: 1018177.8125, Validation Loss: 3751122.75\n",
      "Step 238/1000, Loss: 999943.8125, Validation Loss: 3683629.75\n",
      "Step 239/1000, Loss: 981949.0, Validation Loss: 3617024.25\n",
      "Step 240/1000, Loss: 964191.0, Validation Loss: 3551296.25\n",
      "Step 241/1000, Loss: 946667.375, Validation Loss: 3486438.75\n",
      "Step 242/1000, Loss: 929375.625, Validation Loss: 3422442.25\n",
      "Step 243/1000, Loss: 912313.25, Validation Loss: 3359298.0\n",
      "Step 244/1000, Loss: 895478.1875, Validation Loss: 3296996.75\n",
      "Step 245/1000, Loss: 878867.9375, Validation Loss: 3235529.0\n",
      "Step 246/1000, Loss: 862480.125, Validation Loss: 3174887.75\n",
      "Step 247/1000, Loss: 846312.3125, Validation Loss: 3115064.0\n",
      "Step 248/1000, Loss: 830362.75, Validation Loss: 3056048.75\n",
      "Step 249/1000, Loss: 814628.6875, Validation Loss: 2997833.75\n",
      "Step 250/1000, Loss: 799108.0625, Validation Loss: 2940411.5\n",
      "Step 251/1000, Loss: 783798.75, Validation Loss: 2883773.0\n",
      "Step 252/1000, Loss: 768698.5, Validation Loss: 2827909.75\n",
      "Step 253/1000, Loss: 753804.9375, Validation Loss: 2772814.25\n",
      "Step 254/1000, Loss: 739116.0625, Validation Loss: 2718477.75\n",
      "Step 255/1000, Loss: 724629.625, Validation Loss: 2664893.5\n",
      "Step 256/1000, Loss: 710343.75, Validation Loss: 2612053.25\n",
      "Step 257/1000, Loss: 696256.1875, Validation Loss: 2559949.25\n",
      "Step 258/1000, Loss: 682365.0, Validation Loss: 2508573.75\n",
      "Step 259/1000, Loss: 668667.9375, Validation Loss: 2457918.0\n",
      "Step 260/1000, Loss: 655163.0625, Validation Loss: 2407976.25\n",
      "Step 261/1000, Loss: 641848.25, Validation Loss: 2358739.75\n",
      "Step 262/1000, Loss: 628721.6875, Validation Loss: 2310201.75\n",
      "Step 263/1000, Loss: 615781.375, Validation Loss: 2262354.75\n",
      "Step 264/1000, Loss: 603025.1875, Validation Loss: 2215190.75\n",
      "Step 265/1000, Loss: 590451.25, Validation Loss: 2168703.25\n",
      "Step 266/1000, Loss: 578057.75, Validation Loss: 2122885.25\n",
      "Step 267/1000, Loss: 565842.6875, Validation Loss: 2077730.0\n",
      "Step 268/1000, Loss: 553804.25, Validation Loss: 2033229.125\n",
      "Step 269/1000, Loss: 541940.4375, Validation Loss: 1989377.0\n",
      "Step 270/1000, Loss: 530249.4375, Validation Loss: 1946165.75\n",
      "Step 271/1000, Loss: 518729.46875, Validation Loss: 1903589.5\n",
      "Step 272/1000, Loss: 507378.8125, Validation Loss: 1861641.375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 273/1000, Loss: 496195.59375, Validation Loss: 1820314.5\n",
      "Step 274/1000, Loss: 485178.03125, Validation Loss: 1779602.5\n",
      "Step 275/1000, Loss: 474324.375, Validation Loss: 1739498.5\n",
      "Step 276/1000, Loss: 463632.84375, Validation Loss: 1699996.625\n",
      "Step 277/1000, Loss: 453101.875, Validation Loss: 1661089.625\n",
      "Step 278/1000, Loss: 442729.59375, Validation Loss: 1622772.25\n",
      "Step 279/1000, Loss: 432514.59375, Validation Loss: 1585037.5\n",
      "Step 280/1000, Loss: 422454.78125, Validation Loss: 1547880.25\n",
      "Step 281/1000, Loss: 412549.0, Validation Loss: 1511293.125\n",
      "Step 282/1000, Loss: 402795.21875, Validation Loss: 1475271.0\n",
      "Step 283/1000, Loss: 393192.09375, Validation Loss: 1439807.375\n",
      "Step 284/1000, Loss: 383737.875, Validation Loss: 1404896.625\n",
      "Step 285/1000, Loss: 374431.21875, Validation Loss: 1370533.375\n",
      "Step 286/1000, Loss: 365270.40625, Validation Loss: 1336711.25\n",
      "Step 287/1000, Loss: 356253.9375, Validation Loss: 1303425.0\n",
      "Step 288/1000, Loss: 347380.34375, Validation Loss: 1270668.75\n",
      "Step 289/1000, Loss: 338648.03125, Validation Loss: 1238437.5\n",
      "Step 290/1000, Loss: 330055.75, Validation Loss: 1206725.625\n",
      "Step 291/1000, Loss: 321601.84375, Validation Loss: 1175527.125\n",
      "Step 292/1000, Loss: 313285.0, Validation Loss: 1144837.5\n",
      "Step 293/1000, Loss: 305103.6875, Validation Loss: 1114651.0\n",
      "Step 294/1000, Loss: 297056.65625, Validation Loss: 1084962.625\n",
      "Step 295/1000, Loss: 289142.4375, Validation Loss: 1055767.125\n",
      "Step 296/1000, Loss: 281359.59375, Validation Loss: 1027059.625\n",
      "Step 297/1000, Loss: 273706.8125, Validation Loss: 998835.1875\n",
      "Step 298/1000, Loss: 266182.90625, Validation Loss: 971089.0625\n",
      "Step 299/1000, Loss: 258786.609375, Validation Loss: 943816.375\n",
      "Step 300/1000, Loss: 251516.421875, Validation Loss: 917012.125\n",
      "Step 301/1000, Loss: 244371.21875, Validation Loss: 890671.6875\n",
      "Step 302/1000, Loss: 237349.609375, Validation Loss: 877675.5625\n",
      "Step 303/1000, Loss: 233885.296875, Validation Loss: 864796.8125\n",
      "Step 304/1000, Loss: 230452.203125, Validation Loss: 852034.1875\n",
      "Step 305/1000, Loss: 227050.171875, Validation Loss: 839387.1875\n",
      "Step 306/1000, Loss: 223678.859375, Validation Loss: 826854.5625\n",
      "Step 307/1000, Loss: 220338.09375, Validation Loss: 814435.875\n",
      "Step 308/1000, Loss: 217027.65625, Validation Loss: 802130.0\n",
      "Step 309/1000, Loss: 213747.359375, Validation Loss: 789936.375\n",
      "Step 310/1000, Loss: 210496.96875, Validation Loss: 777854.3125\n",
      "Step 311/1000, Loss: 207276.34375, Validation Loss: 765882.875\n",
      "Step 312/1000, Loss: 204085.1875, Validation Loss: 754021.6875\n",
      "Step 313/1000, Loss: 200923.453125, Validation Loss: 742269.75\n",
      "Step 314/1000, Loss: 197790.875, Validation Loss: 730626.75\n",
      "Step 315/1000, Loss: 194687.28125, Validation Loss: 719092.0\n",
      "Step 316/1000, Loss: 191612.578125, Validation Loss: 707664.5625\n",
      "Step 317/1000, Loss: 188566.53125, Validation Loss: 696344.4375\n",
      "Step 318/1000, Loss: 185549.09375, Validation Loss: 685130.8125\n",
      "Step 319/1000, Loss: 182559.984375, Validation Loss: 674023.0\n",
      "Step 320/1000, Loss: 179599.140625, Validation Loss: 663020.5\n",
      "Step 321/1000, Loss: 176666.34375, Validation Loss: 652122.5625\n",
      "Step 322/1000, Loss: 173761.46875, Validation Loss: 641329.0\n",
      "Step 323/1000, Loss: 170884.375, Validation Loss: 630639.3125\n",
      "Step 324/1000, Loss: 168035.015625, Validation Loss: 620052.6875\n",
      "Step 325/1000, Loss: 165213.140625, Validation Loss: 609568.6875\n",
      "Step 326/1000, Loss: 162418.59375, Validation Loss: 599186.9375\n",
      "Step 327/1000, Loss: 159651.328125, Validation Loss: 588906.8125\n",
      "Step 328/1000, Loss: 156911.1875, Validation Loss: 578728.1875\n",
      "Step 329/1000, Loss: 154198.109375, Validation Loss: 568650.25\n",
      "Step 330/1000, Loss: 151511.8125, Validation Loss: 558672.625\n",
      "Step 331/1000, Loss: 148852.3125, Validation Loss: 548794.5625\n",
      "Step 332/1000, Loss: 146219.390625, Validation Loss: 539016.1875\n",
      "Step 333/1000, Loss: 143612.984375, Validation Loss: 529336.625\n",
      "Step 334/1000, Loss: 141032.96875, Validation Loss: 519755.625\n",
      "Step 335/1000, Loss: 138479.203125, Validation Loss: 510272.59375\n",
      "Step 336/1000, Loss: 135951.609375, Validation Loss: 500887.21875\n",
      "Step 337/1000, Loss: 133450.015625, Validation Loss: 491599.09375\n",
      "Step 338/1000, Loss: 130974.3671875, Validation Loss: 482407.5625\n",
      "Step 339/1000, Loss: 128524.4765625, Validation Loss: 473312.5\n",
      "Step 340/1000, Loss: 126100.2890625, Validation Loss: 464313.3125\n",
      "Step 341/1000, Loss: 123701.6796875, Validation Loss: 455409.625\n",
      "Step 342/1000, Loss: 121328.546875, Validation Loss: 446601.21875\n",
      "Step 343/1000, Loss: 118980.796875, Validation Loss: 437887.46875\n",
      "Step 344/1000, Loss: 116658.3046875, Validation Loss: 429268.09375\n",
      "Step 345/1000, Loss: 114360.953125, Validation Loss: 420742.625\n",
      "Step 346/1000, Loss: 112088.6640625, Validation Loss: 412310.65625\n",
      "Step 347/1000, Loss: 109841.2890625, Validation Loss: 403971.96875\n",
      "Step 348/1000, Loss: 107618.8359375, Validation Loss: 395726.1875\n",
      "Step 349/1000, Loss: 105421.109375, Validation Loss: 387572.96875\n",
      "Step 350/1000, Loss: 103248.0546875, Validation Loss: 379511.625\n",
      "Step 351/1000, Loss: 101099.5234375, Validation Loss: 371542.125\n",
      "Step 352/1000, Loss: 98975.46875, Validation Loss: 363664.0\n",
      "Step 353/1000, Loss: 96875.8125, Validation Loss: 355877.125\n",
      "Step 354/1000, Loss: 94800.4296875, Validation Loss: 348180.71875\n",
      "Step 355/1000, Loss: 92749.2265625, Validation Loss: 340574.875\n",
      "Step 356/1000, Loss: 90722.1484375, Validation Loss: 333058.96875\n",
      "Step 357/1000, Loss: 88719.0625, Validation Loss: 325632.84375\n",
      "Step 358/1000, Loss: 86739.8828125, Validation Loss: 318296.15625\n",
      "Step 359/1000, Loss: 84784.578125, Validation Loss: 311048.65625\n",
      "Step 360/1000, Loss: 82853.0390625, Validation Loss: 303889.9375\n",
      "Step 361/1000, Loss: 80945.1796875, Validation Loss: 296819.6875\n",
      "Step 362/1000, Loss: 79060.90625, Validation Loss: 289837.5625\n",
      "Step 363/1000, Loss: 77200.140625, Validation Loss: 282943.4375\n",
      "Step 364/1000, Loss: 75362.84375, Validation Loss: 276136.9375\n",
      "Step 365/1000, Loss: 73548.890625, Validation Loss: 269417.78125\n",
      "Step 366/1000, Loss: 71758.2421875, Validation Loss: 262785.6875\n",
      "Step 367/1000, Loss: 69990.8046875, Validation Loss: 256240.40625\n",
      "Step 368/1000, Loss: 68246.5234375, Validation Loss: 249781.640625\n",
      "Step 369/1000, Loss: 66525.3125, Validation Loss: 243409.140625\n",
      "Step 370/1000, Loss: 64827.078125, Validation Loss: 237122.6875\n",
      "Step 371/1000, Loss: 63151.80859375, Validation Loss: 230921.953125\n",
      "Step 372/1000, Loss: 61499.3984375, Validation Loss: 224806.796875\n",
      "Step 373/1000, Loss: 59869.80859375, Validation Loss: 218776.890625\n",
      "Step 374/1000, Loss: 58262.9375, Validation Loss: 212832.015625\n",
      "Step 375/1000, Loss: 56678.75, Validation Loss: 206971.953125\n",
      "Step 376/1000, Loss: 55117.1796875, Validation Loss: 201196.546875\n",
      "Step 377/1000, Loss: 53578.17578125, Validation Loss: 195505.5\n",
      "Step 378/1000, Loss: 52061.671875, Validation Loss: 189898.640625\n",
      "Step 379/1000, Loss: 50567.609375, Validation Loss: 184375.71875\n",
      "Step 380/1000, Loss: 49095.93359375, Validation Loss: 178936.53125\n",
      "Step 381/1000, Loss: 47646.5859375, Validation Loss: 173580.953125\n",
      "Step 382/1000, Loss: 46219.52734375, Validation Loss: 168308.734375\n",
      "Step 383/1000, Loss: 44814.69921875, Validation Loss: 163119.734375\n",
      "Step 384/1000, Loss: 43432.0546875, Validation Loss: 158013.71875\n",
      "Step 385/1000, Loss: 42071.55078125, Validation Loss: 152990.53125\n",
      "Step 386/1000, Loss: 40733.13671875, Validation Loss: 148050.03125\n",
      "Step 387/1000, Loss: 39416.76171875, Validation Loss: 143192.0625\n",
      "Step 388/1000, Loss: 38122.390625, Validation Loss: 138416.453125\n",
      "Step 389/1000, Loss: 36849.98046875, Validation Loss: 133723.03125\n",
      "Step 390/1000, Loss: 35599.48046875, Validation Loss: 129111.6484375\n",
      "Step 391/1000, Loss: 34370.8671875, Validation Loss: 124582.1953125\n",
      "Step 392/1000, Loss: 33164.1015625, Validation Loss: 120134.5390625\n",
      "Step 393/1000, Loss: 31979.146484375, Validation Loss: 115768.5546875\n",
      "Step 394/1000, Loss: 30815.96484375, Validation Loss: 111484.140625\n",
      "Step 395/1000, Loss: 29674.53125, Validation Loss: 107281.09375\n",
      "Step 396/1000, Loss: 28554.80859375, Validation Loss: 103159.34375\n",
      "Step 397/1000, Loss: 27456.748046875, Validation Loss: 99118.78125\n",
      "Step 398/1000, Loss: 26380.345703125, Validation Loss: 95159.3671875\n",
      "Step 399/1000, Loss: 25325.580078125, Validation Loss: 91280.953125\n",
      "Step 400/1000, Loss: 24292.404296875, Validation Loss: 87483.4453125\n",
      "Step 401/1000, Loss: 23280.81640625, Validation Loss: 83766.8125\n",
      "Step 402/1000, Loss: 22290.787109375, Validation Loss: 81938.75\n",
      "Step 403/1000, Loss: 21803.83203125, Validation Loss: 80130.8125\n",
      "Step 404/1000, Loss: 21322.25390625, Validation Loss: 78343.03125\n",
      "Step 405/1000, Loss: 20846.044921875, Validation Loss: 76575.3203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 406/1000, Loss: 20375.19921875, Validation Loss: 74827.828125\n",
      "Step 407/1000, Loss: 19909.736328125, Validation Loss: 73100.46875\n",
      "Step 408/1000, Loss: 19449.638671875, Validation Loss: 71393.2421875\n",
      "Step 409/1000, Loss: 18994.908203125, Validation Loss: 69706.1484375\n",
      "Step 410/1000, Loss: 18545.5546875, Validation Loss: 68039.1953125\n",
      "Step 411/1000, Loss: 18101.568359375, Validation Loss: 66392.40625\n",
      "Step 412/1000, Loss: 17662.955078125, Validation Loss: 64765.7265625\n",
      "Step 413/1000, Loss: 17229.7109375, Validation Loss: 63159.1875\n",
      "Step 414/1000, Loss: 16801.83203125, Validation Loss: 61572.7734375\n",
      "Step 415/1000, Loss: 16379.32421875, Validation Loss: 60006.50390625\n",
      "Step 416/1000, Loss: 15962.1806640625, Validation Loss: 58460.35546875\n",
      "Step 417/1000, Loss: 15550.40625, Validation Loss: 56934.34375\n",
      "Step 418/1000, Loss: 15144.0029296875, Validation Loss: 55428.38671875\n",
      "Step 419/1000, Loss: 14742.953125, Validation Loss: 53942.59375\n",
      "Step 420/1000, Loss: 14347.2734375, Validation Loss: 52476.9453125\n",
      "Step 421/1000, Loss: 13956.9609375, Validation Loss: 51031.38671875\n",
      "Step 422/1000, Loss: 13572.0068359375, Validation Loss: 49605.91015625\n",
      "Step 423/1000, Loss: 13192.4130859375, Validation Loss: 48200.57421875\n",
      "Step 424/1000, Loss: 12818.1884765625, Validation Loss: 46815.34375\n",
      "Step 425/1000, Loss: 12449.32421875, Validation Loss: 45450.19921875\n",
      "Step 426/1000, Loss: 12085.814453125, Validation Loss: 44105.17578125\n",
      "Step 427/1000, Loss: 11727.6708984375, Validation Loss: 42780.22265625\n",
      "Step 428/1000, Loss: 11374.884765625, Validation Loss: 41475.3984375\n",
      "Step 429/1000, Loss: 11027.458984375, Validation Loss: 40190.671875\n",
      "Step 430/1000, Loss: 10685.3916015625, Validation Loss: 38926.046875\n",
      "Step 431/1000, Loss: 10348.6884765625, Validation Loss: 37681.51171875\n",
      "Step 432/1000, Loss: 10017.3408203125, Validation Loss: 36457.0859375\n",
      "Step 433/1000, Loss: 9691.35546875, Validation Loss: 35252.7578125\n",
      "Step 434/1000, Loss: 9370.728515625, Validation Loss: 34068.55859375\n",
      "Step 435/1000, Loss: 9055.470703125, Validation Loss: 32904.42578125\n",
      "Step 436/1000, Loss: 8745.564453125, Validation Loss: 31760.423828125\n",
      "Step 437/1000, Loss: 8441.0263671875, Validation Loss: 30636.515625\n",
      "Step 438/1000, Loss: 8141.84130859375, Validation Loss: 29532.689453125\n",
      "Step 439/1000, Loss: 7848.017578125, Validation Loss: 28449.005859375\n",
      "Step 440/1000, Loss: 7559.5615234375, Validation Loss: 27385.421875\n",
      "Step 441/1000, Loss: 7276.4658203125, Validation Loss: 26341.958984375\n",
      "Step 442/1000, Loss: 6998.732421875, Validation Loss: 25318.62109375\n",
      "Step 443/1000, Loss: 6726.3671875, Validation Loss: 24315.408203125\n",
      "Step 444/1000, Loss: 6459.373046875, Validation Loss: 23332.3203125\n",
      "Step 445/1000, Loss: 6197.7431640625, Validation Loss: 22369.357421875\n",
      "Step 446/1000, Loss: 5941.47900390625, Validation Loss: 21426.541015625\n",
      "Step 447/1000, Loss: 5690.587890625, Validation Loss: 20503.876953125\n",
      "Step 448/1000, Loss: 5445.06884765625, Validation Loss: 19601.36328125\n",
      "Step 449/1000, Loss: 5204.92236328125, Validation Loss: 18719.0078125\n",
      "Step 450/1000, Loss: 4970.1513671875, Validation Loss: 17856.814453125\n",
      "Step 451/1000, Loss: 4740.7578125, Validation Loss: 17014.77734375\n",
      "Step 452/1000, Loss: 4516.74072265625, Validation Loss: 16192.919921875\n",
      "Step 453/1000, Loss: 4298.1044921875, Validation Loss: 15391.2509765625\n",
      "Step 454/1000, Loss: 4084.84814453125, Validation Loss: 14609.7841796875\n",
      "Step 455/1000, Loss: 3876.980712890625, Validation Loss: 13848.525390625\n",
      "Step 456/1000, Loss: 3674.500732421875, Validation Loss: 13107.478515625\n",
      "Step 457/1000, Loss: 3477.41259765625, Validation Loss: 12386.6513671875\n",
      "Step 458/1000, Loss: 3285.71337890625, Validation Loss: 11686.046875\n",
      "Step 459/1000, Loss: 3099.41015625, Validation Loss: 11005.685546875\n",
      "Step 460/1000, Loss: 2918.505615234375, Validation Loss: 10345.5888671875\n",
      "Step 461/1000, Loss: 2743.001708984375, Validation Loss: 9705.75390625\n",
      "Step 462/1000, Loss: 2572.901611328125, Validation Loss: 9086.1982421875\n",
      "Step 463/1000, Loss: 2408.208984375, Validation Loss: 8486.9287109375\n",
      "Step 464/1000, Loss: 2248.92626953125, Validation Loss: 7907.9638671875\n",
      "Step 465/1000, Loss: 2095.056640625, Validation Loss: 7349.31396484375\n",
      "Step 466/1000, Loss: 1946.60498046875, Validation Loss: 6810.98046875\n",
      "Step 467/1000, Loss: 1803.5733642578125, Validation Loss: 6292.998046875\n",
      "Step 468/1000, Loss: 1665.9661865234375, Validation Loss: 5795.37255859375\n",
      "Step 469/1000, Loss: 1533.7867431640625, Validation Loss: 5318.11181640625\n",
      "Step 470/1000, Loss: 1407.038330078125, Validation Loss: 4861.23388671875\n",
      "Step 471/1000, Loss: 1285.725341796875, Validation Loss: 4424.75927734375\n",
      "Step 472/1000, Loss: 1169.8525390625, Validation Loss: 4008.694091796875\n",
      "Step 473/1000, Loss: 1059.4224853515625, Validation Loss: 3613.052001953125\n",
      "Step 474/1000, Loss: 954.4408569335938, Validation Loss: 3237.87060546875\n",
      "Step 475/1000, Loss: 854.9129638671875, Validation Loss: 2883.153564453125\n",
      "Step 476/1000, Loss: 760.8414916992188, Validation Loss: 2548.91650390625\n",
      "Step 477/1000, Loss: 672.2314453125, Validation Loss: 2235.177001953125\n",
      "Step 478/1000, Loss: 589.0880126953125, Validation Loss: 1941.952392578125\n",
      "Step 479/1000, Loss: 511.4146728515625, Validation Loss: 1669.264892578125\n",
      "Step 480/1000, Loss: 439.21734619140625, Validation Loss: 1417.130859375\n",
      "Step 481/1000, Loss: 372.5022277832031, Validation Loss: 1185.5745849609375\n",
      "Step 482/1000, Loss: 311.2734680175781, Validation Loss: 974.6064453125\n",
      "Step 483/1000, Loss: 255.5343475341797, Validation Loss: 784.2537841796875\n",
      "Step 484/1000, Loss: 205.29299926757812, Validation Loss: 614.534423828125\n",
      "Step 485/1000, Loss: 160.5535125732422, Validation Loss: 465.4713134765625\n",
      "Step 486/1000, Loss: 121.32243347167969, Validation Loss: 337.0826110839844\n",
      "Step 487/1000, Loss: 87.60416412353516, Validation Loss: 229.39004516601562\n",
      "Step 488/1000, Loss: 59.40543746948242, Validation Loss: 142.4192352294922\n",
      "Step 489/1000, Loss: 36.73187255859375, Validation Loss: 76.19007110595703\n",
      "Step 490/1000, Loss: 19.589139938354492, Validation Loss: 30.72600555419922\n",
      "Step 491/1000, Loss: 7.983827590942383, Validation Loss: 6.0506672859191895\n",
      "Step 492/1000, Loss: 1.922014594078064, Validation Loss: 2.18711519241333\n",
      "Step 493/1000, Loss: 1.409922480583191, Validation Loss: 14.087982177734375\n",
      "Step 494/1000, Loss: 4.997803688049316, Validation Loss: 32.46751403808594\n",
      "Step 495/1000, Loss: 10.219846725463867, Validation Loss: 51.16624450683594\n",
      "Step 496/1000, Loss: 15.442901611328125, Validation Loss: 66.38960266113281\n",
      "Step 497/1000, Loss: 19.66339683532715, Validation Loss: 76.10755920410156\n",
      "Step 498/1000, Loss: 22.347930908203125, Validation Loss: 79.59322357177734\n",
      "Step 499/1000, Loss: 23.309328079223633, Validation Loss: 77.05567169189453\n",
      "Step 500/1000, Loss: 22.60946273803711, Validation Loss: 69.36009216308594\n",
      "Step 501/1000, Loss: 20.4846248626709, Validation Loss: 57.81264877319336\n",
      "Step 502/1000, Loss: 17.288225173950195, Validation Loss: 50.665550231933594\n",
      "Step 503/1000, Loss: 15.303595542907715, Validation Loss: 42.539955139160156\n",
      "Step 504/1000, Loss: 13.040050506591797, Validation Loss: 33.973777770996094\n",
      "Step 505/1000, Loss: 10.642766952514648, Validation Loss: 25.49360466003418\n",
      "Step 506/1000, Loss: 8.2537841796875, Validation Loss: 17.608509063720703\n",
      "Step 507/1000, Loss: 6.010377883911133, Validation Loss: 10.79989242553711\n",
      "Step 508/1000, Loss: 4.042189598083496, Validation Loss: 5.518596649169922\n",
      "Step 509/1000, Loss: 2.4706039428710938, Validation Loss: 2.1815690994262695\n",
      "Step 510/1000, Loss: 1.4079723358154297, Validation Loss: 1.1712605953216553\n",
      "Step 511/1000, Loss: 0.9571757912635803, Validation Loss: 2.8361756801605225\n",
      "Step 512/1000, Loss: 1.2120208740234375, Validation Loss: 5.972296714782715\n",
      "Step 513/1000, Loss: 1.9040064811706543, Validation Loss: 9.232755661010742\n",
      "Step 514/1000, Loss: 2.6694984436035156, Validation Loss: 11.805418968200684\n",
      "Step 515/1000, Loss: 3.287863254547119, Validation Loss: 13.276537895202637\n",
      "Step 516/1000, Loss: 3.645122528076172, Validation Loss: 13.52274227142334\n",
      "Step 517/1000, Loss: 3.7051329612731934, Validation Loss: 12.629339218139648\n",
      "Step 518/1000, Loss: 3.4876933097839355, Validation Loss: 10.826242446899414\n",
      "Step 519/1000, Loss: 3.051456928253174, Validation Loss: 8.440138816833496\n",
      "Step 520/1000, Loss: 2.4811391830444336, Validation Loss: 5.85709285736084\n",
      "Step 521/1000, Loss: 1.8775681257247925, Validation Loss: 3.495736837387085\n",
      "Step 522/1000, Loss: 1.3504304885864258, Validation Loss: 1.7867580652236938\n",
      "Step 523/1000, Loss: 1.0128463506698608, Validation Loss: 1.1582772731781006\n",
      "Step 524/1000, Loss: 0.9774195551872253, Validation Loss: 1.5293270349502563\n",
      "Step 525/1000, Loss: 1.1694308519363403, Validation Loss: 2.0995419025421143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 526/1000, Loss: 1.379382610321045, Validation Loss: 2.4441232681274414\n",
      "Step 527/1000, Loss: 1.4975318908691406, Validation Loss: 2.4130754470825195\n",
      "Step 528/1000, Loss: 1.4870362281799316, Validation Loss: 2.054596185684204\n",
      "Step 529/1000, Loss: 1.3636200428009033, Validation Loss: 1.554507851600647\n",
      "Step 530/1000, Loss: 1.1793839931488037, Validation Loss: 1.1912219524383545\n",
      "Step 531/1000, Loss: 1.0108976364135742, Validation Loss: 1.3006043434143066\n",
      "Step 532/1000, Loss: 0.9497966170310974, Validation Loss: 1.6812145709991455\n",
      "Step 533/1000, Loss: 0.9958732724189758, Validation Loss: 1.920506238937378\n",
      "Step 534/1000, Loss: 1.0356405973434448, Validation Loss: 1.8756781816482544\n",
      "Step 535/1000, Loss: 1.0278689861297607, Validation Loss: 1.597265601158142\n",
      "Step 536/1000, Loss: 0.9831985831260681, Validation Loss: 1.2706530094146729\n",
      "Step 537/1000, Loss: 0.9485129117965698, Validation Loss: 1.171327829360962\n",
      "Step 538/1000, Loss: 0.9949738383293152, Validation Loss: 1.291796088218689\n",
      "Step 539/1000, Loss: 1.0669238567352295, Validation Loss: 1.341784119606018\n",
      "Step 540/1000, Loss: 1.090307593345642, Validation Loss: 1.26310396194458\n",
      "Step 541/1000, Loss: 1.0525850057601929, Validation Loss: 1.1623384952545166\n",
      "Step 542/1000, Loss: 0.9848798513412476, Validation Loss: 1.2593269348144531\n",
      "Step 543/1000, Loss: 0.9482215642929077, Validation Loss: 1.4169881343841553\n",
      "Step 544/1000, Loss: 0.9597554206848145, Validation Loss: 1.4277650117874146\n",
      "Step 545/1000, Loss: 0.9609451293945312, Validation Loss: 1.2932910919189453\n",
      "Step 546/1000, Loss: 0.9493324160575867, Validation Loss: 1.1625068187713623\n",
      "Step 547/1000, Loss: 0.962012529373169, Validation Loss: 1.1585423946380615\n",
      "Step 548/1000, Loss: 0.9779855608940125, Validation Loss: 1.1582531929016113\n",
      "Step 549/1000, Loss: 0.9674986004829407, Validation Loss: 1.2194607257843018\n",
      "Step 550/1000, Loss: 0.948736846446991, Validation Loss: 1.525753378868103\n",
      "Step 551/1000, Loss: 0.9731238484382629, Validation Loss: 1.7476447820663452\n",
      "Step 552/1000, Loss: 1.0064646005630493, Validation Loss: 1.7241828441619873\n",
      "Step 553/1000, Loss: 1.0026788711547852, Validation Loss: 1.491921067237854\n",
      "Step 554/1000, Loss: 0.9686689376831055, Validation Loss: 1.225391149520874\n",
      "Step 555/1000, Loss: 0.948397159576416, Validation Loss: 1.175734281539917\n",
      "Step 556/1000, Loss: 0.9551680684089661, Validation Loss: 1.191004753112793\n",
      "Step 557/1000, Loss: 0.9515723586082458, Validation Loss: 1.307214379310608\n",
      "Step 558/1000, Loss: 0.9499829411506653, Validation Loss: 1.3253482580184937\n",
      "Step 559/1000, Loss: 0.9511726498603821, Validation Loss: 1.2301381826400757\n",
      "Step 560/1000, Loss: 0.9481641054153442, Validation Loss: 1.2499480247497559\n",
      "Step 561/1000, Loss: 0.9479188919067383, Validation Loss: 1.1879147291183472\n",
      "Step 562/1000, Loss: 0.9520263075828552, Validation Loss: 1.205224633216858\n",
      "Step 563/1000, Loss: 0.9495916366577148, Validation Loss: 1.3328019380569458\n",
      "Step 564/1000, Loss: 0.95166015625, Validation Loss: 1.3496214151382446\n",
      "Step 565/1000, Loss: 0.9529996514320374, Validation Loss: 1.2447943687438965\n",
      "Step 566/1000, Loss: 0.9478063583374023, Validation Loss: 1.264674425125122\n",
      "Step 567/1000, Loss: 0.9479954242706299, Validation Loss: 1.1956946849822998\n",
      "Step 568/1000, Loss: 0.9505745768547058, Validation Loss: 1.213836669921875\n",
      "Step 569/1000, Loss: 0.9487196207046509, Validation Loss: 1.347246766090393\n",
      "Step 570/1000, Loss: 0.952724039554596, Validation Loss: 1.3631395101547241\n",
      "Step 571/1000, Loss: 0.9541077613830566, Validation Loss: 1.2530267238616943\n",
      "Step 572/1000, Loss: 0.9476993083953857, Validation Loss: 1.157649278640747\n",
      "Step 573/1000, Loss: 0.9684175848960876, Validation Loss: 1.1639083623886108\n",
      "Step 574/1000, Loss: 0.9860966801643372, Validation Loss: 1.1574020385742188\n",
      "Step 575/1000, Loss: 0.9732576012611389, Validation Loss: 1.2018145322799683\n",
      "Step 576/1000, Loss: 0.9496179223060608, Validation Loss: 1.484399437904358\n",
      "Step 577/1000, Loss: 0.9675821661949158, Validation Loss: 1.6996564865112305\n",
      "Step 578/1000, Loss: 0.9988276958465576, Validation Loss: 1.6818355321884155\n",
      "Step 579/1000, Loss: 0.9960160255432129, Validation Loss: 1.463172435760498\n",
      "Step 580/1000, Loss: 0.9649087190628052, Validation Loss: 1.2142797708511353\n",
      "Step 581/1000, Loss: 0.9484668374061584, Validation Loss: 1.1707359552383423\n",
      "Step 582/1000, Loss: 0.9563290476799011, Validation Loss: 1.1847953796386719\n",
      "Step 583/1000, Loss: 0.9521484971046448, Validation Loss: 1.2948882579803467\n",
      "Step 584/1000, Loss: 0.9489400386810303, Validation Loss: 1.313553810119629\n",
      "Step 585/1000, Loss: 0.9500436782836914, Validation Loss: 1.223390817642212\n",
      "Step 586/1000, Loss: 0.9479284286499023, Validation Loss: 1.2430689334869385\n",
      "Step 587/1000, Loss: 0.9474858045578003, Validation Loss: 1.3927767276763916\n",
      "Step 588/1000, Loss: 0.9568653702735901, Validation Loss: 1.405462622642517\n",
      "Step 589/1000, Loss: 0.9581969380378723, Validation Loss: 1.2795292139053345\n",
      "Step 590/1000, Loss: 0.9481675028800964, Validation Loss: 1.1603620052337646\n",
      "Step 591/1000, Loss: 0.962957501411438, Validation Loss: 1.159563422203064\n",
      "Step 592/1000, Loss: 0.9792509078979492, Validation Loss: 1.1575566530227661\n",
      "Step 593/1000, Loss: 0.9681899547576904, Validation Loss: 1.2137486934661865\n",
      "Step 594/1000, Loss: 0.9483340978622437, Validation Loss: 1.5118852853775024\n",
      "Step 595/1000, Loss: 0.9711304306983948, Validation Loss: 1.7310593128204346\n",
      "Step 596/1000, Loss: 1.0039087533950806, Validation Loss: 1.7096309661865234\n",
      "Step 597/1000, Loss: 1.0004559755325317, Validation Loss: 1.482499361038208\n",
      "Step 598/1000, Loss: 0.9672787189483643, Validation Loss: 1.2220877408981323\n",
      "Step 599/1000, Loss: 0.9478285312652588, Validation Loss: 1.1743030548095703\n",
      "Step 600/1000, Loss: 0.9547170400619507, Validation Loss: 1.189296007156372\n",
      "Step 601/1000, Loss: 0.9510394930839539, Validation Loss: 1.3036209344863892\n",
      "Step 602/1000, Loss: 0.9492899179458618, Validation Loss: 1.3126142024993896\n",
      "Step 603/1000, Loss: 0.9498512148857117, Validation Loss: 1.2626237869262695\n",
      "Step 604/1000, Loss: 0.9475288391113281, Validation Loss: 1.1903743743896484\n",
      "Step 605/1000, Loss: 0.9508317708969116, Validation Loss: 1.1751093864440918\n",
      "Step 606/1000, Loss: 0.9544049501419067, Validation Loss: 1.186204433441162\n",
      "Step 607/1000, Loss: 0.9515827298164368, Validation Loss: 1.2373279333114624\n",
      "Step 608/1000, Loss: 0.9473593235015869, Validation Loss: 1.3730111122131348\n",
      "Step 609/1000, Loss: 0.9548116326332092, Validation Loss: 1.45737624168396\n",
      "Step 610/1000, Loss: 0.9641172289848328, Validation Loss: 1.452002763748169\n",
      "Step 611/1000, Loss: 0.9634619951248169, Validation Loss: 1.3675366640090942\n",
      "Step 612/1000, Loss: 0.9542834758758545, Validation Loss: 1.2488865852355957\n",
      "Step 613/1000, Loss: 0.947303056716919, Validation Loss: 1.1640316247940063\n",
      "Step 614/1000, Loss: 0.9595077037811279, Validation Loss: 1.1584837436676025\n",
      "Step 615/1000, Loss: 0.976729154586792, Validation Loss: 1.16106379032135\n",
      "Step 616/1000, Loss: 0.9816006422042847, Validation Loss: 1.1572437286376953\n",
      "Step 617/1000, Loss: 0.9717022776603699, Validation Loss: 1.172654151916504\n",
      "Step 618/1000, Loss: 0.955183207988739, Validation Loss: 1.2605780363082886\n",
      "Step 619/1000, Loss: 0.9474273920059204, Validation Loss: 1.3328602313995361\n",
      "Step 620/1000, Loss: 0.9512632489204407, Validation Loss: 1.3395891189575195\n",
      "Step 621/1000, Loss: 0.9517964720726013, Validation Loss: 1.282739281654358\n",
      "Step 622/1000, Loss: 0.9481419324874878, Validation Loss: 1.2008554935455322\n",
      "Step 623/1000, Loss: 0.9492889642715454, Validation Loss: 1.1821649074554443\n",
      "Step 624/1000, Loss: 0.9523540139198303, Validation Loss: 1.1940865516662598\n",
      "Step 625/1000, Loss: 0.9501430988311768, Validation Loss: 1.2487680912017822\n",
      "Step 626/1000, Loss: 0.9472442269325256, Validation Loss: 1.261012315750122\n",
      "Step 627/1000, Loss: 0.9474065899848938, Validation Loss: 1.2247751951217651\n",
      "Step 628/1000, Loss: 0.9475769400596619, Validation Loss: 1.237735629081726\n",
      "Step 629/1000, Loss: 0.9472591280937195, Validation Loss: 1.3055107593536377\n",
      "Step 630/1000, Loss: 0.9493159055709839, Validation Loss: 1.3143681287765503\n",
      "Step 631/1000, Loss: 0.9498800039291382, Validation Loss: 1.2639890909194946\n",
      "Step 632/1000, Loss: 0.9474466443061829, Validation Loss: 1.1911578178405762\n",
      "Step 633/1000, Loss: 0.950532078742981, Validation Loss: 1.1756486892700195\n",
      "Step 634/1000, Loss: 0.9540441036224365, Validation Loss: 1.186815857887268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 635/1000, Loss: 0.9512858986854553, Validation Loss: 1.2381614446640015\n",
      "Step 636/1000, Loss: 0.9472114443778992, Validation Loss: 1.3739938735961914\n",
      "Step 637/1000, Loss: 0.9548269510269165, Validation Loss: 1.4582802057266235\n",
      "Step 638/1000, Loss: 0.9641759395599365, Validation Loss: 1.4527688026428223\n",
      "Step 639/1000, Loss: 0.9635014533996582, Validation Loss: 1.3681285381317139\n",
      "Step 640/1000, Loss: 0.9542589783668518, Validation Loss: 1.249311923980713\n",
      "Step 641/1000, Loss: 0.9471644759178162, Validation Loss: 1.1641597747802734\n",
      "Step 642/1000, Loss: 0.9591829776763916, Validation Loss: 1.1583895683288574\n",
      "Step 643/1000, Loss: 0.9762828350067139, Validation Loss: 1.1609314680099487\n",
      "Step 644/1000, Loss: 0.9811369776725769, Validation Loss: 1.1572062969207764\n",
      "Step 645/1000, Loss: 0.9713033437728882, Validation Loss: 1.1728005409240723\n",
      "Step 646/1000, Loss: 0.9549158811569214, Validation Loss: 1.2607877254486084\n",
      "Step 647/1000, Loss: 0.9472977519035339, Validation Loss: 1.3329943418502808\n",
      "Step 648/1000, Loss: 0.9511778354644775, Validation Loss: 1.3396656513214111\n",
      "Step 649/1000, Loss: 0.951710045337677, Validation Loss: 1.2828551530838013\n",
      "Step 650/1000, Loss: 0.94802325963974, Validation Loss: 1.2009859085083008\n",
      "Step 651/1000, Loss: 0.9490972757339478, Validation Loss: 1.1822619438171387\n",
      "Step 652/1000, Loss: 0.9521335363388062, Validation Loss: 1.19417142868042\n",
      "Step 653/1000, Loss: 0.9499473571777344, Validation Loss: 1.2487921714782715\n",
      "Step 654/1000, Loss: 0.9471045732498169, Validation Loss: 1.2610286474227905\n",
      "Step 655/1000, Loss: 0.9472720623016357, Validation Loss: 1.2247862815856934\n",
      "Step 656/1000, Loss: 0.9474186897277832, Validation Loss: 1.2377209663391113\n",
      "Step 657/1000, Loss: 0.9471133947372437, Validation Loss: 1.3053853511810303\n",
      "Step 658/1000, Loss: 0.9491972923278809, Validation Loss: 1.3142354488372803\n",
      "Step 659/1000, Loss: 0.9497693181037903, Validation Loss: 1.2639182806015015\n",
      "Step 660/1000, Loss: 0.9473153948783875, Validation Loss: 1.1911613941192627\n",
      "Step 661/1000, Loss: 0.9503456950187683, Validation Loss: 1.1756460666656494\n",
      "Step 662/1000, Loss: 0.9538329839706421, Validation Loss: 1.186803936958313\n",
      "Step 663/1000, Loss: 0.9510964155197144, Validation Loss: 1.238134503364563\n",
      "Step 664/1000, Loss: 0.9470675587654114, Validation Loss: 1.373748779296875\n",
      "Step 665/1000, Loss: 0.954727292060852, Validation Loss: 1.457895040512085\n",
      "Step 666/1000, Loss: 0.9640838503837585, Validation Loss: 1.4523646831512451\n",
      "Step 667/1000, Loss: 0.96340411901474, Validation Loss: 1.367873191833496\n",
      "Step 668/1000, Loss: 0.954154372215271, Validation Loss: 1.2492705583572388\n",
      "Step 669/1000, Loss: 0.9470286965370178, Validation Loss: 1.1641777753829956\n",
      "Step 670/1000, Loss: 0.9589443802833557, Validation Loss: 1.158342957496643\n",
      "Step 671/1000, Loss: 0.9759701490402222, Validation Loss: 1.1608619689941406\n",
      "Step 672/1000, Loss: 0.980810821056366, Validation Loss: 1.1571879386901855\n",
      "Step 673/1000, Loss: 0.9710227251052856, Validation Loss: 1.1727972030639648\n",
      "Step 674/1000, Loss: 0.9547113180160522, Validation Loss: 1.2606770992279053\n",
      "Step 675/1000, Loss: 0.9471709132194519, Validation Loss: 1.3327507972717285\n",
      "Step 676/1000, Loss: 0.9510677456855774, Validation Loss: 1.3394252061843872\n",
      "Step 677/1000, Loss: 0.9516012072563171, Validation Loss: 1.282684326171875\n",
      "Step 678/1000, Loss: 0.9479045271873474, Validation Loss: 1.200929880142212\n",
      "Step 679/1000, Loss: 0.948937714099884, Validation Loss: 1.182244896888733\n",
      "Step 680/1000, Loss: 0.9519506692886353, Validation Loss: 1.1941593885421753\n",
      "Step 681/1000, Loss: 0.9497798681259155, Validation Loss: 1.2487478256225586\n",
      "Step 682/1000, Loss: 0.9469751715660095, Validation Loss: 1.2609360218048096\n",
      "Step 683/1000, Loss: 0.9471502304077148, Validation Loss: 1.224774956703186\n",
      "Step 684/1000, Loss: 0.9472779631614685, Validation Loss: 1.2377227544784546\n",
      "Step 685/1000, Loss: 0.9469768404960632, Validation Loss: 1.305293083190918\n",
      "Step 686/1000, Loss: 0.9490923285484314, Validation Loss: 1.3141226768493652\n",
      "Step 687/1000, Loss: 0.9496620297431946, Validation Loss: 1.2638612985610962\n",
      "Step 688/1000, Loss: 0.9471957683563232, Validation Loss: 1.1911728382110596\n",
      "Step 689/1000, Loss: 0.950175940990448, Validation Loss: 1.1756805181503296\n",
      "Step 690/1000, Loss: 0.9536380767822266, Validation Loss: 1.1868467330932617\n",
      "Step 691/1000, Loss: 0.9509162306785583, Validation Loss: 1.238102674484253\n",
      "Step 692/1000, Loss: 0.9469357132911682, Validation Loss: 1.3735239505767822\n",
      "Step 693/1000, Loss: 0.9546399712562561, Validation Loss: 1.4575527906417847\n",
      "Step 694/1000, Loss: 0.9640044569969177, Validation Loss: 1.4520130157470703\n",
      "Step 695/1000, Loss: 0.9633238315582275, Validation Loss: 1.3677035570144653\n",
      "Step 696/1000, Loss: 0.9540658593177795, Validation Loss: 1.2492291927337646\n",
      "Step 697/1000, Loss: 0.9469037652015686, Validation Loss: 1.1641836166381836\n",
      "Step 698/1000, Loss: 0.9587273001670837, Validation Loss: 1.158300518989563\n",
      "Step 699/1000, Loss: 0.9756792187690735, Validation Loss: 1.1607881784439087\n",
      "Step 700/1000, Loss: 0.980497419834137, Validation Loss: 1.1571632623672485\n",
      "Step 701/1000, Loss: 0.9707564115524292, Validation Loss: 1.1728169918060303\n",
      "Step 702/1000, Loss: 0.9545166492462158, Validation Loss: 1.2070773839950562\n",
      "Step 703/1000, Loss: 0.9482107162475586, Validation Loss: 1.2808414697647095\n",
      "Step 704/1000, Loss: 0.9477250576019287, Validation Loss: 1.3397414684295654\n",
      "Step 705/1000, Loss: 0.9515476226806641, Validation Loss: 1.3646416664123535\n",
      "Step 706/1000, Loss: 0.9537574648857117, Validation Loss: 1.3517452478408813\n",
      "Step 707/1000, Loss: 0.9525759220123291, Validation Loss: 1.307973027229309\n",
      "Step 708/1000, Loss: 0.9491957426071167, Validation Loss: 1.2477807998657227\n",
      "Step 709/1000, Loss: 0.946862518787384, Validation Loss: 1.1905121803283691\n",
      "Step 710/1000, Loss: 0.9501875042915344, Validation Loss: 1.170579195022583\n",
      "Step 711/1000, Loss: 0.9553543925285339, Validation Loss: 1.166286826133728\n",
      "Step 712/1000, Loss: 0.9573909640312195, Validation Loss: 1.1708787679672241\n",
      "Step 713/1000, Loss: 0.9552280902862549, Validation Loss: 1.1882593631744385\n",
      "Step 714/1000, Loss: 0.9505612254142761, Validation Loss: 1.2297885417938232\n",
      "Step 715/1000, Loss: 0.947005033493042, Validation Loss: 1.3116552829742432\n",
      "Step 716/1000, Loss: 0.9494288563728333, Validation Loss: 1.3729275465011597\n",
      "Step 717/1000, Loss: 0.9545444250106812, Validation Loss: 1.3962634801864624\n",
      "Step 718/1000, Loss: 0.9569213390350342, Validation Loss: 1.3791943788528442\n",
      "Step 719/1000, Loss: 0.9551600813865662, Validation Loss: 1.3297491073608398\n",
      "Step 720/1000, Loss: 0.9507333040237427, Validation Loss: 1.2630717754364014\n",
      "Step 721/1000, Loss: 0.9470852613449097, Validation Loss: 1.1990275382995605\n",
      "Step 722/1000, Loss: 0.9489763379096985, Validation Loss: 1.1755692958831787\n",
      "Step 723/1000, Loss: 0.9535383582115173, Validation Loss: 1.170026421546936\n",
      "Step 724/1000, Loss: 0.9555500149726868, Validation Loss: 1.1749080419540405\n",
      "Step 725/1000, Loss: 0.9537467360496521, Validation Loss: 1.1935935020446777\n",
      "Step 726/1000, Loss: 0.9496778845787048, Validation Loss: 1.2369874715805054\n",
      "Step 727/1000, Loss: 0.9468569755554199, Validation Loss: 1.3210418224334717\n",
      "Step 728/1000, Loss: 0.9500695466995239, Validation Loss: 1.3828413486480713\n",
      "Step 729/1000, Loss: 0.9555216431617737, Validation Loss: 1.4056566953659058\n",
      "Step 730/1000, Loss: 0.957922637462616, Validation Loss: 1.3873264789581299\n",
      "Step 731/1000, Loss: 0.9559779763221741, Validation Loss: 1.3362126350402832\n",
      "Step 732/1000, Loss: 0.9512351751327515, Validation Loss: 1.267653226852417\n",
      "Step 733/1000, Loss: 0.9471977353096008, Validation Loss: 1.2016611099243164\n",
      "Step 734/1000, Loss: 0.9486637115478516, Validation Loss: 1.1771552562713623\n",
      "Step 735/1000, Loss: 0.9530414938926697, Validation Loss: 1.1712348461151123\n",
      "Step 736/1000, Loss: 0.9550345540046692, Validation Loss: 1.1761797666549683\n",
      "Step 737/1000, Loss: 0.9533258676528931, Validation Loss: 1.1952176094055176\n",
      "Step 738/1000, Loss: 0.9494306445121765, Validation Loss: 1.2391198873519897\n",
      "Step 739/1000, Loss: 0.9468215703964233, Validation Loss: 1.3237459659576416\n",
      "Step 740/1000, Loss: 0.9502573609352112, Validation Loss: 1.3856991529464722\n",
      "Step 741/1000, Loss: 0.9558030962944031, Validation Loss: 1.4083524942398071\n",
      "Step 742/1000, Loss: 0.958210825920105, Validation Loss: 1.3896846771240234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 743/1000, Loss: 0.9562124609947205, Validation Loss: 1.3380928039550781\n",
      "Step 744/1000, Loss: 0.9513776302337646, Validation Loss: 1.2689908742904663\n",
      "Step 745/1000, Loss: 0.9472251534461975, Validation Loss: 1.202425241470337\n",
      "Step 746/1000, Loss: 0.948564350605011, Validation Loss: 1.177622675895691\n",
      "Step 747/1000, Loss: 0.9528842568397522, Validation Loss: 1.1715764999389648\n",
      "Step 748/1000, Loss: 0.9548774361610413, Validation Loss: 1.1765323877334595\n",
      "Step 749/1000, Loss: 0.9531958103179932, Validation Loss: 1.1956541538238525\n",
      "Step 750/1000, Loss: 0.9493494033813477, Validation Loss: 1.2396565675735474\n",
      "Step 751/1000, Loss: 0.9468012452125549, Validation Loss: 1.3243967294692993\n",
      "Step 752/1000, Loss: 0.9502936005592346, Validation Loss: 1.3863190412521362\n",
      "Step 753/1000, Loss: 0.9558566808700562, Validation Loss: 1.4088730812072754\n",
      "Step 754/1000, Loss: 0.9582604765892029, Validation Loss: 1.390160322189331\n",
      "Step 755/1000, Loss: 0.9562509059906006, Validation Loss: 1.3384685516357422\n",
      "Step 756/1000, Loss: 0.9513944387435913, Validation Loss: 1.269301176071167\n",
      "Step 757/1000, Loss: 0.9472178220748901, Validation Loss: 1.2026339769363403\n",
      "Step 758/1000, Loss: 0.9485187530517578, Validation Loss: 1.1777459383010864\n",
      "Step 759/1000, Loss: 0.9528201222419739, Validation Loss: 1.171678900718689\n",
      "Step 760/1000, Loss: 0.9548061490058899, Validation Loss: 1.1766388416290283\n",
      "Step 761/1000, Loss: 0.9531353116035461, Validation Loss: 1.1957966089248657\n",
      "Step 762/1000, Loss: 0.9493061304092407, Validation Loss: 1.2398614883422852\n",
      "Step 763/1000, Loss: 0.9467810392379761, Validation Loss: 1.32460618019104\n",
      "Step 764/1000, Loss: 0.9502980709075928, Validation Loss: 1.3865277767181396\n",
      "Step 765/1000, Loss: 0.9558725953102112, Validation Loss: 1.4091154336929321\n",
      "Step 766/1000, Loss: 0.9582834243774414, Validation Loss: 1.3903621435165405\n",
      "Step 767/1000, Loss: 0.9562686085700989, Validation Loss: 1.3386726379394531\n",
      "Step 768/1000, Loss: 0.9514026641845703, Validation Loss: 1.2694463729858398\n",
      "Step 769/1000, Loss: 0.9472060799598694, Validation Loss: 1.2027310132980347\n",
      "Step 770/1000, Loss: 0.9484837651252747, Validation Loss: 1.1778239011764526\n",
      "Step 771/1000, Loss: 0.9527691006660461, Validation Loss: 1.1717489957809448\n",
      "Step 772/1000, Loss: 0.9547469019889832, Validation Loss: 1.1767168045043945\n",
      "Step 773/1000, Loss: 0.9530832171440125, Validation Loss: 1.1959145069122314\n",
      "Step 774/1000, Loss: 0.9492651224136353, Validation Loss: 1.2400147914886475\n",
      "Step 775/1000, Loss: 0.9467588663101196, Validation Loss: 1.3248155117034912\n",
      "Step 776/1000, Loss: 0.9502993822097778, Validation Loss: 1.3867765665054321\n",
      "Step 777/1000, Loss: 0.9558858275413513, Validation Loss: 1.409298062324524\n",
      "Step 778/1000, Loss: 0.9582924246788025, Validation Loss: 1.3905309438705444\n",
      "Step 779/1000, Loss: 0.9562727808952332, Validation Loss: 1.3388067483901978\n",
      "Step 780/1000, Loss: 0.9514008164405823, Validation Loss: 1.2695879936218262\n",
      "Step 781/1000, Loss: 0.9471904635429382, Validation Loss: 1.202825665473938\n",
      "Step 782/1000, Loss: 0.9484512209892273, Validation Loss: 1.1778618097305298\n",
      "Step 783/1000, Loss: 0.9527299404144287, Validation Loss: 1.1717817783355713\n",
      "Step 784/1000, Loss: 0.9547067880630493, Validation Loss: 1.1767616271972656\n",
      "Step 785/1000, Loss: 0.9530420303344727, Validation Loss: 1.1959524154663086\n",
      "Step 786/1000, Loss: 0.949236273765564, Validation Loss: 1.2400436401367188\n",
      "Step 787/1000, Loss: 0.9467375874519348, Validation Loss: 1.324841856956482\n",
      "Step 788/1000, Loss: 0.9502833485603333, Validation Loss: 1.3867881298065186\n",
      "Step 789/1000, Loss: 0.9558694958686829, Validation Loss: 1.4092633724212646\n",
      "Step 790/1000, Loss: 0.9582712054252625, Validation Loss: 1.3905012607574463\n",
      "Step 791/1000, Loss: 0.9562527537345886, Validation Loss: 1.338761568069458\n",
      "Step 792/1000, Loss: 0.951380729675293, Validation Loss: 1.2695428133010864\n",
      "Step 793/1000, Loss: 0.94716876745224, Validation Loss: 1.2028019428253174\n",
      "Step 794/1000, Loss: 0.9484295845031738, Validation Loss: 1.1778640747070312\n",
      "Step 795/1000, Loss: 0.9527041912078857, Validation Loss: 1.1717731952667236\n",
      "Step 796/1000, Loss: 0.9546792507171631, Validation Loss: 1.176759958267212\n",
      "Step 797/1000, Loss: 0.9530175924301147, Validation Loss: 1.1959328651428223\n",
      "Step 798/1000, Loss: 0.9492138028144836, Validation Loss: 1.2400014400482178\n",
      "Step 799/1000, Loss: 0.9467170834541321, Validation Loss: 1.3247666358947754\n",
      "Step 800/1000, Loss: 0.9502626061439514, Validation Loss: 1.3866521120071411\n",
      "Early stopping triggered\n",
      "Step 1/1000, Loss: 0.9560986757278442, Validation Loss: 38.50931930541992\n",
      "Step 2/1000, Loss: 11.88844108581543, Validation Loss: 34.38869857788086\n",
      "Step 3/1000, Loss: 10.735108375549316, Validation Loss: 10.855230331420898\n",
      "Step 4/1000, Loss: 4.044076442718506, Validation Loss: 1.6232725381851196\n",
      "Step 5/1000, Loss: 0.987224280834198, Validation Loss: 4.931600570678711\n",
      "Step 6/1000, Loss: 1.6719269752502441, Validation Loss: 3.4895095825195312\n",
      "Step 7/1000, Loss: 1.3525277376174927, Validation Loss: 1.1626654863357544\n",
      "Step 8/1000, Loss: 0.982633650302887, Validation Loss: 1.2119133472442627\n",
      "Step 9/1000, Loss: 1.020993709564209, Validation Loss: 1.6582963466644287\n",
      "Step 10/1000, Loss: 0.9926170706748962, Validation Loss: 1.3998676538467407\n",
      "Step 11/1000, Loss: 0.9574233889579773, Validation Loss: 1.5825536251068115\n",
      "Step 12/1000, Loss: 1.1852792501449585, Validation Loss: 1.6735892295837402\n",
      "Step 13/1000, Loss: 1.2199136018753052, Validation Loss: 1.1690336465835571\n",
      "Step 14/1000, Loss: 0.9560431241989136, Validation Loss: 3.9848034381866455\n",
      "Step 15/1000, Loss: 1.4605872631072998, Validation Loss: 6.117302894592285\n",
      "Step 16/1000, Loss: 1.9439167976379395, Validation Loss: 4.754453182220459\n",
      "Step 17/1000, Loss: 1.632236123085022, Validation Loss: 1.821179986000061\n",
      "Step 18/1000, Loss: 1.019308090209961, Validation Loss: 1.994966745376587\n",
      "Step 19/1000, Loss: 1.3361647129058838, Validation Loss: 3.989875555038452\n",
      "Step 20/1000, Loss: 1.988103985786438, Validation Loss: 3.759831190109253\n",
      "Step 21/1000, Loss: 1.9158068895339966, Validation Loss: 1.815558671951294\n",
      "Step 22/1000, Loss: 1.2719730138778687, Validation Loss: 1.5578510761260986\n",
      "Step 23/1000, Loss: 0.9775277972221375, Validation Loss: 2.586148500442505\n",
      "Step 24/1000, Loss: 1.1638171672821045, Validation Loss: 2.118655204772949\n",
      "Step 25/1000, Loss: 1.0727401971817017, Validation Loss: 1.163576364517212\n",
      "Step 26/1000, Loss: 0.9589548110961914, Validation Loss: 1.1578518152236938\n",
      "Step 27/1000, Loss: 0.9731429815292358, Validation Loss: 1.5028760433197021\n",
      "Step 28/1000, Loss: 0.9698941707611084, Validation Loss: 1.3489818572998047\n",
      "Step 29/1000, Loss: 0.9522866606712341, Validation Loss: 1.3079530000686646\n",
      "Step 30/1000, Loss: 1.0702780485153198, Validation Loss: 1.3505425453186035\n",
      "Step 31/1000, Loss: 1.089630126953125, Validation Loss: 1.197153925895691\n",
      "Step 32/1000, Loss: 0.9489506483078003, Validation Loss: 3.274271249771118\n",
      "Step 33/1000, Loss: 1.3069117069244385, Validation Loss: 4.770539283752441\n",
      "Step 34/1000, Loss: 1.6363739967346191, Validation Loss: 3.7910032272338867\n",
      "Step 35/1000, Loss: 1.4185750484466553, Validation Loss: 1.6437331438064575\n",
      "Step 36/1000, Loss: 0.9903246760368347, Validation Loss: 1.8092446327209473\n",
      "Step 37/1000, Loss: 1.2690589427947998, Validation Loss: 3.3655319213867188\n",
      "Step 38/1000, Loss: 1.7897762060165405, Validation Loss: 3.1857943534851074\n",
      "Step 39/1000, Loss: 1.7321367263793945, Validation Loss: 1.6456825733184814\n",
      "Step 40/1000, Loss: 1.2083940505981445, Validation Loss: 1.5440458059310913\n",
      "Step 41/1000, Loss: 0.9755130410194397, Validation Loss: 2.4591169357299805\n",
      "Step 42/1000, Loss: 1.1386460065841675, Validation Loss: 2.0416862964630127\n",
      "Step 43/1000, Loss: 1.0585825443267822, Validation Loss: 1.1661144495010376\n",
      "Step 44/1000, Loss: 0.9569351077079773, Validation Loss: 1.1572879552841187\n",
      "Step 45/1000, Loss: 0.9694247245788574, Validation Loss: 1.490749478340149\n",
      "Step 46/1000, Loss: 0.9682008624076843, Validation Loss: 1.3454952239990234\n",
      "Step 47/1000, Loss: 0.9518138766288757, Validation Loss: 1.2861191034317017\n",
      "Step 48/1000, Loss: 1.0593241453170776, Validation Loss: 1.32436203956604\n",
      "Step 49/1000, Loss: 1.0772403478622437, Validation Loss: 1.2010160684585571\n",
      "Step 50/1000, Loss: 0.9482004642486572, Validation Loss: 3.1951897144317627\n",
      "Step 51/1000, Loss: 1.290496587753296, Validation Loss: 4.622069835662842\n",
      "Step 52/1000, Loss: 1.6035692691802979, Validation Loss: 3.684952974319458\n",
      "Step 53/1000, Loss: 1.3959167003631592, Validation Loss: 1.625292420387268\n",
      "Step 54/1000, Loss: 0.9874571561813354, Validation Loss: 1.7841942310333252\n",
      "Step 55/1000, Loss: 1.2591418027877808, Validation Loss: 3.2851200103759766\n",
      "Step 56/1000, Loss: 1.7629213333129883, Validation Loss: 3.111746072769165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 57/1000, Loss: 1.7072017192840576, Validation Loss: 1.6235445737838745\n",
      "Step 58/1000, Loss: 1.1992499828338623, Validation Loss: 1.542887568473816\n",
      "Step 59/1000, Loss: 0.9752854704856873, Validation Loss: 2.443331003189087\n",
      "Step 60/1000, Loss: 1.1357545852661133, Validation Loss: 2.032416582107544\n",
      "Step 61/1000, Loss: 1.057009220123291, Validation Loss: 1.166579246520996\n",
      "Step 62/1000, Loss: 0.9562684893608093, Validation Loss: 1.1571964025497437\n",
      "Step 63/1000, Loss: 0.9684476852416992, Validation Loss: 1.4897422790527344\n",
      "Step 64/1000, Loss: 0.9679797291755676, Validation Loss: 1.3455274105072021\n",
      "Step 65/1000, Loss: 0.9516531229019165, Validation Loss: 1.2825161218643188\n",
      "Step 66/1000, Loss: 1.0570082664489746, Validation Loss: 1.320082187652588\n",
      "Step 67/1000, Loss: 1.0746772289276123, Validation Loss: 1.2018380165100098\n",
      "Step 68/1000, Loss: 0.9478105306625366, Validation Loss: 3.1856868267059326\n",
      "Step 69/1000, Loss: 1.2888442277908325, Validation Loss: 4.603137016296387\n",
      "Step 70/1000, Loss: 1.5999261140823364, Validation Loss: 3.671880006790161\n",
      "Step 71/1000, Loss: 1.3935599327087402, Validation Loss: 1.6235626935958862\n",
      "Step 72/1000, Loss: 0.9871771931648254, Validation Loss: 1.7792761325836182\n",
      "Step 73/1000, Loss: 1.2564990520477295, Validation Loss: 3.2710392475128174\n",
      "Step 74/1000, Loss: 1.7571743726730347, Validation Loss: 3.0985054969787598\n",
      "Step 75/1000, Loss: 1.7017494440078735, Validation Loss: 1.619103193283081\n",
      "Step 76/1000, Loss: 1.1967891454696655, Validation Loss: 1.5436511039733887\n",
      "Step 77/1000, Loss: 0.9753412008285522, Validation Loss: 2.442800760269165\n",
      "Step 78/1000, Loss: 1.1358909606933594, Validation Loss: 2.0326120853424072\n",
      "Step 79/1000, Loss: 1.0571558475494385, Validation Loss: 1.1667602062225342\n",
      "Step 80/1000, Loss: 0.9557674527168274, Validation Loss: 1.1571165323257446\n",
      "Step 81/1000, Loss: 0.9678135514259338, Validation Loss: 1.4906010627746582\n",
      "Step 82/1000, Loss: 0.9680078625679016, Validation Loss: 1.3462660312652588\n",
      "Step 83/1000, Loss: 0.9515516757965088, Validation Loss: 1.2812345027923584\n",
      "Step 84/1000, Loss: 1.0558284521102905, Validation Loss: 1.3185884952545166\n",
      "Step 85/1000, Loss: 1.0733988285064697, Validation Loss: 1.2022418975830078\n",
      "Step 86/1000, Loss: 0.947466254234314, Validation Loss: 3.186431646347046\n",
      "Step 87/1000, Loss: 1.289415717124939, Validation Loss: 4.603238105773926\n",
      "Step 88/1000, Loss: 1.6006066799163818, Validation Loss: 3.6724209785461426\n",
      "Step 89/1000, Loss: 1.394166111946106, Validation Loss: 1.6244542598724365\n",
      "Step 90/1000, Loss: 0.9872992038726807, Validation Loss: 1.7768850326538086\n",
      "Step 91/1000, Loss: 1.2548035383224487, Validation Loss: 3.265760660171509\n",
      "Step 92/1000, Loss: 1.754320740699768, Validation Loss: 3.093705892562866\n",
      "Step 93/1000, Loss: 1.6990575790405273, Validation Loss: 1.6171667575836182\n",
      "Step 94/1000, Loss: 1.1953145265579224, Validation Loss: 1.5447015762329102\n",
      "Step 95/1000, Loss: 0.9754319190979004, Validation Loss: 2.444376230239868\n",
      "Step 96/1000, Loss: 1.13643479347229, Validation Loss: 2.0339720249176025\n",
      "Step 97/1000, Loss: 1.0575441122055054, Validation Loss: 1.1668483018875122\n",
      "Step 98/1000, Loss: 0.9553163051605225, Validation Loss: 1.1570382118225098\n",
      "Step 99/1000, Loss: 0.9672533869743347, Validation Loss: 1.4915279150009155\n",
      "Step 100/1000, Loss: 0.9680556058883667, Validation Loss: 1.3469574451446533\n",
      "Step 101/1000, Loss: 0.9514517784118652, Validation Loss: 1.280426025390625\n",
      "Step 102/1000, Loss: 1.054869294166565, Validation Loss: 1.2984462976455688\n",
      "Step 103/1000, Loss: 1.063467025756836, Validation Loss: 1.1617472171783447\n",
      "Step 104/1000, Loss: 0.9797278642654419, Validation Loss: 1.4453306198120117\n",
      "Step 105/1000, Loss: 0.9621017575263977, Validation Loss: 1.7237725257873535\n",
      "Step 106/1000, Loss: 1.003129482269287, Validation Loss: 1.5385509729385376\n",
      "Step 107/1000, Loss: 0.9745227694511414, Validation Loss: 1.184767246246338\n",
      "Step 108/1000, Loss: 0.9495807886123657, Validation Loss: 1.165103793144226\n",
      "Step 109/1000, Loss: 0.9561023116111755, Validation Loss: 1.2799522876739502\n",
      "Step 110/1000, Loss: 0.946637749671936, Validation Loss: 1.2224068641662598\n",
      "Step 111/1000, Loss: 0.9458746910095215, Validation Loss: 1.4068033695220947\n",
      "Step 112/1000, Loss: 0.9575100541114807, Validation Loss: 1.3090593814849854\n",
      "Step 113/1000, Loss: 0.9483227729797363, Validation Loss: 1.1584323644638062\n",
      "Step 114/1000, Loss: 0.9739518761634827, Validation Loss: 1.165850043296814\n",
      "Step 115/1000, Loss: 0.9845104217529297, Validation Loss: 1.191110372543335\n",
      "Step 116/1000, Loss: 0.9483562707901001, Validation Loss: 1.7428407669067383\n",
      "Step 117/1000, Loss: 1.006302833557129, Validation Loss: 2.0757029056549072\n",
      "Step 118/1000, Loss: 1.0654271841049194, Validation Loss: 1.801560401916504\n",
      "Step 119/1000, Loss: 1.0162209272384644, Validation Loss: 1.2680665254592896\n",
      "Step 120/1000, Loss: 0.9460132122039795, Validation Loss: 1.3321810960769653\n",
      "Step 121/1000, Loss: 1.0784786939620972, Validation Loss: 1.7436182498931885\n",
      "Step 122/1000, Loss: 1.241431474685669, Validation Loss: 1.7181353569030762\n",
      "Step 123/1000, Loss: 1.2320002317428589, Validation Loss: 1.3206068277359009\n",
      "Step 124/1000, Loss: 1.0731292963027954, Validation Loss: 1.212132453918457\n",
      "Step 125/1000, Loss: 0.9461296200752258, Validation Loss: 2.4566211700439453\n",
      "Step 126/1000, Loss: 1.1392377614974976, Validation Loss: 3.798203229904175\n",
      "Step 127/1000, Loss: 1.4226322174072266, Validation Loss: 4.1579365730285645\n",
      "Step 128/1000, Loss: 1.50216543674469, Validation Loss: 3.405008554458618\n",
      "Step 129/1000, Loss: 1.337132453918457, Validation Loss: 2.0789072513580322\n",
      "Step 130/1000, Loss: 1.066115379333496, Validation Loss: 1.1780911684036255\n",
      "Step 131/1000, Loss: 0.9506731033325195, Validation Loss: 1.2235462665557861\n",
      "Step 132/1000, Loss: 1.0245927572250366, Validation Loss: 1.242219090461731\n",
      "Step 133/1000, Loss: 1.0347508192062378, Validation Loss: 1.1569393873214722\n",
      "Step 134/1000, Loss: 0.9662448167800903, Validation Loss: 1.523241400718689\n",
      "Step 135/1000, Loss: 0.9722976684570312, Validation Loss: 1.820214033126831\n",
      "Step 136/1000, Loss: 1.0194655656814575, Validation Loss: 1.6101524829864502\n",
      "Step 137/1000, Loss: 0.9850636124610901, Validation Loss: 1.2038880586624146\n",
      "Step 138/1000, Loss: 0.9465786218643188, Validation Loss: 1.175222396850586\n",
      "Step 139/1000, Loss: 0.9513763189315796, Validation Loss: 1.3102418184280396\n",
      "Step 140/1000, Loss: 0.9481768012046814, Validation Loss: 1.242560863494873\n",
      "Step 141/1000, Loss: 0.9452472925186157, Validation Loss: 1.1726679801940918\n",
      "Step 142/1000, Loss: 0.9905402660369873, Validation Loss: 1.186720371246338\n",
      "Step 143/1000, Loss: 1.00144624710083, Validation Loss: 1.169918417930603\n",
      "Step 144/1000, Loss: 0.9530776739120483, Validation Loss: 1.649728536605835\n",
      "Step 145/1000, Loss: 0.991228461265564, Validation Loss: 1.9697074890136719\n",
      "Step 146/1000, Loss: 1.0460889339447021, Validation Loss: 1.7219667434692383\n",
      "Step 147/1000, Loss: 1.0029078722000122, Validation Loss: 1.2394812107086182\n",
      "Step 148/1000, Loss: 0.9451091289520264, Validation Loss: 1.3685133457183838\n",
      "Step 149/1000, Loss: 1.0934866666793823, Validation Loss: 1.8022631406784058\n",
      "Step 150/1000, Loss: 1.2614120244979858, Validation Loss: 1.7698673009872437\n",
      "Step 151/1000, Loss: 1.2495821714401245, Validation Loss: 1.3462640047073364\n",
      "Step 152/1000, Loss: 1.0835860967636108, Validation Loss: 1.1997963190078735\n",
      "Step 153/1000, Loss: 0.9466450214385986, Validation Loss: 2.399791955947876\n",
      "Step 154/1000, Loss: 1.1283825635910034, Validation Loss: 3.724357843399048\n",
      "Step 155/1000, Loss: 1.4073671102523804, Validation Loss: 4.086312294006348\n",
      "Step 156/1000, Loss: 1.4873020648956299, Validation Loss: 3.348507881164551\n",
      "Step 157/1000, Loss: 1.325776219367981, Validation Loss: 2.0460572242736816\n",
      "Step 158/1000, Loss: 1.0602887868881226, Validation Loss: 1.173689365386963\n",
      "Step 159/1000, Loss: 0.951370120048523, Validation Loss: 1.2310590744018555\n",
      "Step 160/1000, Loss: 1.0278421640396118, Validation Loss: 1.2499157190322876\n",
      "Step 161/1000, Loss: 1.0378187894821167, Validation Loss: 1.156846284866333\n",
      "Step 162/1000, Loss: 0.9673694372177124, Validation Loss: 1.5099958181381226\n",
      "Step 163/1000, Loss: 0.9703949093818665, Validation Loss: 1.803745985031128\n",
      "Step 164/1000, Loss: 1.0168006420135498, Validation Loss: 1.5975998640060425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 165/1000, Loss: 0.9831781983375549, Validation Loss: 1.2001330852508545\n",
      "Step 166/1000, Loss: 0.9464661478996277, Validation Loss: 1.1730318069458008\n",
      "Step 167/1000, Loss: 0.9515097141265869, Validation Loss: 1.3043112754821777\n",
      "Step 168/1000, Loss: 0.9474997520446777, Validation Loss: 1.2384507656097412\n",
      "Step 169/1000, Loss: 0.9448766708374023, Validation Loss: 1.174399733543396\n",
      "Step 170/1000, Loss: 0.9913238286972046, Validation Loss: 1.1891199350357056\n",
      "Step 171/1000, Loss: 1.0023664236068726, Validation Loss: 1.1683001518249512\n",
      "Step 172/1000, Loss: 0.9532116055488586, Validation Loss: 1.6397072076797485\n",
      "Step 173/1000, Loss: 0.9897028803825378, Validation Loss: 1.9568394422531128\n",
      "Step 174/1000, Loss: 1.0439953804016113, Validation Loss: 1.7112586498260498\n",
      "Step 175/1000, Loss: 1.001243233680725, Validation Loss: 1.2353794574737549\n",
      "Step 176/1000, Loss: 0.9447659850120544, Validation Loss: 1.1935638189315796\n",
      "Step 177/1000, Loss: 0.9470288753509521, Validation Loss: 1.3516933917999268\n",
      "Step 178/1000, Loss: 0.9513296484947205, Validation Loss: 1.270262598991394\n",
      "Step 179/1000, Loss: 0.945461630821228, Validation Loss: 1.1643106937408447\n",
      "Step 180/1000, Loss: 0.9813863635063171, Validation Loss: 1.1760022640228271\n",
      "Step 181/1000, Loss: 0.9923388957977295, Validation Loss: 1.177544355392456\n",
      "Step 182/1000, Loss: 0.9499106407165527, Validation Loss: 1.6866570711135864\n",
      "Step 183/1000, Loss: 0.9972397089004517, Validation Loss: 2.010591506958008\n",
      "Step 184/1000, Loss: 1.0539240837097168, Validation Loss: 1.751383662223816\n",
      "Step 185/1000, Loss: 1.0079624652862549, Validation Loss: 1.2491965293884277\n",
      "Step 186/1000, Loss: 0.9447676539421082, Validation Loss: 1.355472445487976\n",
      "Step 187/1000, Loss: 1.0865681171417236, Validation Loss: 1.7825771570205688\n",
      "Step 188/1000, Loss: 1.252691626548767, Validation Loss: 1.753505825996399\n",
      "Step 189/1000, Loss: 1.242108702659607, Validation Loss: 1.3385945558547974\n",
      "Step 190/1000, Loss: 1.079094648361206, Validation Loss: 1.2027850151062012\n",
      "Step 191/1000, Loss: 0.9459087252616882, Validation Loss: 2.4128761291503906\n",
      "Step 192/1000, Loss: 1.131458044052124, Validation Loss: 3.7392354011535645\n",
      "Step 193/1000, Loss: 1.411561369895935, Validation Loss: 4.098563194274902\n",
      "Step 194/1000, Loss: 1.4910184144973755, Validation Loss: 3.3563168048858643\n",
      "Step 195/1000, Loss: 1.328223705291748, Validation Loss: 2.049321413040161\n",
      "Step 196/1000, Loss: 1.061159372329712, Validation Loss: 1.1738954782485962\n",
      "Step 197/1000, Loss: 0.9508312344551086, Validation Loss: 1.2308509349822998\n",
      "Step 198/1000, Loss: 1.026965618133545, Validation Loss: 1.2501205205917358\n",
      "Step 199/1000, Loss: 1.0371425151824951, Validation Loss: 1.1568076610565186\n",
      "Step 200/1000, Loss: 0.9669864177703857, Validation Loss: 1.5080420970916748\n",
      "Step 201/1000, Loss: 0.9700870513916016, Validation Loss: 1.8003891706466675\n",
      "Step 202/1000, Loss: 1.0163501501083374, Validation Loss: 1.6924525499343872\n",
      "Step 203/1000, Loss: 0.9982088208198547, Validation Loss: 1.410605549812317\n",
      "Step 204/1000, Loss: 0.9575432538986206, Validation Loss: 1.1746877431869507\n",
      "Step 205/1000, Loss: 0.9505749940872192, Validation Loss: 1.1579539775848389\n",
      "Step 206/1000, Loss: 0.9715504050254822, Validation Loss: 1.1572142839431763\n",
      "Step 207/1000, Loss: 0.9693490266799927, Validation Loss: 1.1809382438659668\n",
      "Step 208/1000, Loss: 0.9489730000495911, Validation Loss: 1.3863639831542969\n",
      "Step 209/1000, Loss: 0.9548056721687317, Validation Loss: 1.5237488746643066\n",
      "Step 210/1000, Loss: 0.9722920656204224, Validation Loss: 1.4648247957229614\n",
      "Step 211/1000, Loss: 0.9642558693885803, Validation Loss: 1.2755833864212036\n",
      "Step 212/1000, Loss: 0.9455955028533936, Validation Loss: 1.1568549871444702\n",
      "Step 213/1000, Loss: 0.9674651622772217, Validation Loss: 1.183428168296814\n",
      "Step 214/1000, Loss: 0.997819185256958, Validation Loss: 1.1755740642547607\n",
      "Step 215/1000, Loss: 0.9917733669281006, Validation Loss: 1.1593801975250244\n",
      "Step 216/1000, Loss: 0.9589923620223999, Validation Loss: 1.3051559925079346\n",
      "Step 217/1000, Loss: 0.9473544955253601, Validation Loss: 1.4284440279006958\n",
      "Step 218/1000, Loss: 0.9596655964851379, Validation Loss: 1.3861279487609863\n",
      "Step 219/1000, Loss: 0.9547780156135559, Validation Loss: 1.233003854751587\n",
      "Step 220/1000, Loss: 0.9446084499359131, Validation Loss: 1.2234907150268555\n",
      "Step 221/1000, Loss: 0.9447598457336426, Validation Loss: 1.3223813772201538\n",
      "Step 222/1000, Loss: 0.9486566185951233, Validation Loss: 1.2981685400009155\n",
      "Step 223/1000, Loss: 0.9468731880187988, Validation Loss: 1.1904274225234985\n",
      "Step 224/1000, Loss: 0.9472519755363464, Validation Loss: 1.1873581409454346\n",
      "Step 225/1000, Loss: 0.9477236866950989, Validation Loss: 1.2669293880462646\n",
      "Step 226/1000, Loss: 0.945210337638855, Validation Loss: 1.2519505023956299\n",
      "Step 227/1000, Loss: 0.9447487592697144, Validation Loss: 1.1718368530273438\n",
      "Step 228/1000, Loss: 0.9514645934104919, Validation Loss: 1.1712250709533691\n",
      "Step 229/1000, Loss: 0.9516792297363281, Validation Loss: 1.2366793155670166\n",
      "Step 230/1000, Loss: 0.9445749521255493, Validation Loss: 1.508651852607727\n",
      "Step 231/1000, Loss: 0.9701583981513977, Validation Loss: 1.6580257415771484\n",
      "Step 232/1000, Loss: 0.9926415681838989, Validation Loss: 1.574938416481018\n",
      "Step 233/1000, Loss: 0.979753851890564, Validation Loss: 1.3388175964355469\n",
      "Step 234/1000, Loss: 0.9500277042388916, Validation Loss: 1.1608577966690063\n",
      "Step 235/1000, Loss: 0.9573372602462769, Validation Loss: 1.1662932634353638\n",
      "Step 236/1000, Loss: 0.9831958413124084, Validation Loss: 1.162906527519226\n",
      "Step 237/1000, Loss: 0.9793861508369446, Validation Loss: 1.1674696207046509\n",
      "Step 238/1000, Loss: 0.9531148672103882, Validation Loss: 1.3433769941329956\n",
      "Step 239/1000, Loss: 0.9504261016845703, Validation Loss: 1.473659873008728\n",
      "Step 240/1000, Loss: 0.9653944373130798, Validation Loss: 1.4230544567108154\n",
      "Step 241/1000, Loss: 0.9589802026748657, Validation Loss: 1.2523363828659058\n",
      "Step 242/1000, Loss: 0.9446830153465271, Validation Loss: 1.158493995666504\n",
      "Step 243/1000, Loss: 0.9726196527481079, Validation Loss: 1.1939496994018555\n",
      "Step 244/1000, Loss: 1.0048699378967285, Validation Loss: 1.1836986541748047\n",
      "Step 245/1000, Loss: 0.9978020191192627, Validation Loss: 1.1574407815933228\n",
      "Step 246/1000, Loss: 0.96205073595047, Validation Loss: 1.2881170511245728\n",
      "Step 247/1000, Loss: 0.9461725950241089, Validation Loss: 1.4070993661880493\n",
      "Step 248/1000, Loss: 0.9570857882499695, Validation Loss: 1.368050217628479\n",
      "Step 249/1000, Loss: 0.9528126120567322, Validation Loss: 1.223532795906067\n",
      "Step 250/1000, Loss: 0.9446431398391724, Validation Loss: 1.2152953147888184\n",
      "Step 251/1000, Loss: 0.9449273943901062, Validation Loss: 1.3103052377700806\n",
      "Step 252/1000, Loss: 0.9476417899131775, Validation Loss: 1.2878388166427612\n",
      "Step 253/1000, Loss: 0.9461458325386047, Validation Loss: 1.1858397722244263\n",
      "Step 254/1000, Loss: 0.9478404521942139, Validation Loss: 1.183274507522583\n",
      "Step 255/1000, Loss: 0.9483130574226379, Validation Loss: 1.2596054077148438\n",
      "Step 256/1000, Loss: 0.9448556303977966, Validation Loss: 1.2455627918243408\n",
      "Step 257/1000, Loss: 0.9445322155952454, Validation Loss: 1.1694800853729248\n",
      "Step 258/1000, Loss: 0.9521757960319519, Validation Loss: 1.1690455675125122\n",
      "Step 259/1000, Loss: 0.9523510336875916, Validation Loss: 1.2317700386047363\n",
      "Step 260/1000, Loss: 0.9444881081581116, Validation Loss: 1.4992722272872925\n",
      "Step 261/1000, Loss: 0.9688525199890137, Validation Loss: 1.6477668285369873\n",
      "Step 262/1000, Loss: 0.9910320043563843, Validation Loss: 1.5663210153579712\n",
      "Step 263/1000, Loss: 0.9784784913063049, Validation Loss: 1.3336691856384277\n",
      "Step 264/1000, Loss: 0.9495236873626709, Validation Loss: 1.1601574420928955\n",
      "Step 265/1000, Loss: 0.957875669002533, Validation Loss: 1.1673094034194946\n",
      "Step 266/1000, Loss: 0.984093964099884, Validation Loss: 1.1636611223220825\n",
      "Step 267/1000, Loss: 0.9801571369171143, Validation Loss: 1.1665846109390259\n",
      "Step 268/1000, Loss: 0.9534350633621216, Validation Loss: 1.3399124145507812\n",
      "Step 269/1000, Loss: 0.9500746130943298, Validation Loss: 1.4694948196411133\n",
      "Step 270/1000, Loss: 0.9648351669311523, Validation Loss: 1.4195332527160645\n",
      "Step 271/1000, Loss: 0.9585403203964233, Validation Loss: 1.2503674030303955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 272/1000, Loss: 0.944591224193573, Validation Loss: 1.1587414741516113\n",
      "Step 273/1000, Loss: 0.973036527633667, Validation Loss: 1.1950098276138306\n",
      "Step 274/1000, Loss: 1.005456566810608, Validation Loss: 1.1845555305480957\n",
      "Step 275/1000, Loss: 0.9983187317848206, Validation Loss: 1.1573107242584229\n",
      "Step 276/1000, Loss: 0.9622977375984192, Validation Loss: 1.2865294218063354\n",
      "Step 277/1000, Loss: 0.9460459351539612, Validation Loss: 1.4050123691558838\n",
      "Step 278/1000, Loss: 0.9568306803703308, Validation Loss: 1.3662638664245605\n",
      "Step 279/1000, Loss: 0.9526095986366272, Validation Loss: 1.2225725650787354\n",
      "Step 280/1000, Loss: 0.9446223378181458, Validation Loss: 1.2144477367401123\n",
      "Step 281/1000, Loss: 0.94492107629776, Validation Loss: 1.309000849723816\n",
      "Step 282/1000, Loss: 0.9475200176239014, Validation Loss: 1.2867127656936646\n",
      "Step 283/1000, Loss: 0.9460508227348328, Validation Loss: 1.1853355169296265\n",
      "Step 284/1000, Loss: 0.947878897190094, Validation Loss: 1.1828138828277588\n",
      "Step 285/1000, Loss: 0.948354184627533, Validation Loss: 1.2587165832519531\n",
      "Step 286/1000, Loss: 0.9447914958000183, Validation Loss: 1.2447513341903687\n",
      "Step 287/1000, Loss: 0.9444841146469116, Validation Loss: 1.169185996055603\n",
      "Step 288/1000, Loss: 0.9522392749786377, Validation Loss: 1.1687705516815186\n",
      "Step 289/1000, Loss: 0.9524110555648804, Validation Loss: 1.2311006784439087\n",
      "Step 290/1000, Loss: 0.9444562196731567, Validation Loss: 1.497895359992981\n",
      "Step 291/1000, Loss: 0.9686465263366699, Validation Loss: 1.6462470293045044\n",
      "Step 292/1000, Loss: 0.9907839298248291, Validation Loss: 1.5649813413619995\n",
      "Step 293/1000, Loss: 0.9782692193984985, Validation Loss: 1.3327454328536987\n",
      "Step 294/1000, Loss: 0.9494163393974304, Validation Loss: 1.1600335836410522\n",
      "Step 295/1000, Loss: 0.9579495787620544, Validation Loss: 1.16752290725708\n",
      "Step 296/1000, Loss: 0.9842495322227478, Validation Loss: 1.1638370752334595\n",
      "Step 297/1000, Loss: 0.9803088903427124, Validation Loss: 1.1663672924041748\n",
      "Step 298/1000, Loss: 0.9534929394721985, Validation Loss: 1.339033603668213\n",
      "Step 299/1000, Loss: 0.9499678611755371, Validation Loss: 1.4682706594467163\n",
      "Early stopping triggered\n",
      "Step 1/1000, Loss: 0.9649357199668884, Validation Loss: 37.507015228271484\n",
      "Step 2/1000, Loss: 11.56751823425293, Validation Loss: 33.44672393798828\n",
      "Step 3/1000, Loss: 10.432357788085938, Validation Loss: 10.357548713684082\n",
      "Step 4/1000, Loss: 3.876391887664795, Validation Loss: 1.7368675470352173\n",
      "Step 5/1000, Loss: 1.005836844444275, Validation Loss: 5.2416534423828125\n",
      "Step 6/1000, Loss: 1.7506535053253174, Validation Loss: 3.735172748565674\n",
      "Step 7/1000, Loss: 1.4115360975265503, Validation Loss: 1.1570903062820435\n",
      "Step 8/1000, Loss: 0.9648592472076416, Validation Loss: 1.1807645559310913\n",
      "Step 9/1000, Loss: 0.9954230785369873, Validation Loss: 1.7757728099822998\n",
      "Step 10/1000, Loss: 1.0124332904815674, Validation Loss: 1.4834890365600586\n",
      "Step 11/1000, Loss: 0.9668973684310913, Validation Loss: 1.4847800731658936\n",
      "Step 12/1000, Loss: 1.1394314765930176, Validation Loss: 1.5651886463165283\n",
      "Step 13/1000, Loss: 1.1708379983901978, Validation Loss: 1.1919300556182861\n",
      "Step 14/1000, Loss: 0.9470116496086121, Validation Loss: 4.253834247589111\n",
      "Step 15/1000, Loss: 1.5268045663833618, Validation Loss: 6.470267295837402\n",
      "Step 16/1000, Loss: 2.035834312438965, Validation Loss: 5.056829452514648\n",
      "Step 17/1000, Loss: 1.708608865737915, Validation Loss: 1.9553362131118774\n",
      "Step 18/1000, Loss: 1.0442441701889038, Validation Loss: 1.8551280498504639\n",
      "Step 19/1000, Loss: 1.2775298357009888, Validation Loss: 3.7266266345977783\n",
      "Step 20/1000, Loss: 1.8919272422790527, Validation Loss: 3.5079710483551025\n",
      "Step 21/1000, Loss: 1.8230955600738525, Validation Loss: 1.6924731731414795\n",
      "Step 22/1000, Loss: 1.2184712886810303, Validation Loss: 1.663241982460022\n",
      "Step 23/1000, Loss: 0.9937328696250916, Validation Loss: 2.779341459274292\n",
      "Step 24/1000, Loss: 1.2068623304367065, Validation Loss: 2.2783377170562744\n",
      "Step 25/1000, Loss: 1.1055550575256348, Validation Loss: 1.1818342208862305\n",
      "Step 26/1000, Loss: 0.9485700130462646, Validation Loss: 1.1602163314819336\n",
      "Step 27/1000, Loss: 0.9579112529754639, Validation Loss: 1.601101040840149\n",
      "Step 28/1000, Loss: 0.9839369654655457, Validation Loss: 1.4236966371536255\n",
      "Step 29/1000, Loss: 0.9591479301452637, Validation Loss: 1.2524057626724243\n",
      "Step 30/1000, Loss: 1.0373879671096802, Validation Loss: 1.2868345975875854\n",
      "Step 31/1000, Loss: 1.0542532205581665, Validation Loss: 1.2344253063201904\n",
      "Step 32/1000, Loss: 0.9443961977958679, Validation Loss: 3.506674289703369\n",
      "Step 33/1000, Loss: 1.3620502948760986, Validation Loss: 5.070049285888672\n",
      "Step 34/1000, Loss: 1.712181806564331, Validation Loss: 4.047119617462158\n",
      "Step 35/1000, Loss: 1.4812037944793701, Validation Loss: 1.7573037147521973\n",
      "Step 36/1000, Loss: 1.0093464851379395, Validation Loss: 1.6890373229980469\n",
      "Step 37/1000, Loss: 1.2165018320083618, Validation Loss: 3.1391587257385254\n",
      "Step 38/1000, Loss: 1.7047207355499268, Validation Loss: 2.9695112705230713\n",
      "Step 39/1000, Loss: 1.6502084732055664, Validation Loss: 1.5430763959884644\n",
      "Step 40/1000, Loss: 1.1612653732299805, Validation Loss: 1.6447975635528564\n",
      "Step 41/1000, Loss: 0.9908071160316467, Validation Loss: 2.638326644897461\n",
      "Step 42/1000, Loss: 1.1781184673309326, Validation Loss: 2.190431594848633\n",
      "Step 43/1000, Loss: 1.0886826515197754, Validation Loss: 1.1859638690948486\n",
      "Step 44/1000, Loss: 0.9474577307701111, Validation Loss: 1.162522315979004\n",
      "Step 45/1000, Loss: 0.9554870128631592, Validation Loss: 1.5839787721633911\n",
      "Step 46/1000, Loss: 0.9813145399093628, Validation Loss: 1.4168235063552856\n",
      "Step 47/1000, Loss: 0.9582338929176331, Validation Loss: 1.2368316650390625\n",
      "Step 48/1000, Loss: 1.0287405252456665, Validation Loss: 1.2675342559814453\n",
      "Step 49/1000, Loss: 1.0443744659423828, Validation Loss: 1.2380980253219604\n",
      "Step 50/1000, Loss: 0.9441899657249451, Validation Loss: 1.1886680126190186\n",
      "Step 51/1000, Loss: 0.9469383955001831, Validation Loss: 1.710422396659851\n",
      "Step 52/1000, Loss: 1.0015228986740112, Validation Loss: 1.5068228244781494\n",
      "Step 53/1000, Loss: 0.970005452632904, Validation Loss: 1.1996493339538574\n",
      "Step 54/1000, Loss: 1.0072860717773438, Validation Loss: 1.2266347408294678\n",
      "Step 55/1000, Loss: 1.0230134725570679, Validation Loss: 1.2759016752243042\n",
      "Step 56/1000, Loss: 0.9452277421951294, Validation Loss: 1.2109673023223877\n",
      "Step 57/1000, Loss: 0.9446778893470764, Validation Loss: 1.783624529838562\n",
      "Step 58/1000, Loss: 1.0139431953430176, Validation Loss: 1.5595955848693848\n",
      "Step 59/1000, Loss: 0.9776490330696106, Validation Loss: 1.1846578121185303\n",
      "Step 60/1000, Loss: 0.9971588253974915, Validation Loss: 1.2087867259979248\n",
      "Step 61/1000, Loss: 1.0126886367797852, Validation Loss: 1.298823595046997\n",
      "Step 62/1000, Loss: 0.9465514421463013, Validation Loss: 1.2251173257827759\n",
      "Step 63/1000, Loss: 0.9441102743148804, Validation Loss: 1.8239325284957886\n",
      "Step 64/1000, Loss: 1.0209969282150269, Validation Loss: 1.5888293981552124\n",
      "Step 65/1000, Loss: 0.982072114944458, Validation Loss: 1.1780909299850464\n",
      "Step 66/1000, Loss: 0.9921512007713318, Validation Loss: 1.2005366086959839\n",
      "Step 67/1000, Loss: 1.007535457611084, Validation Loss: 1.3115615844726562\n",
      "Step 68/1000, Loss: 0.9474467039108276, Validation Loss: 1.233137607574463\n",
      "Step 69/1000, Loss: 0.9439693093299866, Validation Loss: 1.845189094543457\n",
      "Step 70/1000, Loss: 1.024770736694336, Validation Loss: 1.6039067506790161\n",
      "Step 71/1000, Loss: 0.9844104647636414, Validation Loss: 1.175156593322754\n",
      "Step 72/1000, Loss: 0.9896230697631836, Validation Loss: 1.1967697143554688\n",
      "Step 73/1000, Loss: 1.0049350261688232, Validation Loss: 1.3177112340927124\n",
      "Step 74/1000, Loss: 0.9478594064712524, Validation Loss: 1.2369272708892822\n",
      "Step 75/1000, Loss: 0.9438551664352417, Validation Loss: 1.3895585536956787\n",
      "Step 76/1000, Loss: 1.0981632471084595, Validation Loss: 1.4192980527877808\n",
      "Step 77/1000, Loss: 1.1104813814163208, Validation Loss: 1.171127438545227\n",
      "Step 78/1000, Loss: 0.9504958391189575, Validation Loss: 2.971315860748291\n",
      "Step 79/1000, Loss: 1.2484163045883179, Validation Loss: 4.3714919090271\n",
      "Step 80/1000, Loss: 1.5556902885437012, Validation Loss: 3.5144407749176025\n",
      "Step 81/1000, Loss: 1.3653812408447266, Validation Loss: 1.5727545022964478\n",
      "Step 82/1000, Loss: 0.9796408414840698, Validation Loss: 1.821437120437622\n",
      "Step 83/1000, Loss: 1.2622548341751099, Validation Loss: 3.319356918334961\n",
      "Step 84/1000, Loss: 1.7586649656295776, Validation Loss: 3.1214654445648193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 85/1000, Loss: 1.6956257820129395, Validation Loss: 1.6205267906188965\n",
      "Step 86/1000, Loss: 1.1886950731277466, Validation Loss: 1.5491493940353394\n",
      "Step 87/1000, Loss: 0.9760951399803162, Validation Loss: 2.466151475906372\n",
      "Step 88/1000, Loss: 1.1439423561096191, Validation Loss: 2.06250262260437\n",
      "Step 89/1000, Loss: 1.0648280382156372, Validation Loss: 1.1711171865463257\n",
      "Step 90/1000, Loss: 0.9502803087234497, Validation Loss: 1.1576796770095825\n",
      "Step 91/1000, Loss: 0.9600241184234619, Validation Loss: 1.5252923965454102\n",
      "Step 92/1000, Loss: 0.9726006388664246, Validation Loss: 1.3759738206863403\n",
      "Step 93/1000, Loss: 0.9533262252807617, Validation Loss: 1.2563258409500122\n",
      "Step 94/1000, Loss: 1.0373170375823975, Validation Loss: 1.287962794303894\n",
      "Step 95/1000, Loss: 1.0526386499404907, Validation Loss: 1.2215917110443115\n",
      "Step 96/1000, Loss: 0.9437435269355774, Validation Loss: 3.3122451305389404\n",
      "Step 97/1000, Loss: 1.32175874710083, Validation Loss: 4.7735748291015625\n",
      "Step 98/1000, Loss: 1.6473162174224854, Validation Loss: 3.824638605117798\n",
      "Step 99/1000, Loss: 1.4339888095855713, Validation Loss: 1.6943385601043701\n",
      "Step 100/1000, Loss: 0.9990262389183044, Validation Loss: 1.6978087425231934\n",
      "Step 101/1000, Loss: 1.2170196771621704, Validation Loss: 3.1134002208709717\n",
      "Step 102/1000, Loss: 1.692469835281372, Validation Loss: 3.0281050205230713\n",
      "Step 103/1000, Loss: 1.665136456489563, Validation Loss: 2.178703546524048\n",
      "Step 104/1000, Loss: 1.3858858346939087, Validation Loss: 1.2866147756576538\n",
      "Step 105/1000, Loss: 1.0518877506256104, Validation Loss: 1.4643245935440063\n",
      "Step 106/1000, Loss: 0.9640970826148987, Validation Loss: 2.2564022541046143\n",
      "Step 107/1000, Loss: 1.1023023128509521, Validation Loss: 2.5117366313934326\n",
      "Early stopping triggered\n",
      "Step 1/1000, Loss: 1.1534336805343628, Validation Loss: 30.54541778564453\n",
      "Step 2/1000, Loss: 9.601750373840332, Validation Loss: 26.908056259155273\n",
      "Step 3/1000, Loss: 8.58057975769043, Validation Loss: 7.043417453765869\n",
      "Step 4/1000, Loss: 2.8923003673553467, Validation Loss: 3.0262465476989746\n",
      "Step 5/1000, Loss: 1.2605793476104736, Validation Loss: 8.054584503173828\n",
      "Step 6/1000, Loss: 2.415391206741333, Validation Loss: 6.045804023742676\n",
      "Step 7/1000, Loss: 1.9410967826843262, Validation Loss: 1.532678246498108\n",
      "Step 8/1000, Loss: 0.9737505316734314, Validation Loss: 4.618243217468262\n",
      "Step 9/1000, Loss: 2.1619057655334473, Validation Loss: 9.308311462402344\n",
      "Step 10/1000, Loss: 3.559401512145996, Validation Loss: 8.371731758117676\n",
      "Step 11/1000, Loss: 3.2846689224243164, Validation Loss: 3.659907102584839\n",
      "Step 12/1000, Loss: 1.8651634454727173, Validation Loss: 1.238555669784546\n",
      "Step 13/1000, Loss: 0.9436373114585876, Validation Loss: 2.197709560394287\n",
      "Step 14/1000, Loss: 1.0909347534179688, Validation Loss: 1.805788278579712\n",
      "Step 15/1000, Loss: 1.0181453227996826, Validation Loss: 1.2015032768249512\n",
      "Step 16/1000, Loss: 1.0070456266403198, Validation Loss: 1.2439045906066895\n",
      "Step 17/1000, Loss: 1.0306693315505981, Validation Loss: 1.334596872329712\n",
      "Step 18/1000, Loss: 0.9491505026817322, Validation Loss: 1.234776258468628\n",
      "Step 19/1000, Loss: 0.9435282945632935, Validation Loss: 1.5272128582000732\n",
      "Step 20/1000, Loss: 1.1522899866104126, Validation Loss: 1.5736571550369263\n",
      "Step 21/1000, Loss: 1.1700631380081177, Validation Loss: 1.160561203956604\n",
      "Step 22/1000, Loss: 0.9558334946632385, Validation Loss: 3.1712822914123535\n",
      "Step 23/1000, Loss: 1.291933298110962, Validation Loss: 4.79060173034668\n",
      "Step 24/1000, Loss: 1.6520940065383911, Validation Loss: 3.7964560985565186\n",
      "Step 25/1000, Loss: 1.4284940958023071, Validation Loss: 1.597831130027771\n",
      "Step 26/1000, Loss: 0.9836036562919617, Validation Loss: 1.96266770362854\n",
      "Step 27/1000, Loss: 1.3105655908584595, Validation Loss: 3.6991946697235107\n",
      "Step 28/1000, Loss: 1.875999093055725, Validation Loss: 3.4763224124908447\n",
      "Step 29/1000, Loss: 1.8060240745544434, Validation Loss: 1.7468141317367554\n",
      "Step 30/1000, Loss: 1.2338402271270752, Validation Loss: 1.511520266532898\n",
      "Step 31/1000, Loss: 0.9706703424453735, Validation Loss: 2.440345048904419\n",
      "Step 32/1000, Loss: 1.1393665075302124, Validation Loss: 2.026970863342285\n",
      "Step 33/1000, Loss: 1.0585603713989258, Validation Loss: 1.1623070240020752\n",
      "Step 34/1000, Loss: 0.9541839361190796, Validation Loss: 1.1572058200836182\n",
      "Step 35/1000, Loss: 0.9665252566337585, Validation Loss: 1.4843130111694336\n",
      "Step 36/1000, Loss: 0.9668264389038086, Validation Loss: 1.3414909839630127\n",
      "Step 37/1000, Loss: 0.9496784210205078, Validation Loss: 1.2939443588256836\n",
      "Step 38/1000, Loss: 1.0544962882995605, Validation Loss: 1.3316879272460938\n",
      "Step 39/1000, Loss: 1.0715742111206055, Validation Loss: 1.2001725435256958\n",
      "Step 40/1000, Loss: 0.9445213675498962, Validation Loss: 3.2284553050994873\n",
      "Step 41/1000, Loss: 1.3045860528945923, Validation Loss: 4.684969425201416\n",
      "Step 42/1000, Loss: 1.6285068988800049, Validation Loss: 3.737668037414551\n",
      "Step 43/1000, Loss: 1.4158400297164917, Validation Loss: 1.641985297203064\n",
      "Step 44/1000, Loss: 0.9906244874000549, Validation Loss: 1.775132417678833\n",
      "Step 45/1000, Loss: 1.243770718574524, Validation Loss: 3.2748918533325195\n",
      "Step 46/1000, Loss: 1.7418408393859863, Validation Loss: 3.098344564437866\n",
      "Step 47/1000, Loss: 1.6856529712677002, Validation Loss: 1.611923098564148\n",
      "Step 48/1000, Loss: 1.1838147640228271, Validation Loss: 1.5593390464782715\n",
      "Step 49/1000, Loss: 0.9777095317840576, Validation Loss: 2.4786431789398193\n",
      "Step 50/1000, Loss: 1.1472301483154297, Validation Loss: 2.0618209838867188\n",
      "Step 51/1000, Loss: 1.0652008056640625, Validation Loss: 1.1692309379577637\n",
      "Step 52/1000, Loss: 0.9504262804985046, Validation Loss: 1.1571110486984253\n",
      "Step 53/1000, Loss: 0.9610422253608704, Validation Loss: 1.508433222770691\n",
      "Step 54/1000, Loss: 0.970216691493988, Validation Loss: 1.359753131866455\n",
      "Step 55/1000, Loss: 0.9514744281768799, Validation Loss: 1.2711135149002075\n",
      "Step 56/1000, Loss: 1.0435049533843994, Validation Loss: 1.3068981170654297\n",
      "Step 57/1000, Loss: 1.060274600982666, Validation Loss: 1.208903431892395\n",
      "Step 58/1000, Loss: 0.9438471794128418, Validation Loss: 3.236250162124634\n",
      "Step 59/1000, Loss: 1.306400179862976, Validation Loss: 4.670760154724121\n",
      "Step 60/1000, Loss: 1.6254640817642212, Validation Loss: 3.729940414428711\n",
      "Step 61/1000, Loss: 1.4142769575119019, Validation Loss: 1.6485552787780762\n",
      "Step 62/1000, Loss: 0.9917015433311462, Validation Loss: 1.7504221200942993\n",
      "Step 63/1000, Loss: 1.2346630096435547, Validation Loss: 3.2175779342651367\n",
      "Step 64/1000, Loss: 1.7233452796936035, Validation Loss: 3.0473415851593018\n",
      "Step 65/1000, Loss: 1.6690630912780762, Validation Loss: 1.5942662954330444\n",
      "Step 66/1000, Loss: 1.1769970655441284, Validation Loss: 1.5660772323608398\n",
      "Step 67/1000, Loss: 0.9787341952323914, Validation Loss: 2.4833974838256836\n",
      "Step 68/1000, Loss: 1.1482683420181274, Validation Loss: 2.066227674484253\n",
      "Step 69/1000, Loss: 1.0660821199417114, Validation Loss: 1.170385479927063\n",
      "Step 70/1000, Loss: 0.9499228000640869, Validation Loss: 1.157293677330017\n",
      "Step 71/1000, Loss: 0.9602910876274109, Validation Loss: 1.5115169286727905\n",
      "Step 72/1000, Loss: 0.9706597924232483, Validation Loss: 1.362095832824707\n",
      "Step 73/1000, Loss: 0.9517010450363159, Validation Loss: 1.2682095766067505\n",
      "Step 74/1000, Loss: 1.0419765710830688, Validation Loss: 1.3037620782852173\n",
      "Step 75/1000, Loss: 1.058711290359497, Validation Loss: 1.2100321054458618\n",
      "Step 76/1000, Loss: 0.943739652633667, Validation Loss: 3.23569393157959\n",
      "Step 77/1000, Loss: 1.306398868560791, Validation Loss: 4.666288375854492\n",
      "Step 78/1000, Loss: 1.624599814414978, Validation Loss: 3.726716995239258\n",
      "Step 79/1000, Loss: 1.4136825799942017, Validation Loss: 1.6487597227096558\n",
      "Step 80/1000, Loss: 0.9917409420013428, Validation Loss: 1.7475157976150513\n",
      "Step 81/1000, Loss: 1.2334364652633667, Validation Loss: 3.2103285789489746\n",
      "Step 82/1000, Loss: 1.7208147048950195, Validation Loss: 3.040850877761841\n",
      "Step 83/1000, Loss: 1.666758418083191, Validation Loss: 1.5922335386276245\n",
      "Step 84/1000, Loss: 1.1760767698287964, Validation Loss: 1.5665111541748047\n",
      "Step 85/1000, Loss: 0.978796124458313, Validation Loss: 2.4829981327056885\n",
      "Step 86/1000, Loss: 1.1482428312301636, Validation Loss: 2.066105365753174\n",
      "Step 87/1000, Loss: 1.0660905838012695, Validation Loss: 1.1704883575439453\n",
      "Step 88/1000, Loss: 0.9498202800750732, Validation Loss: 1.1573058366775513\n",
      "Step 89/1000, Loss: 0.9601553082466125, Validation Loss: 1.5115723609924316\n",
      "Step 90/1000, Loss: 0.9706564545631409, Validation Loss: 1.3621453046798706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 91/1000, Loss: 0.9516820311546326, Validation Loss: 1.2680034637451172\n",
      "Step 92/1000, Loss: 1.0417536497116089, Validation Loss: 1.303500771522522\n",
      "Step 93/1000, Loss: 1.058468222618103, Validation Loss: 1.210034728050232\n",
      "Step 94/1000, Loss: 0.9436856508255005, Validation Loss: 3.234591007232666\n",
      "Step 95/1000, Loss: 1.3062583208084106, Validation Loss: 4.663971900939941\n",
      "Step 96/1000, Loss: 1.6242634057998657, Validation Loss: 3.7248785495758057\n",
      "Step 97/1000, Loss: 1.4134306907653809, Validation Loss: 1.648154616355896\n",
      "Step 98/1000, Loss: 0.9916614294052124, Validation Loss: 1.74766206741333\n",
      "Step 99/1000, Loss: 1.2332878112792969, Validation Loss: 3.2102556228637695\n",
      "Step 100/1000, Loss: 1.7204701900482178, Validation Loss: 3.0408401489257812\n",
      "Step 101/1000, Loss: 1.6664401292800903, Validation Loss: 1.592375636100769\n",
      "Step 102/1000, Loss: 1.175933599472046, Validation Loss: 1.1569082736968994\n",
      "Step 103/1000, Loss: 0.9647318124771118, Validation Loss: 1.9738794565200806\n",
      "Step 104/1000, Loss: 1.0488189458847046, Validation Loss: 3.1095714569091797\n",
      "Step 105/1000, Loss: 1.27943754196167, Validation Loss: 3.4440319538116455\n",
      "Step 106/1000, Loss: 1.3518015146255493, Validation Loss: 2.8123061656951904\n",
      "Step 107/1000, Loss: 1.2163816690444946, Validation Loss: 1.7259509563446045\n",
      "Step 108/1000, Loss: 1.0045902729034424, Validation Loss: 1.1596895456314087\n",
      "Step 109/1000, Loss: 0.9718478322029114, Validation Loss: 1.36110520362854\n",
      "Step 110/1000, Loss: 1.08381187915802, Validation Loss: 1.388426661491394\n",
      "Step 111/1000, Loss: 1.0953741073608398, Validation Loss: 1.188654899597168\n",
      "Step 112/1000, Loss: 0.9978047609329224, Validation Loss: 1.335888385772705\n",
      "Step 113/1000, Loss: 0.9490436911582947, Validation Loss: 1.5607407093048096\n",
      "Step 114/1000, Loss: 0.9779284000396729, Validation Loss: 1.404348611831665\n",
      "Step 115/1000, Loss: 0.9564196467399597, Validation Loss: 1.158753514289856\n",
      "Step 116/1000, Loss: 0.9572526812553406, Validation Loss: 1.1580079793930054\n",
      "Step 117/1000, Loss: 0.9685946702957153, Validation Loss: 1.2067081928253174\n",
      "Step 118/1000, Loss: 0.9438186287879944, Validation Loss: 1.7728248834609985\n",
      "Step 119/1000, Loss: 1.012638807296753, Validation Loss: 2.080491781234741\n",
      "Step 120/1000, Loss: 1.0689032077789307, Validation Loss: 1.781728982925415\n",
      "Step 121/1000, Loss: 1.014189600944519, Validation Loss: 1.2515863180160522\n",
      "Step 122/1000, Loss: 0.943422794342041, Validation Loss: 1.3644027709960938\n",
      "Step 123/1000, Loss: 1.0851656198501587, Validation Loss: 1.8188166618347168\n",
      "Step 124/1000, Loss: 1.258681058883667, Validation Loss: 1.8076419830322266\n",
      "Step 125/1000, Loss: 1.2546805143356323, Validation Loss: 1.3790321350097656\n",
      "Step 126/1000, Loss: 1.091340184211731, Validation Loss: 1.1822305917739868\n",
      "Step 127/1000, Loss: 0.9466168284416199, Validation Loss: 2.2729387283325195\n",
      "Step 128/1000, Loss: 1.1063003540039062, Validation Loss: 3.5118513107299805\n",
      "Step 129/1000, Loss: 1.3667418956756592, Validation Loss: 3.8327152729034424\n",
      "Step 130/1000, Loss: 1.4375061988830566, Validation Loss: 3.1105525493621826\n",
      "Step 131/1000, Loss: 1.2797526121139526, Validation Loss: 1.886786699295044\n",
      "Step 132/1000, Loss: 1.0328834056854248, Validation Loss: 1.1581027507781982\n",
      "Step 133/1000, Loss: 0.9581857323646545, Validation Loss: 1.2944728136062622\n",
      "Step 134/1000, Loss: 1.0540698766708374, Validation Loss: 1.3235819339752197\n",
      "Step 135/1000, Loss: 1.0672883987426758, Validation Loss: 1.1695506572723389\n",
      "Step 136/1000, Loss: 0.9830315113067627, Validation Loss: 1.3892507553100586\n",
      "Step 137/1000, Loss: 0.9546313881874084, Validation Loss: 1.6309701204299927\n",
      "Step 138/1000, Loss: 0.9889104962348938, Validation Loss: 1.4540958404541016\n",
      "Step 139/1000, Loss: 0.9627001285552979, Validation Loss: 1.164385199546814\n",
      "Step 140/1000, Loss: 0.9523987174034119, Validation Loss: 1.1568082571029663\n",
      "Step 141/1000, Loss: 0.9625333547592163, Validation Loss: 1.2234444618225098\n",
      "Step 142/1000, Loss: 0.9431487917900085, Validation Loss: 1.8229401111602783\n",
      "Step 143/1000, Loss: 1.0214444398880005, Validation Loss: 2.1353554725646973\n",
      "Step 144/1000, Loss: 1.0794607400894165, Validation Loss: 1.8223545551300049\n",
      "Step 145/1000, Loss: 1.0213426351547241, Validation Loss: 1.2660893201828003\n",
      "Step 146/1000, Loss: 0.9439178705215454, Validation Loss: 1.3461992740631104\n",
      "Step 147/1000, Loss: 1.0771576166152954, Validation Loss: 1.7892875671386719\n",
      "Step 148/1000, Loss: 1.2478944063186646, Validation Loss: 1.781317949295044\n",
      "Step 149/1000, Loss: 1.2450200319290161, Validation Loss: 1.365277647972107\n",
      "Step 150/1000, Loss: 1.085337519645691, Validation Loss: 1.1866388320922852\n",
      "Step 151/1000, Loss: 0.9457683563232422, Validation Loss: 2.2982521057128906\n",
      "Step 152/1000, Loss: 1.1113789081573486, Validation Loss: 3.5447447299957275\n",
      "Step 153/1000, Loss: 1.3741083145141602, Validation Loss: 3.8640403747558594\n",
      "Step 154/1000, Loss: 1.4446358680725098, Validation Loss: 3.1344377994537354\n",
      "Step 155/1000, Loss: 1.2850109338760376, Validation Loss: 1.8998552560806274\n",
      "Step 156/1000, Loss: 1.0352929830551147, Validation Loss: 1.1586376428604126\n",
      "Step 157/1000, Loss: 0.9572221636772156, Validation Loss: 1.2899079322814941\n",
      "Step 158/1000, Loss: 1.0517703294754028, Validation Loss: 1.3190758228302002\n",
      "Step 159/1000, Loss: 1.0650888681411743, Validation Loss: 1.16843581199646\n",
      "Step 160/1000, Loss: 0.9818704128265381, Validation Loss: 1.393508791923523\n",
      "Step 161/1000, Loss: 0.9550964832305908, Validation Loss: 1.6362831592559814\n",
      "Step 162/1000, Loss: 0.9897752404212952, Validation Loss: 1.4578728675842285\n",
      "Step 163/1000, Loss: 0.9631821513175964, Validation Loss: 1.164919376373291\n",
      "Step 164/1000, Loss: 0.9519866704940796, Validation Loss: 1.1568230390548706\n",
      "Step 165/1000, Loss: 0.9620186686515808, Validation Loss: 1.2247004508972168\n",
      "Step 166/1000, Loss: 0.9430554509162903, Validation Loss: 1.8263410329818726\n",
      "Step 167/1000, Loss: 1.0220996141433716, Validation Loss: 2.1388723850250244\n",
      "Step 168/1000, Loss: 1.08023202419281, Validation Loss: 1.824906826019287\n",
      "Step 169/1000, Loss: 1.0218509435653687, Validation Loss: 1.2669755220413208\n",
      "Step 170/1000, Loss: 0.9438974261283875, Validation Loss: 1.3450710773468018\n",
      "Step 171/1000, Loss: 1.0764377117156982, Validation Loss: 1.7874295711517334\n",
      "Step 172/1000, Loss: 1.24691641330719, Validation Loss: 1.7796735763549805\n",
      "Step 173/1000, Loss: 1.2441047430038452, Validation Loss: 1.3645108938217163\n",
      "Step 174/1000, Loss: 1.0847663879394531, Validation Loss: 1.186858057975769\n",
      "Step 175/1000, Loss: 0.9456319212913513, Validation Loss: 2.2994461059570312\n",
      "Step 176/1000, Loss: 1.11172354221344, Validation Loss: 3.546175718307495\n",
      "Step 177/1000, Loss: 1.374644160270691, Validation Loss: 3.865427017211914\n",
      "Step 178/1000, Loss: 1.4451909065246582, Validation Loss: 3.135509729385376\n",
      "Step 179/1000, Loss: 1.2854440212249756, Validation Loss: 1.9003894329071045\n",
      "Step 180/1000, Loss: 1.0354595184326172, Validation Loss: 1.1586486101150513\n",
      "Step 181/1000, Loss: 0.9570498466491699, Validation Loss: 1.2897047996520996\n",
      "Step 182/1000, Loss: 1.0514485836029053, Validation Loss: 1.3188868761062622\n",
      "Step 183/1000, Loss: 1.064781665802002, Validation Loss: 1.1683998107910156\n",
      "Step 184/1000, Loss: 0.9816794991493225, Validation Loss: 1.3935604095458984\n",
      "Step 185/1000, Loss: 0.9550688862800598, Validation Loss: 1.6363497972488403\n",
      "Step 186/1000, Loss: 0.9897991418838501, Validation Loss: 1.4579099416732788\n",
      "Step 187/1000, Loss: 0.9631674885749817, Validation Loss: 1.1649173498153687\n",
      "Step 188/1000, Loss: 0.9518628716468811, Validation Loss: 1.1568081378936768\n",
      "Step 189/1000, Loss: 0.9618721008300781, Validation Loss: 1.2246744632720947\n",
      "Step 190/1000, Loss: 0.9429700374603271, Validation Loss: 1.8262687921524048\n",
      "Step 191/1000, Loss: 1.0221244096755981, Validation Loss: 2.1387555599212646\n",
      "Step 192/1000, Loss: 1.0802855491638184, Validation Loss: 1.8248116970062256\n",
      "Step 193/1000, Loss: 1.021884560585022, Validation Loss: 1.2669386863708496\n",
      "Step 194/1000, Loss: 0.9438298344612122, Validation Loss: 1.3451340198516846\n",
      "Step 195/1000, Loss: 1.0762171745300293, Validation Loss: 1.787601113319397\n",
      "Step 196/1000, Loss: 1.2466363906860352, Validation Loss: 1.7798383235931396\n",
      "Step 197/1000, Loss: 1.2438366413116455, Validation Loss: 1.364611029624939\n",
      "Step 198/1000, Loss: 1.0845569372177124, Validation Loss: 1.1867918968200684\n",
      "Step 199/1000, Loss: 0.9455304145812988, Validation Loss: 2.299086093902588\n",
      "Step 200/1000, Loss: 1.1117628812789917, Validation Loss: 3.545626163482666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 201/1000, Loss: 1.3747518062591553, Validation Loss: 3.864778995513916\n",
      "Step 202/1000, Loss: 1.4453033208847046, Validation Loss: 3.485522508621216\n",
      "Step 203/1000, Loss: 1.361583948135376, Validation Loss: 2.728961706161499\n",
      "Step 204/1000, Loss: 1.1994644403457642, Validation Loss: 1.8697324991226196\n",
      "Step 205/1000, Loss: 1.029952049255371, Validation Loss: 1.253618597984314\n",
      "Step 206/1000, Loss: 0.943257212638855, Validation Loss: 1.2677135467529297\n",
      "Step 207/1000, Loss: 1.0407755374908447, Validation Loss: 1.7169550657272339\n",
      "Step 208/1000, Loss: 1.2209962606430054, Validation Loss: 2.0740413665771484\n",
      "Step 209/1000, Loss: 1.3466579914093018, Validation Loss: 2.120220422744751\n",
      "Step 210/1000, Loss: 1.362430453300476, Validation Loss: 1.8597229719161987\n",
      "Step 211/1000, Loss: 1.2721341848373413, Validation Loss: 1.4538872241973877\n",
      "Step 212/1000, Loss: 1.121235966682434, Validation Loss: 1.1703215837478638\n",
      "Step 213/1000, Loss: 0.9832637906074524, Validation Loss: 1.344954013824463\n",
      "Step 214/1000, Loss: 0.9497646689414978, Validation Loss: 1.7412437200546265\n",
      "Step 215/1000, Loss: 1.0073199272155762, Validation Loss: 1.962386131286621\n",
      "Step 216/1000, Loss: 1.0469294786453247, Validation Loss: 1.884082317352295\n",
      "Step 217/1000, Loss: 1.0325565338134766, Validation Loss: 1.5766561031341553\n",
      "Step 218/1000, Loss: 0.9803829789161682, Validation Loss: 1.2461918592453003\n",
      "Step 219/1000, Loss: 0.9430320858955383, Validation Loss: 1.1889677047729492\n",
      "Step 220/1000, Loss: 0.9973896145820618, Validation Loss: 1.3553829193115234\n",
      "Step 221/1000, Loss: 1.0804805755615234, Validation Loss: 1.4278947114944458\n",
      "Step 222/1000, Loss: 1.1107257604599, Validation Loss: 1.3362171649932861\n",
      "Step 223/1000, Loss: 1.0721420049667358, Validation Loss: 1.186244249343872\n",
      "Step 224/1000, Loss: 0.9955171942710876, Validation Loss: 1.2049534320831299\n",
      "Step 225/1000, Loss: 0.9435919523239136, Validation Loss: 1.698784351348877\n",
      "Step 226/1000, Loss: 1.000125527381897, Validation Loss: 2.231492757797241\n",
      "Step 227/1000, Loss: 1.0984688997268677, Validation Loss: 2.462296485900879\n",
      "Step 228/1000, Loss: 1.1446115970611572, Validation Loss: 2.308840036392212\n",
      "Step 229/1000, Loss: 1.113767147064209, Validation Loss: 1.8731703758239746\n",
      "Step 230/1000, Loss: 1.030603051185608, Validation Loss: 1.3848861455917358\n",
      "Step 231/1000, Loss: 0.9540322422981262, Validation Loss: 1.1571044921875\n",
      "Step 232/1000, Loss: 0.9654858112335205, Validation Loss: 1.247390866279602\n",
      "Step 233/1000, Loss: 1.0305980443954468, Validation Loss: 1.3092756271362305\n",
      "Step 234/1000, Loss: 1.0601047277450562, Validation Loss: 1.2506990432739258\n",
      "Step 235/1000, Loss: 1.032265305519104, Validation Loss: 1.1611467599868774\n",
      "Step 236/1000, Loss: 0.9734659194946289, Validation Loss: 1.2555584907531738\n",
      "Step 237/1000, Loss: 0.9432861804962158, Validation Loss: 1.399225115776062\n",
      "Step 238/1000, Loss: 0.9557019472122192, Validation Loss: 1.3950319290161133\n",
      "Step 239/1000, Loss: 0.9552061557769775, Validation Loss: 1.2590103149414062\n",
      "Step 240/1000, Loss: 0.9434101581573486, Validation Loss: 1.1567622423171997\n",
      "Step 241/1000, Loss: 0.9620961546897888, Validation Loss: 1.1702656745910645\n",
      "Step 242/1000, Loss: 0.9831290245056152, Validation Loss: 1.1603083610534668\n",
      "Step 243/1000, Loss: 0.9722470045089722, Validation Loss: 1.1810336112976074\n",
      "Step 244/1000, Loss: 0.9464443922042847, Validation Loss: 1.4212948083877563\n",
      "Step 245/1000, Loss: 0.9583887457847595, Validation Loss: 1.6089403629302979\n",
      "Step 246/1000, Loss: 0.9854373335838318, Validation Loss: 1.5794658660888672\n",
      "Step 247/1000, Loss: 0.9808197021484375, Validation Loss: 1.3734993934631348\n",
      "Step 248/1000, Loss: 0.9527333378791809, Validation Loss: 1.1749305725097656\n",
      "Step 249/1000, Loss: 0.9478556513786316, Validation Loss: 1.1567463874816895\n",
      "Step 250/1000, Loss: 0.9623462557792664, Validation Loss: 1.1589114665985107\n",
      "Step 251/1000, Loss: 0.9564074277877808, Validation Loss: 1.2199965715408325\n",
      "Step 252/1000, Loss: 0.9429128766059875, Validation Loss: 1.5172233581542969\n",
      "Step 253/1000, Loss: 0.9714576601982117, Validation Loss: 1.7190544605255127\n",
      "Step 254/1000, Loss: 1.0035510063171387, Validation Loss: 1.6749056577682495\n",
      "Step 255/1000, Loss: 0.9961462616920471, Validation Loss: 1.4358100891113281\n",
      "Step 256/1000, Loss: 0.9602303504943848, Validation Loss: 1.193336844444275\n",
      "Step 257/1000, Loss: 0.944537341594696, Validation Loss: 1.1595102548599243\n",
      "Step 258/1000, Loss: 0.9556694626808167, Validation Loss: 1.1652518510818481\n",
      "Step 259/1000, Loss: 0.9514339566230774, Validation Loss: 1.2423222064971924\n",
      "Step 260/1000, Loss: 0.9428835511207581, Validation Loss: 1.251726746559143\n",
      "Step 261/1000, Loss: 0.9431136846542358, Validation Loss: 1.1816343069076538\n",
      "Step 262/1000, Loss: 0.9462667107582092, Validation Loss: 1.1916755437850952\n",
      "Step 263/1000, Loss: 0.9447082877159119, Validation Loss: 1.2993887662887573\n",
      "Step 264/1000, Loss: 0.9457187056541443, Validation Loss: 1.3048473596572876\n",
      "Step 265/1000, Loss: 0.9461333155632019, Validation Loss: 1.20816969871521\n",
      "Step 266/1000, Loss: 0.9433029294013977, Validation Loss: 1.2187778949737549\n",
      "Step 267/1000, Loss: 0.9428890943527222, Validation Loss: 1.344796895980835\n",
      "Step 268/1000, Loss: 0.9496927857398987, Validation Loss: 1.3461638689041138\n",
      "Step 269/1000, Loss: 0.9498247504234314, Validation Loss: 1.230789065361023\n",
      "Step 270/1000, Loss: 0.9427297115325928, Validation Loss: 1.179829716682434\n",
      "Step 271/1000, Loss: 0.9465634226799011, Validation Loss: 1.2003099918365479\n",
      "Step 272/1000, Loss: 0.9437745213508606, Validation Loss: 1.3370789289474487\n",
      "Step 273/1000, Loss: 0.9489172101020813, Validation Loss: 1.3630027770996094\n",
      "Step 274/1000, Loss: 0.9515460133552551, Validation Loss: 1.2564696073532104\n",
      "Step 275/1000, Loss: 0.9431924223899841, Validation Loss: 1.1574145555496216\n",
      "Step 276/1000, Loss: 0.9587206244468689, Validation Loss: 1.1614964008331299\n",
      "Step 277/1000, Loss: 0.9736484885215759, Validation Loss: 1.1568089723587036\n",
      "Step 278/1000, Loss: 0.9609588980674744, Validation Loss: 1.217025876045227\n",
      "Step 279/1000, Loss: 0.9428296089172363, Validation Loss: 1.5444567203521729\n",
      "Step 280/1000, Loss: 0.9754869937896729, Validation Loss: 1.7922799587249756\n",
      "Step 281/1000, Loss: 1.0162851810455322, Validation Loss: 1.7798932790756226\n",
      "Step 282/1000, Loss: 1.0141173601150513, Validation Loss: 1.5380542278289795\n",
      "Step 283/1000, Loss: 0.9745338559150696, Validation Loss: 1.245507001876831\n",
      "Step 284/1000, Loss: 0.9428102374076843, Validation Loss: 1.1805697679519653\n",
      "Step 285/1000, Loss: 0.9910163283348083, Validation Loss: 1.3141732215881348\n",
      "Step 286/1000, Loss: 1.0617343187332153, Validation Loss: 1.3592560291290283\n",
      "Step 287/1000, Loss: 1.08146071434021, Validation Loss: 1.2677048444747925\n",
      "Step 288/1000, Loss: 1.0401105880737305, Validation Loss: 1.1606804132461548\n",
      "Step 289/1000, Loss: 0.9724581241607666, Validation Loss: 1.2756412029266357\n",
      "Step 290/1000, Loss: 0.9440439939498901, Validation Loss: 1.4560755491256714\n",
      "Step 291/1000, Loss: 0.9628684520721436, Validation Loss: 1.475205659866333\n",
      "Step 292/1000, Loss: 0.9654762148857117, Validation Loss: 1.3289345502853394\n",
      "Step 293/1000, Loss: 0.9480971693992615, Validation Loss: 1.1700897216796875\n",
      "Step 294/1000, Loss: 0.9490706920623779, Validation Loss: 1.1568255424499512\n",
      "Step 295/1000, Loss: 0.9606673121452332, Validation Loss: 1.1634122133255005\n",
      "Step 296/1000, Loss: 0.9521777033805847, Validation Loss: 1.2531018257141113\n",
      "Step 297/1000, Loss: 0.9430062770843506, Validation Loss: 1.2801437377929688\n",
      "Step 298/1000, Loss: 0.9442986249923706, Validation Loss: 1.2065255641937256\n",
      "Step 299/1000, Loss: 0.9432167410850525, Validation Loss: 1.2311266660690308\n",
      "Step 300/1000, Loss: 0.9425973296165466, Validation Loss: 1.1799216270446777\n",
      "Step 301/1000, Loss: 0.9463717341423035, Validation Loss: 1.2003170251846313\n",
      "Step 302/1000, Loss: 0.9436360001564026, Validation Loss: 1.2569564580917358\n",
      "Step 303/1000, Loss: 0.9431366920471191, Validation Loss: 1.2664045095443726\n",
      "Step 304/1000, Loss: 0.9435445666313171, Validation Loss: 1.225306510925293\n",
      "Step 305/1000, Loss: 0.9426136016845703, Validation Loss: 1.2362126111984253\n",
      "Step 306/1000, Loss: 0.942618191242218, Validation Loss: 1.2040598392486572\n",
      "Step 307/1000, Loss: 0.9433584213256836, Validation Loss: 1.2153007984161377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 308/1000, Loss: 0.9428160786628723, Validation Loss: 1.2767847776412964\n",
      "Step 309/1000, Loss: 0.9440927505493164, Validation Loss: 1.2849383354187012\n",
      "Step 310/1000, Loss: 0.9445923566818237, Validation Loss: 1.2385469675064087\n",
      "Step 311/1000, Loss: 0.9426432251930237, Validation Loss: 1.1759878396987915\n",
      "Step 312/1000, Loss: 0.9472671151161194, Validation Loss: 1.1645809412002563\n",
      "Step 313/1000, Loss: 0.9514721632003784, Validation Loss: 1.1726064682006836\n",
      "Step 314/1000, Loss: 0.9482073783874512, Validation Loss: 1.2153347730636597\n",
      "Step 315/1000, Loss: 0.942812979221344, Validation Loss: 1.3406826257705688\n",
      "Step 316/1000, Loss: 0.9492040872573853, Validation Loss: 1.4215774536132812\n",
      "Step 317/1000, Loss: 0.9583653211593628, Validation Loss: 1.4161572456359863\n",
      "Step 318/1000, Loss: 0.957686722278595, Validation Loss: 1.3349827527999878\n",
      "Step 319/1000, Loss: 0.9486513137817383, Validation Loss: 1.2250415086746216\n",
      "Step 320/1000, Loss: 0.9426065683364868, Validation Loss: 1.1963223218917847\n",
      "Step 321/1000, Loss: 0.9439734220504761, Validation Loss: 1.2074637413024902\n",
      "Step 322/1000, Loss: 0.9431459307670593, Validation Loss: 1.266513705253601\n",
      "Step 323/1000, Loss: 0.9435421824455261, Validation Loss: 1.2753195762634277\n",
      "Step 324/1000, Loss: 0.944003701210022, Validation Loss: 1.2316248416900635\n",
      "Step 325/1000, Loss: 0.9425826668739319, Validation Loss: 1.1729960441589355\n",
      "Step 326/1000, Loss: 0.9480865597724915, Validation Loss: 1.1628707647323608\n",
      "Step 327/1000, Loss: 0.9524869322776794, Validation Loss: 1.170341968536377\n",
      "Step 328/1000, Loss: 0.9489450454711914, Validation Loss: 1.2113001346588135\n",
      "Step 329/1000, Loss: 0.9429574012756348, Validation Loss: 1.3341064453125\n",
      "Step 330/1000, Loss: 0.9485647082328796, Validation Loss: 1.4143222570419312\n",
      "Step 331/1000, Loss: 0.9574554562568665, Validation Loss: 1.409593939781189\n",
      "Step 332/1000, Loss: 0.9568716287612915, Validation Loss: 1.3300198316574097\n",
      "Step 333/1000, Loss: 0.9481821656227112, Validation Loss: 1.2222256660461426\n",
      "Step 334/1000, Loss: 0.9426405429840088, Validation Loss: 1.194357991218567\n",
      "Step 335/1000, Loss: 0.9441695213317871, Validation Loss: 1.2054154872894287\n",
      "Step 336/1000, Loss: 0.9432617425918579, Validation Loss: 1.2637290954589844\n",
      "Step 337/1000, Loss: 0.9434061646461487, Validation Loss: 1.2726540565490723\n",
      "Step 338/1000, Loss: 0.9438520073890686, Validation Loss: 1.229660987854004\n",
      "Step 339/1000, Loss: 0.942575991153717, Validation Loss: 1.240234613418579\n",
      "Step 340/1000, Loss: 0.9426589012145996, Validation Loss: 1.2067114114761353\n",
      "Step 341/1000, Loss: 0.9431791305541992, Validation Loss: 1.217797875404358\n",
      "Step 342/1000, Loss: 0.9427281022071838, Validation Loss: 1.2797846794128418\n",
      "Step 343/1000, Loss: 0.9442580938339233, Validation Loss: 1.2875103950500488\n",
      "Step 344/1000, Loss: 0.9447495341300964, Validation Loss: 1.240220308303833\n",
      "Step 345/1000, Loss: 0.9426563382148743, Validation Loss: 1.1766319274902344\n",
      "Step 346/1000, Loss: 0.9470802545547485, Validation Loss: 1.1648964881896973\n",
      "Step 347/1000, Loss: 0.9512784481048584, Validation Loss: 1.1729257106781006\n",
      "Step 348/1000, Loss: 0.9480905532836914, Validation Loss: 1.2157232761383057\n",
      "Step 349/1000, Loss: 0.9427821636199951, Validation Loss: 1.3410286903381348\n",
      "Early stopping triggered\n",
      "fixing (0,0,0) with erfc, r2=0.9999300837516785\n",
      "fixing (0,0,1) with erfc, r2=0.9997511506080627\n",
      "fixing (0,1,0) with x, r2=1.0000007152557373\n",
      "fixing (0,1,1) with x, r2=1.0000004768371582\n",
      "fixing (1,0,0) with x, r2=1.0000003576278687\n",
      "fixing (1,0,1) with x, r2=1.0000003576278687\n",
      "fixing (1,1,0) with x, r2=1.0000003576278687\n",
      "fixing (1,1,1) with x, r2=1.0000005960464478\n",
      "fixing (2,0,0) with x, r2=1.000000238418579\n",
      "fixing (2,1,0) with x, r2=1.0000003576278687\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sympy import symbols, sympify\n",
    "from scipy.special import erfc\n",
    "from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# Parameters\n",
    "alpha = 2.5e-6\n",
    "T0 = 20\n",
    "T1 = 40\n",
    "L = 4\n",
    "dx = 0.1\n",
    "dt = 1800\n",
    "tMax = 86400\n",
    "W = 30\n",
    "\n",
    "# Time and space grid\n",
    "x = np.arange(0, L + dx, dx)\n",
    "t = np.arange(dt, tMax + dt, dt)\n",
    "\n",
    "# Initialize temperature data storage\n",
    "TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# Compute temperature distribution and store results\n",
    "for k in range(len(t)):\n",
    "    TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# Prepare the data\n",
    "x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Normalize the data\n",
    "x_mean = np.mean(x_data, axis=0)\n",
    "x_std = np.std(x_data, axis=0)\n",
    "y_mean = np.mean(y_data)\n",
    "y_std = np.std(y_data)\n",
    "\n",
    "x_data_normalized = (x_data - x_mean) / x_std\n",
    "y_data_normalized = (y_data - y_mean) / y_std\n",
    "# x_data_normalized = x_data\n",
    "# y_data_normalized = y_data\n",
    "\n",
    "# Convert to tensors\n",
    "x_tensor = torch.tensor(x_data_normalized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_data_normalized, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "train_size = int(0.8 * len(x_tensor))\n",
    "test_size = len(x_tensor) - train_size\n",
    "train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# Create the dataset dictionary as expected by KAN\n",
    "dataset = {\n",
    "    'train_input': train_input,\n",
    "    'train_label': train_label,\n",
    "    'test_input': test_input,\n",
    "    'test_label': test_label\n",
    "}\n",
    "\n",
    "# Add erfc to the symbolic library\n",
    "add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# Function to train KAN with early stopping and learning rate scheduling\n",
    "def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "    current_lr = initial_lr\n",
    "\n",
    "    for step in range(steps):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(dataset['train_input'])\n",
    "            loss = criterion(outputs, dataset['train_label'])\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(closure).item()\n",
    "\n",
    "        # Validation step\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(dataset['test_input'])\n",
    "            val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "        print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        if step % patience == 0 and step > 0:\n",
    "            current_lr = max(min_lr, current_lr * lr_decay)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "\n",
    "        # Check for NaN values\n",
    "        if np.isnan(loss) or np.isnan(val_loss):\n",
    "            print(\"NaN detected, stopping training\")\n",
    "            break\n",
    "\n",
    "    # Load the best model\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "# Initial training with a coarse grid\n",
    "initial_grid = 3\n",
    "model = KAN(width=[2, 2, 2, 1], grid=initial_grid, k=3, seed=0)\n",
    "train_with_early_stopping(model, dataset, steps=1000, patience=100, initial_lr=0.002)\n",
    "\n",
    "# Iteratively refine the grid and retrain the model\n",
    "grids = [5, 10, 20]\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for grid in grids:\n",
    "    new_model = KAN(width=[2, 2, 2, 1], grid=grid, k=3).initialize_from_another_model(model, dataset['train_input'])\n",
    "    train_with_early_stopping(new_model, dataset, steps=1000, patience=100, initial_lr=0.00025)\n",
    "    model = new_model  # Update the model to the new refined grid model\n",
    "\n",
    "    # Collect training and test losses\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(dataset['train_input'])\n",
    "        train_loss = torch.nn.functional.mse_loss(train_outputs, dataset['train_label']).item()\n",
    "        test_outputs = model(dataset['test_input'])\n",
    "        test_loss = torch.nn.functional.mse_loss(test_outputs, dataset['test_label']).item()\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "# Automatically set activation functions to be symbolic\n",
    "lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "model.auto_symbolic(lib=lib)\n",
    "\n",
    "# Prune the model\n",
    "model = model.prune()\n",
    "\n",
    "# Obtain the symbolic formula and denormalize it\n",
    "symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# symbolic_formula_denormalized = sympify(symbolic_formula.replace('x_1', '(x_1*{:.6f}+{:.6f})'.format(x_std[0], x_mean[0]))\n",
    "#                                         .replace('x_2', '(x_2*{:.6f}+{:.6f})'.format(x_std[1], x_mean[1])))\n",
    "# symbolic_formula_denormalized = (symbolic_formula_denormalized * y_std + y_mean).simplify()\n",
    "\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula_denormalized)\n",
    "\n",
    "# Create output directory for plots\n",
    "outputDir = 'TemperaturePlots'\n",
    "if not os.path.exists(outputDir):\n",
    "    os.makedirs(outputDir)\n",
    "\n",
    "# Plot predicted temperature distribution\n",
    "x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "x_test_normalized = (x_test - x_mean) / x_std\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "predicted_temperature = model(x_test_tensor).detach().numpy().flatten() * y_std + y_mean\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, predicted_temperature, label='Predicted')\n",
    "plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "plt.xlabel('Position along the wall thickness (m)')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and test losses over different grid refinements\n",
    "plt.figure()\n",
    "plt.plot(grids, train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(grids, test_losses, marker='o', label='Test Loss')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Grid Size')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Training and Test Losses over Grid Refinements')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14422989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered Symbolic Formula (Normalized):\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.01 \\operatorname{erfc}{\\left(0.72 - 0.28 x_{1} \\right)} - 0.46$"
      ],
      "text/plain": [
       "-0.01*erfc(0.72 - 0.28*x_1) - 0.46"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Discovered Symbolic Formula (Normalized):\")\n",
    "model.symbolic_formula()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd4dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Discovered Symbolic Formula (Original):\")\n",
    "# print(original_formula)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
