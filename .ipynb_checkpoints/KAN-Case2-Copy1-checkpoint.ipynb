{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f8a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://swarma.org/?p=50312\n",
    "# %% 固定壁面温度的一维稳态计算\n",
    "\n",
    "# % 参数设置\n",
    "# L = 0.24;          % 墙体的厚度，单位为米 (240mm)\n",
    "# W = 1.0;           % 墙体的长度，单位为米 (1000mm)\n",
    "# alpha = 2.5*10^(-6);      % 热扩散率，单位为平方米每秒\n",
    "# T0 = 20;           % x=0处的温度，单位为摄氏度\n",
    "# TL = 40;           % x=L处的温度，单位为摄氏度\n",
    "# Ti = 25;           % 初始平均温度，单位为摄氏度\n",
    "\n",
    "# % 计算温度分布\n",
    "# x = linspace(0, L, 100); % 在墙的厚度方向生成100个点以计算温度分布\n",
    "# T = T0 + (TL - T0) * (x / L); % 线性温度分布计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553725ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6  # Thermal diffusivity of the soil in m^2/s\n",
    "# T0 = 20        # Initial surface temperature in Celsius\n",
    "# T1 = 40        # Surface temperature after change in Celsius\n",
    "# L = 4          # Calculation thickness in meters\n",
    "# dx = 0.1       # Spatial step in meters\n",
    "# dt = 1800      # Time step in seconds\n",
    "# tMax = 86400   # Simulation duration in seconds (1 day)\n",
    "# W = 30         # Image width for plotting in meters\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)  # Space grid\n",
    "# t = np.arange(dt, tMax + dt, dt)  # Time grid, start from dt to avoid divide by zero\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Normalize the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# x_mean = np.mean(x_data, axis=0)\n",
    "# x_std = np.std(x_data, axis=0) + 1e-8  # Add small constant to avoid division by zero\n",
    "# x_data_norm = (x_data - x_mean) / x_std\n",
    "\n",
    "# y_mean = np.mean(y_data)\n",
    "# y_std = np.std(y_data) + 1e-8  # Add small constant to avoid division by zero\n",
    "# y_data_norm = (y_data - y_mean) / y_std\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data_norm, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data_norm, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Initialize KAN with adjusted parameters\n",
    "# model = KAN(width=[2, 10, 1], grid=20, k=3, seed=0)  # Input layer now has 2 neurons\n",
    "\n",
    "# # Plot KAN at initialization\n",
    "# model(dataset['train_input'])\n",
    "# model.plot(beta=100)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Train the model with early stopping and learning rate scheduling\n",
    "# train_with_early_stopping(model, dataset, steps=500, patience=100, initial_lr=0.0025)\n",
    "\n",
    "# # Plot trained KAN\n",
    "# model.plot()\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "# # # Continue training to almost machine precision with conservative settings\n",
    "# # train_with_early_stopping(model, dataset, steps=200, patience=50, initial_lr=0.00001)\n",
    "\n",
    "# # Obtain the symbolic formula in terms of normalized data\n",
    "# symbolic_formula_normalized = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula (Normalized):\")\n",
    "# print(symbolic_formula_normalized)\n",
    "\n",
    "# # Reverse normalization for symbolic formula using sympy\n",
    "# x_sym, t_sym = symbols('x t')\n",
    "# normalized_formula = sympify(symbolic_formula_normalized)\n",
    "\n",
    "# # Replace normalized variables with original scale variables\n",
    "# original_formula = normalized_formula * (y_std / x_std[0]) + (y_mean - y_std / x_std[0] * x_mean[0])\n",
    "\n",
    "# print(\"Discovered Symbolic Formula (Original):\")\n",
    "# print(original_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_norm = (x_test - x_mean) / x_std\n",
    "# x_test_tensor = torch.tensor(x_test_norm, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "# predicted_temperature = predicted_temperature * y_std + y_mean  # Denormalize the prediction\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff501151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6  # Thermal diffusivity of the soil in m^2/s\n",
    "# T0 = 20        # Initial surface temperature in Celsius\n",
    "# T1 = 40        # Surface temperature after change in Celsius\n",
    "# L = 4          # Calculation thickness in meters\n",
    "# dx = 0.1       # Spatial step in meters\n",
    "# dt = 1800      # Time step in seconds\n",
    "# tMax = 86400   # Simulation duration in seconds (1 day)\n",
    "# W = 30         # Image width for plotting in meters\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)  # Space grid\n",
    "# t = np.arange(dt, tMax + dt, dt)  # Time grid, start from dt to avoid divide by zero\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Prepare the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Initialize KAN with adjusted parameters\n",
    "# model = KAN(width=[2, 20, 1], grid=20, k=3, seed=0)  # Input layer now has 2 neurons\n",
    "\n",
    "# # Plot KAN at initialization\n",
    "# model(dataset['train_input'])\n",
    "# model.plot(beta=100)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Train the model with early stopping and learning rate scheduling\n",
    "# train_with_early_stopping(model, dataset, steps=500, patience=100, initial_lr=0.0025)\n",
    "\n",
    "# # Plot trained KAN\n",
    "# model.plot()\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "# # # Continue training to almost machine precision with conservative settings\n",
    "# # train_with_early_stopping(model, dataset, steps=200, patience=50, initial_lr=0.00001)\n",
    "\n",
    "# # Obtain the symbolic formula\n",
    "# symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f414d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sympy import symbols, sympify\n",
    "from scipy.special import erfc\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# Parameters\n",
    "alpha = 2.5e-6\n",
    "T0 = 20\n",
    "T1 = 40\n",
    "L = 1\n",
    "dx = 0.1\n",
    "dt = 1800\n",
    "tMax = 86400\n",
    "W = 30\n",
    "\n",
    "# Time and space grid\n",
    "x = np.arange(0, L + dx, dx)\n",
    "t = np.arange(dt, tMax + dt, dt)\n",
    "\n",
    "# Initialize temperature data storage\n",
    "TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# Compute temperature distribution and store results\n",
    "for k in range(len(t)):\n",
    "    TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# Prepare the data\n",
    "x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "y_data = TemperatureData.flatten()\n",
    "\n",
    "# Normalize the data\n",
    "# x_mean = np.mean(x_data, axis=0)\n",
    "# x_std = np.std(x_data, axis=0)\n",
    "# y_mean = np.mean(y_data)\n",
    "# y_std = np.std(y_data)\n",
    "\n",
    "# x_data_normalized = (x_data - x_mean) / x_std\n",
    "# y_data_normalized = (y_data - y_mean) / y_std\n",
    "\n",
    "x_data_normalized = x_data\n",
    "y_data_normalized = y_data\n",
    "\n",
    "# Convert to tensors\n",
    "x_tensor = torch.tensor(x_data_normalized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_data_normalized, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "train_size = int(0.8 * len(x_tensor))\n",
    "test_size = len(x_tensor) - train_size\n",
    "train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# Create the dataset dictionary as expected by KAN\n",
    "dataset = {\n",
    "    'train_input': train_input,\n",
    "    'train_label': train_label,\n",
    "    'test_input': test_input,\n",
    "    'test_label': test_label\n",
    "}\n",
    "\n",
    "# Add erfc to the symbolic library\n",
    "add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# Train the model using the tutorial's approach\n",
    "model = KAN(width=[2, 2, 2, 1], grid=5, k=3, seed=0)\n",
    "# model.train(dataset, opt=\"LBFGS\", steps=100, lamb=0.1, lamb_entropy=0)\n",
    "model.train(dataset, opt=\"LBFGS\", steps=100, lamb=0.1, lamb_entropy=2.)\n",
    "\n",
    "\n",
    "# Automatically set activation functions to be symbolic\n",
    "lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "model.auto_symbolic(lib=lib)\n",
    "\n",
    "# Prune the model\n",
    "model.prune()\n",
    "\n",
    "# Plot the model\n",
    "model.plot()\n",
    "model.plot(mask=True)\n",
    "\n",
    "# Obtain the symbolic formula and denormalize it\n",
    "symbolic_formula = model.symbolic_formula()[0][0]\n",
    "\n",
    "# Evaluate the final accuracy of the prediction\n",
    "x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_normalized = (x_test - x_mean) / x_std\n",
    "# x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten() * y_std + y_mean\n",
    "predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(TemperatureData[-1, :], predicted_temperature)\n",
    "mse = mean_squared_error(TemperatureData[-1, :], predicted_temperature)\n",
    "r2 = r2_score(TemperatureData[-1, :], predicted_temperature)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n",
    "\n",
    "# Create output directory for plots\n",
    "outputDir = 'TemperaturePlots'\n",
    "if not os.path.exists(outputDir):\n",
    "    os.makedirs(outputDir)\n",
    "\n",
    "# Plot predicted temperature distribution\n",
    "plt.figure()\n",
    "plt.plot(x, predicted_temperature, label='Predicted')\n",
    "plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "plt.xlabel('Position along the wall thickness (m)')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and test losses over different grid refinements\n",
    "plt.figure()\n",
    "plt.plot(grids, train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(grids, test_losses, marker='o', label='Test Loss')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Grid Size')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Training and Test Losses over Grid Refinements')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42530ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Discovered Symbolic Formula (Normalized):\")\n",
    "model.symbolic_formula()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21b6691",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6\n",
    "# T0 = 20\n",
    "# T1 = 40\n",
    "# L = 1\n",
    "# dx = 0.1\n",
    "# dt = 1800\n",
    "# tMax = 86400\n",
    "# W = 30\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)\n",
    "# t = np.arange(dt, tMax + dt, dt)\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Prepare the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Normalize the data\n",
    "# x_mean = np.mean(x_data, axis=0)\n",
    "# x_std = np.std(x_data, axis=0)\n",
    "# y_mean = np.mean(y_data)\n",
    "# y_std = np.std(y_data)\n",
    "\n",
    "# x_data_normalized = (x_data - x_mean) / x_std\n",
    "# y_data_normalized = (y_data - y_mean) / y_std\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data_normalized, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data_normalized, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Initial training with a coarse grid\n",
    "# initial_grid = 3\n",
    "# model = KAN(width=[2, 2, 2, 1], grid=initial_grid, k=3, seed=0)\n",
    "# train_with_early_stopping(model, dataset, steps=1000, patience=100, initial_lr=0.002)\n",
    "\n",
    "# # Iteratively refine the grid and retrain the model\n",
    "# grids = [5, 10, 20]\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "\n",
    "# for grid in grids:\n",
    "#     new_model = KAN(width=[2, 2, 2, 1], grid=grid, k=3).initialize_from_another_model(model, dataset['train_input'])\n",
    "#     train_with_early_stopping(new_model, dataset, steps=1000, patience=100, initial_lr=0.002)\n",
    "#     model = new_model  # Update the model to the new refined grid model\n",
    "\n",
    "#     # Collect training and test losses\n",
    "#     with torch.no_grad():\n",
    "#         train_outputs = model(dataset['train_input'])\n",
    "#         train_loss = torch.nn.functional.mse_loss(train_outputs, dataset['train_label']).item()\n",
    "#         test_outputs = model(dataset['test_input'])\n",
    "#         test_loss = torch.nn.functional.mse_loss(test_outputs, dataset['test_label']).item()\n",
    "#         train_losses.append(train_loss)\n",
    "#         test_losses.append(test_loss)\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "\n",
    "# # Obtain the symbolic formula and denormalize it\n",
    "# symbolic_formula = model.symbolic_formula()[0][0]\n",
    "\n",
    "# # Evaluate the final accuracy of the prediction\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_normalized = (x_test - x_mean) / x_std\n",
    "# x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten() * y_std + y_mean\n",
    "\n",
    "# # Calculate metrics\n",
    "# mae = mean_absolute_error(TemperatureData[-1, :], predicted_temperature)\n",
    "# mse = mean_squared_error(TemperatureData[-1, :], predicted_temperature)\n",
    "# r2 = r2_score(TemperatureData[-1, :], predicted_temperature)\n",
    "\n",
    "# print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "# print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "# print(f\"R-squared (R²): {r2}\")\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training and test losses over different grid refinements\n",
    "# plt.figure()\n",
    "# plt.plot(grids, train_losses, marker='o', label='Train Loss')\n",
    "# plt.plot(grids, test_losses, marker='o', label='Test Loss')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel('Grid Size')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.title('Training and Test Losses over Grid Refinements')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14422989",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Discovered Symbolic Formula (Normalized):\")\n",
    "model.symbolic_formula()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd4dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Discovered Symbolic Formula (Original):\")\n",
    "# print(original_formula)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
