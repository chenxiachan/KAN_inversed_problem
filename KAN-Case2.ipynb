{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f8a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 固定壁面温度的一维稳态计算\n",
    "\n",
    "# % 参数设置\n",
    "# L = 0.24;          % 墙体的厚度，单位为米 (240mm)\n",
    "# W = 1.0;           % 墙体的长度，单位为米 (1000mm)\n",
    "# alpha = 2.5*10^(-6);      % 热扩散率，单位为平方米每秒\n",
    "# T0 = 20;           % x=0处的温度，单位为摄氏度\n",
    "# TL = 40;           % x=L处的温度，单位为摄氏度\n",
    "# Ti = 25;           % 初始平均温度，单位为摄氏度\n",
    "\n",
    "# % 计算温度分布\n",
    "# x = linspace(0, L, 100); % 在墙的厚度方向生成100个点以计算温度分布\n",
    "# T = T0 + (TL - T0) * (x / L); % 线性温度分布计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553725ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6  # Thermal diffusivity of the soil in m^2/s\n",
    "# T0 = 20        # Initial surface temperature in Celsius\n",
    "# T1 = 40        # Surface temperature after change in Celsius\n",
    "# L = 4          # Calculation thickness in meters\n",
    "# dx = 0.1       # Spatial step in meters\n",
    "# dt = 1800      # Time step in seconds\n",
    "# tMax = 86400   # Simulation duration in seconds (1 day)\n",
    "# W = 30         # Image width for plotting in meters\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)  # Space grid\n",
    "# t = np.arange(dt, tMax + dt, dt)  # Time grid, start from dt to avoid divide by zero\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Normalize the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# x_mean = np.mean(x_data, axis=0)\n",
    "# x_std = np.std(x_data, axis=0) + 1e-8  # Add small constant to avoid division by zero\n",
    "# x_data_norm = (x_data - x_mean) / x_std\n",
    "\n",
    "# y_mean = np.mean(y_data)\n",
    "# y_std = np.std(y_data) + 1e-8  # Add small constant to avoid division by zero\n",
    "# y_data_norm = (y_data - y_mean) / y_std\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data_norm, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data_norm, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Initialize KAN with adjusted parameters\n",
    "# model = KAN(width=[2, 10, 1], grid=20, k=3, seed=0)  # Input layer now has 2 neurons\n",
    "\n",
    "# # Plot KAN at initialization\n",
    "# model(dataset['train_input'])\n",
    "# model.plot(beta=100)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Train the model with early stopping and learning rate scheduling\n",
    "# train_with_early_stopping(model, dataset, steps=500, patience=100, initial_lr=0.0025)\n",
    "\n",
    "# # Plot trained KAN\n",
    "# model.plot()\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "# # # Continue training to almost machine precision with conservative settings\n",
    "# # train_with_early_stopping(model, dataset, steps=200, patience=50, initial_lr=0.00001)\n",
    "\n",
    "# # Obtain the symbolic formula in terms of normalized data\n",
    "# symbolic_formula_normalized = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula (Normalized):\")\n",
    "# print(symbolic_formula_normalized)\n",
    "\n",
    "# # Reverse normalization for symbolic formula using sympy\n",
    "# x_sym, t_sym = symbols('x t')\n",
    "# normalized_formula = sympify(symbolic_formula_normalized)\n",
    "\n",
    "# # Replace normalized variables with original scale variables\n",
    "# original_formula = normalized_formula * (y_std / x_std[0]) + (y_mean - y_std / x_std[0] * x_mean[0])\n",
    "\n",
    "# print(\"Discovered Symbolic Formula (Original):\")\n",
    "# print(original_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_norm = (x_test - x_mean) / x_std\n",
    "# x_test_tensor = torch.tensor(x_test_norm, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "# predicted_temperature = predicted_temperature * y_std + y_mean  # Denormalize the prediction\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff501151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6  # Thermal diffusivity of the soil in m^2/s\n",
    "# T0 = 20        # Initial surface temperature in Celsius\n",
    "# T1 = 40        # Surface temperature after change in Celsius\n",
    "# L = 4          # Calculation thickness in meters\n",
    "# dx = 0.1       # Spatial step in meters\n",
    "# dt = 1800      # Time step in seconds\n",
    "# tMax = 86400   # Simulation duration in seconds (1 day)\n",
    "# W = 30         # Image width for plotting in meters\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)  # Space grid\n",
    "# t = np.arange(dt, tMax + dt, dt)  # Time grid, start from dt to avoid divide by zero\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Prepare the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Initialize KAN with adjusted parameters\n",
    "# model = KAN(width=[2, 20, 1], grid=20, k=3, seed=0)  # Input layer now has 2 neurons\n",
    "\n",
    "# # Plot KAN at initialization\n",
    "# model(dataset['train_input'])\n",
    "# model.plot(beta=100)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Train the model with early stopping and learning rate scheduling\n",
    "# train_with_early_stopping(model, dataset, steps=500, patience=100, initial_lr=0.0025)\n",
    "\n",
    "# # Plot trained KAN\n",
    "# model.plot()\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "# # # Continue training to almost machine precision with conservative settings\n",
    "# # train_with_early_stopping(model, dataset, steps=200, patience=50, initial_lr=0.00001)\n",
    "\n",
    "# # Obtain the symbolic formula\n",
    "# symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4f414d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6\n",
    "# T0 = 20\n",
    "# T1 = 40\n",
    "# L = 4\n",
    "# dx = 0.1\n",
    "# dt = 1800\n",
    "# tMax = 86400\n",
    "# W = 30\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)\n",
    "# t = np.arange(dt, tMax + dt, dt)\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Prepare the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Initial training with a coarse grid\n",
    "# initial_grid = 3\n",
    "# model = KAN(width=[2, 2, 2, 1], grid=initial_grid, k=3, seed=0)\n",
    "# train_with_early_stopping(model, dataset, steps=1000, patience=100, initial_lr=0.002)\n",
    "\n",
    "# # Iteratively refine the grid and retrain the model\n",
    "# grids = [5, 10, 20, 50, 100]\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "\n",
    "# for grid in grids:\n",
    "#     new_model = KAN(width=[2, 2, 2, 1], grid=grid, k=3).initialize_from_another_model(model, dataset['train_input'])\n",
    "#     train_with_early_stopping(new_model, dataset, steps=500, patience=100, initial_lr=0.00005)\n",
    "#     model = new_model  # Update the model to the new refined grid model\n",
    "\n",
    "#     # Collect training and test losses\n",
    "#     with torch.no_grad():\n",
    "#         train_outputs = model(dataset['train_input'])\n",
    "#         train_loss = torch.nn.functional.mse_loss(train_outputs, dataset['train_label']).item()\n",
    "#         test_outputs = model(dataset['test_input'])\n",
    "#         test_loss = torch.nn.functional.mse_loss(test_outputs, dataset['test_label']).item()\n",
    "#         train_losses.append(train_loss)\n",
    "#         test_losses.append(test_loss)\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "\n",
    "# # Obtain the symbolic formula\n",
    "# symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training and test losses over different grid refinements\n",
    "# plt.figure()\n",
    "# plt.plot(grids, train_losses, marker='o', label='Train Loss')\n",
    "# plt.plot(grids, test_losses, marker='o', label='Test Loss')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel('Grid Size')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.title('Training and Test Losses over Grid Refinements')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b21b6691",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1000, Loss: 1.006316065788269, Validation Loss: 1.371787428855896\n",
      "Step 2/1000, Loss: 1.0023128986358643, Validation Loss: 1.359310269355774\n",
      "Step 3/1000, Loss: 0.9985527396202087, Validation Loss: 1.3476669788360596\n",
      "Step 4/1000, Loss: 0.9950290322303772, Validation Loss: 1.3368865251541138\n",
      "Step 5/1000, Loss: 0.9917342066764832, Validation Loss: 1.3269546031951904\n",
      "Step 6/1000, Loss: 0.9886600375175476, Validation Loss: 1.3178017139434814\n",
      "Step 7/1000, Loss: 0.9857967495918274, Validation Loss: 1.3093459606170654\n",
      "Step 8/1000, Loss: 0.9831345081329346, Validation Loss: 1.301518440246582\n",
      "Step 9/1000, Loss: 0.9806627631187439, Validation Loss: 1.294265866279602\n",
      "Step 10/1000, Loss: 0.9783706068992615, Validation Loss: 1.2875436544418335\n",
      "Step 11/1000, Loss: 0.97624671459198, Validation Loss: 1.2813127040863037\n",
      "Step 12/1000, Loss: 0.9742792844772339, Validation Loss: 1.2755358219146729\n",
      "Step 13/1000, Loss: 0.9724573493003845, Validation Loss: 1.2701786756515503\n",
      "Step 14/1000, Loss: 0.9707695245742798, Validation Loss: 1.2652071714401245\n",
      "Step 15/1000, Loss: 0.969205379486084, Validation Loss: 1.2605899572372437\n",
      "Step 16/1000, Loss: 0.9677556157112122, Validation Loss: 1.2562960386276245\n",
      "Step 17/1000, Loss: 0.9664110541343689, Validation Loss: 1.2522974014282227\n",
      "Step 18/1000, Loss: 0.9651638865470886, Validation Loss: 1.2485681772232056\n",
      "Step 19/1000, Loss: 0.964007556438446, Validation Loss: 1.2450841665267944\n",
      "Step 20/1000, Loss: 0.9629359245300293, Validation Loss: 1.241824746131897\n",
      "Step 21/1000, Loss: 0.9619439840316772, Validation Loss: 1.2387703657150269\n",
      "Step 22/1000, Loss: 0.9610271453857422, Validation Loss: 1.2359050512313843\n",
      "Step 23/1000, Loss: 0.9601820707321167, Validation Loss: 1.233214259147644\n",
      "Step 24/1000, Loss: 0.9594050049781799, Validation Loss: 1.2306855916976929\n",
      "Step 25/1000, Loss: 0.9586933255195618, Validation Loss: 1.2283083200454712\n",
      "Step 26/1000, Loss: 0.9580438733100891, Validation Loss: 1.2260736227035522\n",
      "Step 27/1000, Loss: 0.9574540853500366, Validation Loss: 1.2239737510681152\n",
      "Step 28/1000, Loss: 0.9569212198257446, Validation Loss: 1.2220022678375244\n",
      "Step 29/1000, Loss: 0.9564422369003296, Validation Loss: 1.2201539278030396\n",
      "Step 30/1000, Loss: 0.9560143351554871, Validation Loss: 1.2184239625930786\n",
      "Step 31/1000, Loss: 0.9556341171264648, Validation Loss: 1.216808557510376\n",
      "Step 32/1000, Loss: 0.9552984833717346, Validation Loss: 1.2153042554855347\n",
      "Step 33/1000, Loss: 0.9550034999847412, Validation Loss: 1.213908314704895\n",
      "Step 34/1000, Loss: 0.9547455906867981, Validation Loss: 1.2126178741455078\n",
      "Step 35/1000, Loss: 0.9545210003852844, Validation Loss: 1.211430311203003\n",
      "Step 36/1000, Loss: 0.9543253779411316, Validation Loss: 1.2103431224822998\n",
      "Step 37/1000, Loss: 0.9541550874710083, Validation Loss: 1.2093533277511597\n",
      "Step 38/1000, Loss: 0.9540061950683594, Validation Loss: 1.2084585428237915\n",
      "Step 39/1000, Loss: 0.9538751244544983, Validation Loss: 1.2076553106307983\n",
      "Step 40/1000, Loss: 0.9537583589553833, Validation Loss: 1.2069404125213623\n",
      "Step 41/1000, Loss: 0.9536526203155518, Validation Loss: 1.2063097953796387\n",
      "Step 42/1000, Loss: 0.9535552859306335, Validation Loss: 1.2057592868804932\n",
      "Step 43/1000, Loss: 0.9534639120101929, Validation Loss: 1.2052843570709229\n",
      "Step 44/1000, Loss: 0.953376054763794, Validation Loss: 1.204879879951477\n",
      "Step 45/1000, Loss: 0.9532900452613831, Validation Loss: 1.2045402526855469\n",
      "Step 46/1000, Loss: 0.9532045722007751, Validation Loss: 1.2042597532272339\n",
      "Step 47/1000, Loss: 0.9531185030937195, Validation Loss: 1.204032063484192\n",
      "Step 48/1000, Loss: 0.9530307054519653, Validation Loss: 1.203850507736206\n",
      "Step 49/1000, Loss: 0.9529401659965515, Validation Loss: 1.203708529472351\n",
      "Step 50/1000, Loss: 0.9528465867042542, Validation Loss: 1.2035988569259644\n",
      "Step 51/1000, Loss: 0.9527490139007568, Validation Loss: 1.2035146951675415\n",
      "Step 52/1000, Loss: 0.9526470303535461, Validation Loss: 1.2034488916397095\n",
      "Step 53/1000, Loss: 0.95253986120224, Validation Loss: 1.2033944129943848\n",
      "Step 54/1000, Loss: 0.9524268507957458, Validation Loss: 1.2033445835113525\n",
      "Step 55/1000, Loss: 0.9523075222969055, Validation Loss: 1.2032928466796875\n",
      "Step 56/1000, Loss: 0.9521806836128235, Validation Loss: 1.203233003616333\n",
      "Step 57/1000, Loss: 0.9520459175109863, Validation Loss: 1.2031595706939697\n",
      "Step 58/1000, Loss: 0.9519021511077881, Validation Loss: 1.2030670642852783\n",
      "Step 59/1000, Loss: 0.9517484307289124, Validation Loss: 1.2029513120651245\n",
      "Step 60/1000, Loss: 0.951583981513977, Validation Loss: 1.202807903289795\n",
      "Step 61/1000, Loss: 0.951407790184021, Validation Loss: 1.2026338577270508\n",
      "Step 62/1000, Loss: 0.9512190222740173, Validation Loss: 1.2024258375167847\n",
      "Step 63/1000, Loss: 0.9510166049003601, Validation Loss: 1.202182650566101\n",
      "Step 64/1000, Loss: 0.9507996439933777, Validation Loss: 1.2019020318984985\n",
      "Step 65/1000, Loss: 0.9505671858787537, Validation Loss: 1.2015831470489502\n",
      "Step 66/1000, Loss: 0.9503183364868164, Validation Loss: 1.2012255191802979\n",
      "Step 67/1000, Loss: 0.9500519037246704, Validation Loss: 1.2008287906646729\n",
      "Step 68/1000, Loss: 0.9497670531272888, Validation Loss: 1.2003929615020752\n",
      "Step 69/1000, Loss: 0.9494625926017761, Validation Loss: 1.1999183893203735\n",
      "Step 70/1000, Loss: 0.9491373300552368, Validation Loss: 1.1994054317474365\n",
      "Step 71/1000, Loss: 0.9487900137901306, Validation Loss: 1.1988542079925537\n",
      "Step 72/1000, Loss: 0.9484193325042725, Validation Loss: 1.1982648372650146\n",
      "Step 73/1000, Loss: 0.948023796081543, Validation Loss: 1.197637677192688\n",
      "Step 74/1000, Loss: 0.9476017355918884, Validation Loss: 1.1969720125198364\n",
      "Step 75/1000, Loss: 0.9471518397331238, Validation Loss: 1.1962674856185913\n",
      "Step 76/1000, Loss: 0.9466719627380371, Validation Loss: 1.1955229043960571\n",
      "Step 77/1000, Loss: 0.9461601376533508, Validation Loss: 1.194736361503601\n",
      "Step 78/1000, Loss: 0.9456146359443665, Validation Loss: 1.1939059495925903\n",
      "Step 79/1000, Loss: 0.9450330138206482, Validation Loss: 1.1930289268493652\n",
      "Step 80/1000, Loss: 0.9444131851196289, Validation Loss: 1.1921019554138184\n",
      "Step 81/1000, Loss: 0.9437525868415833, Validation Loss: 1.1911208629608154\n",
      "Step 82/1000, Loss: 0.9430487155914307, Validation Loss: 1.1900814771652222\n",
      "Step 83/1000, Loss: 0.9422988295555115, Validation Loss: 1.1889783143997192\n",
      "Step 84/1000, Loss: 0.9415000081062317, Validation Loss: 1.1878058910369873\n",
      "Step 85/1000, Loss: 0.9406492710113525, Validation Loss: 1.1865577697753906\n",
      "Step 86/1000, Loss: 0.9397430419921875, Validation Loss: 1.185227394104004\n",
      "Step 87/1000, Loss: 0.9387778043746948, Validation Loss: 1.1838072538375854\n",
      "Step 88/1000, Loss: 0.9377497434616089, Validation Loss: 1.182289958000183\n",
      "Step 89/1000, Loss: 0.9366546869277954, Validation Loss: 1.180667519569397\n",
      "Step 90/1000, Loss: 0.9354879260063171, Validation Loss: 1.178931713104248\n",
      "Step 91/1000, Loss: 0.9342446327209473, Validation Loss: 1.1770737171173096\n",
      "Step 92/1000, Loss: 0.9329196214675903, Validation Loss: 1.1750850677490234\n",
      "Step 93/1000, Loss: 0.931506872177124, Validation Loss: 1.172956943511963\n",
      "Step 94/1000, Loss: 0.9300000667572021, Validation Loss: 1.170680284500122\n",
      "Step 95/1000, Loss: 0.9283928275108337, Validation Loss: 1.1682463884353638\n",
      "Step 96/1000, Loss: 0.9266780614852905, Validation Loss: 1.165645956993103\n",
      "Step 97/1000, Loss: 0.9248483777046204, Validation Loss: 1.1628704071044922\n",
      "Step 98/1000, Loss: 0.9228960275650024, Validation Loss: 1.1599104404449463\n",
      "Step 99/1000, Loss: 0.9208124279975891, Validation Loss: 1.1567572355270386\n",
      "Step 100/1000, Loss: 0.9185892343521118, Validation Loss: 1.1534019708633423\n",
      "Step 101/1000, Loss: 0.9162172675132751, Validation Loss: 1.149835467338562\n",
      "Step 102/1000, Loss: 0.9136872291564941, Validation Loss: 1.1479629278182983\n",
      "Step 103/1000, Loss: 0.9123519659042358, Validation Loss: 1.1460212469100952\n",
      "Step 104/1000, Loss: 0.9109598398208618, Validation Loss: 1.1440114974975586\n",
      "Step 105/1000, Loss: 0.9095107913017273, Validation Loss: 1.1419342756271362\n",
      "Step 106/1000, Loss: 0.9080047607421875, Validation Loss: 1.1397897005081177\n",
      "Step 107/1000, Loss: 0.9064410924911499, Validation Loss: 1.1375783681869507\n",
      "Step 108/1000, Loss: 0.904819905757904, Validation Loss: 1.1353000402450562\n",
      "Step 109/1000, Loss: 0.9031403660774231, Validation Loss: 1.1329554319381714\n",
      "Step 110/1000, Loss: 0.9014021158218384, Validation Loss: 1.130543828010559\n",
      "Step 111/1000, Loss: 0.8996041417121887, Validation Loss: 1.128065586090088\n",
      "Step 112/1000, Loss: 0.8977463245391846, Validation Loss: 1.1255199909210205\n",
      "Step 113/1000, Loss: 0.8958274722099304, Validation Loss: 1.1229074001312256\n",
      "Step 114/1000, Loss: 0.8938472867012024, Validation Loss: 1.1202269792556763\n",
      "Step 115/1000, Loss: 0.8918049931526184, Validation Loss: 1.1174790859222412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 116/1000, Loss: 0.8897001147270203, Validation Loss: 1.114662528038025\n",
      "Step 117/1000, Loss: 0.8875318765640259, Validation Loss: 1.1117775440216064\n",
      "Step 118/1000, Loss: 0.8852999806404114, Validation Loss: 1.1088236570358276\n",
      "Step 119/1000, Loss: 0.8830040097236633, Validation Loss: 1.1058006286621094\n",
      "Step 120/1000, Loss: 0.8806434869766235, Validation Loss: 1.102708101272583\n",
      "Step 121/1000, Loss: 0.878217875957489, Validation Loss: 1.0995458364486694\n",
      "Step 122/1000, Loss: 0.8757271766662598, Validation Loss: 1.0963135957717896\n",
      "Step 123/1000, Loss: 0.8731709718704224, Validation Loss: 1.0930109024047852\n",
      "Step 124/1000, Loss: 0.8705492615699768, Validation Loss: 1.0896377563476562\n",
      "Step 125/1000, Loss: 0.8678617477416992, Validation Loss: 1.086193561553955\n",
      "Step 126/1000, Loss: 0.8651086091995239, Validation Loss: 1.0826783180236816\n",
      "Step 127/1000, Loss: 0.8622896671295166, Validation Loss: 1.079092025756836\n",
      "Step 128/1000, Loss: 0.8594049215316772, Validation Loss: 1.0754340887069702\n",
      "Step 129/1000, Loss: 0.856454610824585, Validation Loss: 1.0717042684555054\n",
      "Step 130/1000, Loss: 0.8534388542175293, Validation Loss: 1.0679025650024414\n",
      "Step 131/1000, Loss: 0.850357711315155, Validation Loss: 1.0640286207199097\n",
      "Step 132/1000, Loss: 0.847211480140686, Validation Loss: 1.0600817203521729\n",
      "Step 133/1000, Loss: 0.8440003395080566, Validation Loss: 1.0560617446899414\n",
      "Step 134/1000, Loss: 0.840724527835846, Validation Loss: 1.0519680976867676\n",
      "Step 135/1000, Loss: 0.8373843431472778, Validation Loss: 1.047800064086914\n",
      "Step 136/1000, Loss: 0.8339800834655762, Validation Loss: 1.0435569286346436\n",
      "Step 137/1000, Loss: 0.8305119872093201, Validation Loss: 1.0392374992370605\n",
      "Step 138/1000, Loss: 0.8269802927970886, Validation Loss: 1.0348412990570068\n",
      "Step 139/1000, Loss: 0.8233853578567505, Validation Loss: 1.030367136001587\n",
      "Step 140/1000, Loss: 0.8197274208068848, Validation Loss: 1.0258136987686157\n",
      "Step 141/1000, Loss: 0.8160068988800049, Validation Loss: 1.021180272102356\n",
      "Step 142/1000, Loss: 0.8122239112854004, Validation Loss: 1.0164657831192017\n",
      "Step 143/1000, Loss: 0.8083789944648743, Validation Loss: 1.011669397354126\n",
      "Step 144/1000, Loss: 0.8044723868370056, Validation Loss: 1.0067905187606812\n",
      "Step 145/1000, Loss: 0.8005044460296631, Validation Loss: 1.0018283128738403\n",
      "Step 146/1000, Loss: 0.7964754700660706, Validation Loss: 0.9967820048332214\n",
      "Step 147/1000, Loss: 0.7923859357833862, Validation Loss: 0.9916514158248901\n",
      "Step 148/1000, Loss: 0.7882363200187683, Validation Loss: 0.9864358901977539\n",
      "Step 149/1000, Loss: 0.7840270400047302, Validation Loss: 0.9811349511146545\n",
      "Step 150/1000, Loss: 0.7797586917877197, Validation Loss: 0.9757480025291443\n",
      "Step 151/1000, Loss: 0.775431752204895, Validation Loss: 0.970274806022644\n",
      "Step 152/1000, Loss: 0.7710465788841248, Validation Loss: 0.9647147059440613\n",
      "Step 153/1000, Loss: 0.7666038870811462, Validation Loss: 0.9590675830841064\n",
      "Step 154/1000, Loss: 0.762104332447052, Validation Loss: 0.9533331394195557\n",
      "Step 155/1000, Loss: 0.7575485110282898, Validation Loss: 0.9475111365318298\n",
      "Step 156/1000, Loss: 0.7529371380805969, Validation Loss: 0.9416015148162842\n",
      "Step 157/1000, Loss: 0.7482709288597107, Validation Loss: 0.9356046319007874\n",
      "Step 158/1000, Loss: 0.7435507774353027, Validation Loss: 0.9295211434364319\n",
      "Step 159/1000, Loss: 0.7387774586677551, Validation Loss: 0.9233517646789551\n",
      "Step 160/1000, Loss: 0.7339519262313843, Validation Loss: 0.9170975089073181\n",
      "Step 161/1000, Loss: 0.7290752530097961, Validation Loss: 0.9107598662376404\n",
      "Step 162/1000, Loss: 0.724148690700531, Validation Loss: 0.9043408632278442\n",
      "Step 163/1000, Loss: 0.7191734313964844, Validation Loss: 0.8978426456451416\n",
      "Step 164/1000, Loss: 0.7141505479812622, Validation Loss: 0.8912674784660339\n",
      "Step 165/1000, Loss: 0.709081768989563, Validation Loss: 0.8846184611320496\n",
      "Step 166/1000, Loss: 0.703968346118927, Validation Loss: 0.8778984546661377\n",
      "Step 167/1000, Loss: 0.6988120079040527, Validation Loss: 0.8711106181144714\n",
      "Step 168/1000, Loss: 0.6936143040657043, Validation Loss: 0.8642584085464478\n",
      "Step 169/1000, Loss: 0.6883769631385803, Validation Loss: 0.8573452234268188\n",
      "Step 170/1000, Loss: 0.6831017732620239, Validation Loss: 0.8503748178482056\n",
      "Step 171/1000, Loss: 0.6777908205986023, Validation Loss: 0.8433505892753601\n",
      "Step 172/1000, Loss: 0.6724456548690796, Validation Loss: 0.8362762331962585\n",
      "Step 173/1000, Loss: 0.6670684814453125, Validation Loss: 0.829155683517456\n",
      "Step 174/1000, Loss: 0.6616612672805786, Validation Loss: 0.8219925761222839\n",
      "Step 175/1000, Loss: 0.6562260389328003, Validation Loss: 0.8147906064987183\n",
      "Step 176/1000, Loss: 0.6507648825645447, Validation Loss: 0.807553768157959\n",
      "Step 177/1000, Loss: 0.6452797651290894, Validation Loss: 0.800286054611206\n",
      "Step 178/1000, Loss: 0.639772891998291, Validation Loss: 0.7929915189743042\n",
      "Step 179/1000, Loss: 0.6342464089393616, Validation Loss: 0.7856744527816772\n",
      "Step 180/1000, Loss: 0.6287024617195129, Validation Loss: 0.7783389091491699\n",
      "Step 181/1000, Loss: 0.6231430172920227, Validation Loss: 0.7709898352622986\n",
      "Step 182/1000, Loss: 0.6175702810287476, Validation Loss: 0.7636314630508423\n",
      "Step 183/1000, Loss: 0.6119864583015442, Validation Loss: 0.7562687397003174\n",
      "Step 184/1000, Loss: 0.6063938140869141, Validation Loss: 0.7489067316055298\n",
      "Step 185/1000, Loss: 0.6007943153381348, Validation Loss: 0.7415499091148376\n",
      "Step 186/1000, Loss: 0.5951902866363525, Validation Loss: 0.7342034578323364\n",
      "Step 187/1000, Loss: 0.5895838737487793, Validation Loss: 0.7268717885017395\n",
      "Step 188/1000, Loss: 0.583977222442627, Validation Loss: 0.7195595502853394\n",
      "Step 189/1000, Loss: 0.5783725380897522, Validation Loss: 0.7122711539268494\n",
      "Step 190/1000, Loss: 0.5727719664573669, Validation Loss: 0.7050105333328247\n",
      "Step 191/1000, Loss: 0.5671777725219727, Validation Loss: 0.6977812647819519\n",
      "Step 192/1000, Loss: 0.5615921020507812, Validation Loss: 0.6905864477157593\n",
      "Step 193/1000, Loss: 0.5560171008110046, Validation Loss: 0.6834291815757751\n",
      "Step 194/1000, Loss: 0.5504549741744995, Validation Loss: 0.6763112545013428\n",
      "Step 195/1000, Loss: 0.5449076294898987, Validation Loss: 0.6692349314689636\n",
      "Step 196/1000, Loss: 0.5393774509429932, Validation Loss: 0.6622014045715332\n",
      "Step 197/1000, Loss: 0.533866286277771, Validation Loss: 0.6552114486694336\n",
      "Step 198/1000, Loss: 0.5283761620521545, Validation Loss: 0.6482660174369812\n",
      "Step 199/1000, Loss: 0.5229093432426453, Validation Loss: 0.6413649916648865\n",
      "Step 200/1000, Loss: 0.5174674391746521, Validation Loss: 0.6345082521438599\n",
      "Step 201/1000, Loss: 0.5120524764060974, Validation Loss: 0.6276958584785461\n",
      "Step 202/1000, Loss: 0.5066663026809692, Validation Loss: 0.6243146061897278\n",
      "Step 203/1000, Loss: 0.5039883255958557, Validation Loss: 0.6209606528282166\n",
      "Step 204/1000, Loss: 0.5013258457183838, Validation Loss: 0.6176331639289856\n",
      "Step 205/1000, Loss: 0.4986792504787445, Validation Loss: 0.6143308281898499\n",
      "Step 206/1000, Loss: 0.49604859948158264, Validation Loss: 0.6110527515411377\n",
      "Step 207/1000, Loss: 0.49343425035476685, Validation Loss: 0.6077976226806641\n",
      "Step 208/1000, Loss: 0.49083614349365234, Validation Loss: 0.6045642495155334\n",
      "Step 209/1000, Loss: 0.48825448751449585, Validation Loss: 0.6013515591621399\n",
      "Step 210/1000, Loss: 0.4856894016265869, Validation Loss: 0.5981585383415222\n",
      "Step 211/1000, Loss: 0.4831409156322479, Validation Loss: 0.5949839949607849\n",
      "Step 212/1000, Loss: 0.4806089997291565, Validation Loss: 0.5918270945549011\n",
      "Step 213/1000, Loss: 0.4780937433242798, Validation Loss: 0.5886868834495544\n",
      "Step 214/1000, Loss: 0.4755951762199402, Validation Loss: 0.5855624079704285\n",
      "Step 215/1000, Loss: 0.47311314940452576, Validation Loss: 0.5824530124664307\n",
      "Step 216/1000, Loss: 0.47064775228500366, Validation Loss: 0.5793582797050476\n",
      "Step 217/1000, Loss: 0.4681989252567291, Validation Loss: 0.5762771964073181\n",
      "Step 218/1000, Loss: 0.465766578912735, Validation Loss: 0.5732096433639526\n",
      "Step 219/1000, Loss: 0.46335071325302124, Validation Loss: 0.5701552033424377\n",
      "Step 220/1000, Loss: 0.4609512686729431, Validation Loss: 0.5671135187149048\n",
      "Step 221/1000, Loss: 0.45856815576553345, Validation Loss: 0.564084529876709\n",
      "Step 222/1000, Loss: 0.4562011957168579, Validation Loss: 0.561068058013916\n",
      "Step 223/1000, Loss: 0.45385050773620605, Validation Loss: 0.5580640435218811\n",
      "Step 224/1000, Loss: 0.45151591300964355, Validation Loss: 0.5550726652145386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 225/1000, Loss: 0.4491972327232361, Validation Loss: 0.5520938634872437\n",
      "Step 226/1000, Loss: 0.4468945562839508, Validation Loss: 0.5491279363632202\n",
      "Step 227/1000, Loss: 0.444607675075531, Validation Loss: 0.5461750626564026\n",
      "Step 228/1000, Loss: 0.4423365592956543, Validation Loss: 0.5432354807853699\n",
      "Step 229/1000, Loss: 0.44008100032806396, Validation Loss: 0.5403094291687012\n",
      "Step 230/1000, Loss: 0.4378410875797272, Validation Loss: 0.5373971462249756\n",
      "Step 231/1000, Loss: 0.43561652302742004, Validation Loss: 0.5344990491867065\n",
      "Step 232/1000, Loss: 0.43340733647346497, Validation Loss: 0.5316153764724731\n",
      "Step 233/1000, Loss: 0.4312133491039276, Validation Loss: 0.528746485710144\n",
      "Step 234/1000, Loss: 0.4290345311164856, Validation Loss: 0.5258926153182983\n",
      "Step 235/1000, Loss: 0.42687076330184937, Validation Loss: 0.5230541229248047\n",
      "Step 236/1000, Loss: 0.4247218668460846, Validation Loss: 0.5202312469482422\n",
      "Step 237/1000, Loss: 0.4225878119468689, Validation Loss: 0.5174242258071899\n",
      "Step 238/1000, Loss: 0.4204684793949127, Validation Loss: 0.514633297920227\n",
      "Step 239/1000, Loss: 0.41836366057395935, Validation Loss: 0.5118587613105774\n",
      "Step 240/1000, Loss: 0.4162735044956207, Validation Loss: 0.5091007947921753\n",
      "Step 241/1000, Loss: 0.4141975939273834, Validation Loss: 0.5063595175743103\n",
      "Step 242/1000, Loss: 0.412136048078537, Validation Loss: 0.5036352276802063\n",
      "Step 243/1000, Loss: 0.4100886285305023, Validation Loss: 0.5009279847145081\n",
      "Step 244/1000, Loss: 0.4080553352832794, Validation Loss: 0.4982379674911499\n",
      "Step 245/1000, Loss: 0.4060359597206116, Validation Loss: 0.4955652356147766\n",
      "Step 246/1000, Loss: 0.4040304720401764, Validation Loss: 0.49290984869003296\n",
      "Step 247/1000, Loss: 0.40203872323036194, Validation Loss: 0.4902718961238861\n",
      "Step 248/1000, Loss: 0.4000606834888458, Validation Loss: 0.487651526927948\n",
      "Step 249/1000, Loss: 0.3980961740016937, Validation Loss: 0.4850486218929291\n",
      "Step 250/1000, Loss: 0.3961451053619385, Validation Loss: 0.48246335983276367\n",
      "Step 251/1000, Loss: 0.3942073881626129, Validation Loss: 0.47989559173583984\n",
      "Step 252/1000, Loss: 0.39228296279907227, Validation Loss: 0.47734543681144714\n",
      "Step 253/1000, Loss: 0.3903716206550598, Validation Loss: 0.47481289505958557\n",
      "Step 254/1000, Loss: 0.3884733021259308, Validation Loss: 0.4722978174686432\n",
      "Step 255/1000, Loss: 0.3865880072116852, Validation Loss: 0.46980026364326477\n",
      "Step 256/1000, Loss: 0.3847154378890991, Validation Loss: 0.46732017397880554\n",
      "Step 257/1000, Loss: 0.38285574316978455, Validation Loss: 0.46485745906829834\n",
      "Step 258/1000, Loss: 0.3810085952281952, Validation Loss: 0.4624120891094208\n",
      "Step 259/1000, Loss: 0.37917405366897583, Validation Loss: 0.45998409390449524\n",
      "Step 260/1000, Loss: 0.37735193967819214, Validation Loss: 0.45757320523262024\n",
      "Step 261/1000, Loss: 0.37554219365119934, Validation Loss: 0.4551796019077301\n",
      "Step 262/1000, Loss: 0.3737446367740631, Validation Loss: 0.45280301570892334\n",
      "Step 263/1000, Loss: 0.3719593584537506, Validation Loss: 0.45044344663619995\n",
      "Step 264/1000, Loss: 0.37018609046936035, Validation Loss: 0.4481008052825928\n",
      "Step 265/1000, Loss: 0.36842480301856995, Validation Loss: 0.4457750618457794\n",
      "Step 266/1000, Loss: 0.36667540669441223, Validation Loss: 0.44346609711647034\n",
      "Step 267/1000, Loss: 0.36493781208992004, Validation Loss: 0.4411737620830536\n",
      "Step 268/1000, Loss: 0.36321187019348145, Validation Loss: 0.4388982057571411\n",
      "Step 269/1000, Loss: 0.36149758100509644, Validation Loss: 0.4366391599178314\n",
      "Step 270/1000, Loss: 0.3597947955131531, Validation Loss: 0.4343966543674469\n",
      "Step 271/1000, Loss: 0.3581034243106842, Validation Loss: 0.43217065930366516\n",
      "Step 272/1000, Loss: 0.35642337799072266, Validation Loss: 0.4299609959125519\n",
      "Step 273/1000, Loss: 0.35475456714630127, Validation Loss: 0.42776769399642944\n",
      "Step 274/1000, Loss: 0.35309696197509766, Validation Loss: 0.42559075355529785\n",
      "Step 275/1000, Loss: 0.3514503836631775, Validation Loss: 0.42343005537986755\n",
      "Step 276/1000, Loss: 0.349814772605896, Validation Loss: 0.42128556966781616\n",
      "Step 277/1000, Loss: 0.3481900990009308, Validation Loss: 0.4191572368144989\n",
      "Step 278/1000, Loss: 0.34657615423202515, Validation Loss: 0.4170450270175934\n",
      "Step 279/1000, Loss: 0.3449729084968567, Validation Loss: 0.414948970079422\n",
      "Step 280/1000, Loss: 0.3433803617954254, Validation Loss: 0.4128689765930176\n",
      "Step 281/1000, Loss: 0.341798335313797, Validation Loss: 0.41080501675605774\n",
      "Step 282/1000, Loss: 0.3402267396450043, Validation Loss: 0.4087570905685425\n",
      "Step 283/1000, Loss: 0.33866554498672485, Validation Loss: 0.406725138425827\n",
      "Step 284/1000, Loss: 0.3371146321296692, Validation Loss: 0.4047090411186218\n",
      "Step 285/1000, Loss: 0.33557388186454773, Validation Loss: 0.40270882844924927\n",
      "Step 286/1000, Loss: 0.3340432345867157, Validation Loss: 0.40072450041770935\n",
      "Step 287/1000, Loss: 0.3325227200984955, Validation Loss: 0.39875587821006775\n",
      "Step 288/1000, Loss: 0.331012099981308, Validation Loss: 0.39680296182632446\n",
      "Step 289/1000, Loss: 0.3295113146305084, Validation Loss: 0.39486563205718994\n",
      "Step 290/1000, Loss: 0.3280203342437744, Validation Loss: 0.3929439187049866\n",
      "Step 291/1000, Loss: 0.3265390992164612, Validation Loss: 0.3910376727581024\n",
      "Step 292/1000, Loss: 0.3250674307346344, Validation Loss: 0.3891468346118927\n",
      "Step 293/1000, Loss: 0.32360532879829407, Validation Loss: 0.3872712552547455\n",
      "Step 294/1000, Loss: 0.32215267419815063, Validation Loss: 0.385410875082016\n",
      "Step 295/1000, Loss: 0.3207094371318817, Validation Loss: 0.38356560468673706\n",
      "Step 296/1000, Loss: 0.319275438785553, Validation Loss: 0.381735235452652\n",
      "Step 297/1000, Loss: 0.3178507387638092, Validation Loss: 0.37991979718208313\n",
      "Step 298/1000, Loss: 0.31643515825271606, Validation Loss: 0.37811917066574097\n",
      "Step 299/1000, Loss: 0.31502866744995117, Validation Loss: 0.3763331174850464\n",
      "Step 300/1000, Loss: 0.3136311173439026, Validation Loss: 0.37456151843070984\n",
      "Step 301/1000, Loss: 0.3122424781322479, Validation Loss: 0.37280431389808655\n",
      "Step 302/1000, Loss: 0.3108627200126648, Validation Loss: 0.3719324469566345\n",
      "Step 303/1000, Loss: 0.3101768493652344, Validation Loss: 0.3710665702819824\n",
      "Step 304/1000, Loss: 0.30949434638023376, Validation Loss: 0.37020644545555115\n",
      "Step 305/1000, Loss: 0.3088151216506958, Validation Loss: 0.3693520426750183\n",
      "Step 306/1000, Loss: 0.3081391751766205, Validation Loss: 0.36850300431251526\n",
      "Step 307/1000, Loss: 0.30746641755104065, Validation Loss: 0.367659330368042\n",
      "Step 308/1000, Loss: 0.306796669960022, Validation Loss: 0.3668206036090851\n",
      "Step 309/1000, Loss: 0.30612999200820923, Validation Loss: 0.3659868538379669\n",
      "Step 310/1000, Loss: 0.3054662048816681, Validation Loss: 0.3651578426361084\n",
      "Step 311/1000, Loss: 0.30480527877807617, Validation Loss: 0.36433354020118713\n",
      "Step 312/1000, Loss: 0.30414721369743347, Validation Loss: 0.36351364850997925\n",
      "Step 313/1000, Loss: 0.30349189043045044, Validation Loss: 0.36269816756248474\n",
      "Step 314/1000, Loss: 0.3028392791748047, Validation Loss: 0.3618869185447693\n",
      "Step 315/1000, Loss: 0.30218935012817383, Validation Loss: 0.3610798120498657\n",
      "Step 316/1000, Loss: 0.3015420138835907, Validation Loss: 0.3602766692638397\n",
      "Step 317/1000, Loss: 0.3008972704410553, Validation Loss: 0.35947751998901367\n",
      "Step 318/1000, Loss: 0.30025508999824524, Validation Loss: 0.35868215560913086\n",
      "Step 319/1000, Loss: 0.29961544275283813, Validation Loss: 0.35789060592651367\n",
      "Step 320/1000, Loss: 0.2989782691001892, Validation Loss: 0.357102632522583\n",
      "Step 321/1000, Loss: 0.29834359884262085, Validation Loss: 0.35631829500198364\n",
      "Step 322/1000, Loss: 0.29771125316619873, Validation Loss: 0.35553744435310364\n",
      "Step 323/1000, Loss: 0.2970813810825348, Validation Loss: 0.3547600209712982\n",
      "Step 324/1000, Loss: 0.2964538037776947, Validation Loss: 0.3539859354496002\n",
      "Step 325/1000, Loss: 0.29582861065864563, Validation Loss: 0.35321518778800964\n",
      "Step 326/1000, Loss: 0.2952057123184204, Validation Loss: 0.3524476885795593\n",
      "Step 327/1000, Loss: 0.29458513855934143, Validation Loss: 0.3516834080219269\n",
      "Step 328/1000, Loss: 0.29396679997444153, Validation Loss: 0.35092225670814514\n",
      "Step 329/1000, Loss: 0.2933506667613983, Validation Loss: 0.3501642942428589\n",
      "Step 330/1000, Loss: 0.29273682832717896, Validation Loss: 0.3494093716144562\n",
      "Step 331/1000, Loss: 0.2921251654624939, Validation Loss: 0.3486575484275818\n",
      "Step 332/1000, Loss: 0.29151567816734314, Validation Loss: 0.34790879487991333\n",
      "Step 333/1000, Loss: 0.2909083962440491, Validation Loss: 0.34716302156448364\n",
      "Step 334/1000, Loss: 0.29030323028564453, Validation Loss: 0.3464202880859375\n",
      "Step 335/1000, Loss: 0.2897002398967743, Validation Loss: 0.3456803858280182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 336/1000, Loss: 0.2890992760658264, Validation Loss: 0.3449435532093048\n",
      "Step 337/1000, Loss: 0.28850045800209045, Validation Loss: 0.34420961141586304\n",
      "Step 338/1000, Loss: 0.28790369629859924, Validation Loss: 0.3434787094593048\n",
      "Step 339/1000, Loss: 0.28730908036231995, Validation Loss: 0.3427506387233734\n",
      "Step 340/1000, Loss: 0.28671643137931824, Validation Loss: 0.3420255184173584\n",
      "Step 341/1000, Loss: 0.2861258387565613, Validation Loss: 0.3413033187389374\n",
      "Step 342/1000, Loss: 0.2855372726917267, Validation Loss: 0.3405839800834656\n",
      "Step 343/1000, Loss: 0.28495073318481445, Validation Loss: 0.33986756205558777\n",
      "Step 344/1000, Loss: 0.2843661606311798, Validation Loss: 0.3391540050506592\n",
      "Step 345/1000, Loss: 0.28378358483314514, Validation Loss: 0.3384433090686798\n",
      "Step 346/1000, Loss: 0.2832029461860657, Validation Loss: 0.33773547410964966\n",
      "Step 347/1000, Loss: 0.2826242744922638, Validation Loss: 0.33703047037124634\n",
      "Step 348/1000, Loss: 0.2820475697517395, Validation Loss: 0.336328387260437\n",
      "Step 349/1000, Loss: 0.28147274255752563, Validation Loss: 0.33562907576560974\n",
      "Step 350/1000, Loss: 0.28089988231658936, Validation Loss: 0.3349325954914093\n",
      "Step 351/1000, Loss: 0.2803289294242859, Validation Loss: 0.33423885703086853\n",
      "Step 352/1000, Loss: 0.27975982427597046, Validation Loss: 0.33354803919792175\n",
      "Step 353/1000, Loss: 0.27919262647628784, Validation Loss: 0.33285990357398987\n",
      "Step 354/1000, Loss: 0.27862727642059326, Validation Loss: 0.3321745693683624\n",
      "Step 355/1000, Loss: 0.27806374430656433, Validation Loss: 0.33149203658103943\n",
      "Step 356/1000, Loss: 0.2775021493434906, Validation Loss: 0.3308122456073761\n",
      "Step 357/1000, Loss: 0.27694234251976013, Validation Loss: 0.3301351070404053\n",
      "Step 358/1000, Loss: 0.27638429403305054, Validation Loss: 0.3294607102870941\n",
      "Step 359/1000, Loss: 0.27582812309265137, Validation Loss: 0.32878902554512024\n",
      "Step 360/1000, Loss: 0.27527371048927307, Validation Loss: 0.32812005281448364\n",
      "Step 361/1000, Loss: 0.2747211456298828, Validation Loss: 0.32745373249053955\n",
      "Step 362/1000, Loss: 0.2741703391075134, Validation Loss: 0.32679009437561035\n",
      "Step 363/1000, Loss: 0.2736212909221649, Validation Loss: 0.32612910866737366\n",
      "Step 364/1000, Loss: 0.2730740010738373, Validation Loss: 0.3254706561565399\n",
      "Step 365/1000, Loss: 0.27252843976020813, Validation Loss: 0.32481497526168823\n",
      "Step 366/1000, Loss: 0.27198466658592224, Validation Loss: 0.3241617679595947\n",
      "Step 367/1000, Loss: 0.27144259214401245, Validation Loss: 0.3235113024711609\n",
      "Step 368/1000, Loss: 0.27090224623680115, Validation Loss: 0.3228633403778076\n",
      "Step 369/1000, Loss: 0.27036359906196594, Validation Loss: 0.3222179412841797\n",
      "Step 370/1000, Loss: 0.2698266804218292, Validation Loss: 0.3215751349925995\n",
      "Step 371/1000, Loss: 0.26929140090942383, Validation Loss: 0.32093486189842224\n",
      "Step 372/1000, Loss: 0.2687578797340393, Validation Loss: 0.32029709219932556\n",
      "Step 373/1000, Loss: 0.2682259678840637, Validation Loss: 0.3196618854999542\n",
      "Step 374/1000, Loss: 0.26769569516181946, Validation Loss: 0.31902918219566345\n",
      "Step 375/1000, Loss: 0.2671671211719513, Validation Loss: 0.31839895248413086\n",
      "Step 376/1000, Loss: 0.26664021611213684, Validation Loss: 0.3177712559700012\n",
      "Step 377/1000, Loss: 0.2661149203777313, Validation Loss: 0.31714606285095215\n",
      "Step 378/1000, Loss: 0.26559123396873474, Validation Loss: 0.3165232539176941\n",
      "Step 379/1000, Loss: 0.2650691568851471, Validation Loss: 0.3159030079841614\n",
      "Step 380/1000, Loss: 0.2645486891269684, Validation Loss: 0.3152851462364197\n",
      "Step 381/1000, Loss: 0.264029860496521, Validation Loss: 0.3146698474884033\n",
      "Step 382/1000, Loss: 0.26351261138916016, Validation Loss: 0.3140569031238556\n",
      "Step 383/1000, Loss: 0.26299697160720825, Validation Loss: 0.3134463131427765\n",
      "Step 384/1000, Loss: 0.2624828517436981, Validation Loss: 0.31283819675445557\n",
      "Step 385/1000, Loss: 0.2619703412055969, Validation Loss: 0.31223249435424805\n",
      "Step 386/1000, Loss: 0.2614593803882599, Validation Loss: 0.31162911653518677\n",
      "Step 387/1000, Loss: 0.26094990968704224, Validation Loss: 0.31102821230888367\n",
      "Step 388/1000, Loss: 0.2604420781135559, Validation Loss: 0.31042957305908203\n",
      "Step 389/1000, Loss: 0.25993573665618896, Validation Loss: 0.3098333775997162\n",
      "Step 390/1000, Loss: 0.2594309151172638, Validation Loss: 0.30923953652381897\n",
      "Step 391/1000, Loss: 0.25892767310142517, Validation Loss: 0.308648020029068\n",
      "Step 392/1000, Loss: 0.25842586159706116, Validation Loss: 0.30805879831314087\n",
      "Step 393/1000, Loss: 0.2579255998134613, Validation Loss: 0.30747199058532715\n",
      "Step 394/1000, Loss: 0.25742682814598083, Validation Loss: 0.3068874478340149\n",
      "Step 395/1000, Loss: 0.25692957639694214, Validation Loss: 0.3063051998615265\n",
      "Step 396/1000, Loss: 0.25643375515937805, Validation Loss: 0.3057252764701843\n",
      "Step 397/1000, Loss: 0.25593942403793335, Validation Loss: 0.30514758825302124\n",
      "Step 398/1000, Loss: 0.25544658303260803, Validation Loss: 0.3045722544193268\n",
      "Step 399/1000, Loss: 0.2549552023410797, Validation Loss: 0.303999125957489\n",
      "Step 400/1000, Loss: 0.2544652819633484, Validation Loss: 0.3034282922744751\n",
      "Step 401/1000, Loss: 0.25397682189941406, Validation Loss: 0.30285966396331787\n",
      "Step 402/1000, Loss: 0.25348979234695435, Validation Loss: 0.30257636308670044\n",
      "Step 403/1000, Loss: 0.25324687361717224, Validation Loss: 0.30229395627975464\n",
      "Step 404/1000, Loss: 0.2530045807361603, Validation Loss: 0.30201229453086853\n",
      "Step 405/1000, Loss: 0.2527627646923065, Validation Loss: 0.30173152685165405\n",
      "Step 406/1000, Loss: 0.25252145528793335, Validation Loss: 0.30145150423049927\n",
      "Step 407/1000, Loss: 0.25228068232536316, Validation Loss: 0.30117231607437134\n",
      "Step 408/1000, Loss: 0.25204038619995117, Validation Loss: 0.3008938431739807\n",
      "Step 409/1000, Loss: 0.2518005967140198, Validation Loss: 0.30061614513397217\n",
      "Step 410/1000, Loss: 0.2515611946582794, Validation Loss: 0.3003391623497009\n",
      "Step 411/1000, Loss: 0.25132235884666443, Validation Loss: 0.3000629246234894\n",
      "Step 412/1000, Loss: 0.2510839104652405, Validation Loss: 0.29978734254837036\n",
      "Step 413/1000, Loss: 0.25084593892097473, Validation Loss: 0.29951247572898865\n",
      "Step 414/1000, Loss: 0.2506083548069, Validation Loss: 0.29923829436302185\n",
      "Step 415/1000, Loss: 0.2503712773323059, Validation Loss: 0.2989647686481476\n",
      "Step 416/1000, Loss: 0.25013458728790283, Validation Loss: 0.29869192838668823\n",
      "Step 417/1000, Loss: 0.24989831447601318, Validation Loss: 0.298419713973999\n",
      "Step 418/1000, Loss: 0.24966248869895935, Validation Loss: 0.29814809560775757\n",
      "Step 419/1000, Loss: 0.24942699074745178, Validation Loss: 0.2978772222995758\n",
      "Step 420/1000, Loss: 0.2491919845342636, Validation Loss: 0.2976069152355194\n",
      "Step 421/1000, Loss: 0.2489573210477829, Validation Loss: 0.29733720421791077\n",
      "Step 422/1000, Loss: 0.2487230747938156, Validation Loss: 0.29706814885139465\n",
      "Step 423/1000, Loss: 0.24848923087120056, Validation Loss: 0.2967996597290039\n",
      "Step 424/1000, Loss: 0.24825577437877655, Validation Loss: 0.2965317666530609\n",
      "Step 425/1000, Loss: 0.24802272021770477, Validation Loss: 0.29626449942588806\n",
      "Step 426/1000, Loss: 0.24779000878334045, Validation Loss: 0.2959977984428406\n",
      "Step 427/1000, Loss: 0.24755774438381195, Validation Loss: 0.29573163390159607\n",
      "Step 428/1000, Loss: 0.2473258078098297, Validation Loss: 0.29546600580215454\n",
      "Step 429/1000, Loss: 0.2470942586660385, Validation Loss: 0.29520100355148315\n",
      "Step 430/1000, Loss: 0.24686306715011597, Validation Loss: 0.29493656754493713\n",
      "Step 431/1000, Loss: 0.24663229286670685, Validation Loss: 0.2946726083755493\n",
      "Step 432/1000, Loss: 0.246401846408844, Validation Loss: 0.2944093346595764\n",
      "Step 433/1000, Loss: 0.24617180228233337, Validation Loss: 0.29414650797843933\n",
      "Step 434/1000, Loss: 0.24594208598136902, Validation Loss: 0.2938842475414276\n",
      "Step 435/1000, Loss: 0.2457127720117569, Validation Loss: 0.29362252354621887\n",
      "Step 436/1000, Loss: 0.24548380076885223, Validation Loss: 0.2933613061904907\n",
      "Step 437/1000, Loss: 0.24525520205497742, Validation Loss: 0.29310059547424316\n",
      "Step 438/1000, Loss: 0.24502694606781006, Validation Loss: 0.2928404211997986\n",
      "Step 439/1000, Loss: 0.24479906260967255, Validation Loss: 0.292580783367157\n",
      "Step 440/1000, Loss: 0.2445715218782425, Validation Loss: 0.29232171177864075\n",
      "Step 441/1000, Loss: 0.2443443238735199, Validation Loss: 0.2920631468296051\n",
      "Step 442/1000, Loss: 0.24411749839782715, Validation Loss: 0.2918050289154053\n",
      "Step 443/1000, Loss: 0.24389101564884186, Validation Loss: 0.2915474474430084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 444/1000, Loss: 0.24366487562656403, Validation Loss: 0.29129037261009216\n",
      "Step 445/1000, Loss: 0.24343906342983246, Validation Loss: 0.29103389382362366\n",
      "Step 446/1000, Loss: 0.24321365356445312, Validation Loss: 0.29077786207199097\n",
      "Step 447/1000, Loss: 0.24298857152462006, Validation Loss: 0.29052236676216125\n",
      "Step 448/1000, Loss: 0.24276380240917206, Validation Loss: 0.29026737809181213\n",
      "Step 449/1000, Loss: 0.2425394058227539, Validation Loss: 0.29001280665397644\n",
      "Step 450/1000, Loss: 0.2423153519630432, Validation Loss: 0.2897588610649109\n",
      "Step 451/1000, Loss: 0.24209162592887878, Validation Loss: 0.28950536251068115\n",
      "Step 452/1000, Loss: 0.24186822772026062, Validation Loss: 0.289252370595932\n",
      "Step 453/1000, Loss: 0.2416451871395111, Validation Loss: 0.28899988532066345\n",
      "Step 454/1000, Loss: 0.24142244458198547, Validation Loss: 0.2887479066848755\n",
      "Step 455/1000, Loss: 0.24120007455348969, Validation Loss: 0.2884964048862457\n",
      "Step 456/1000, Loss: 0.24097801744937897, Validation Loss: 0.28824540972709656\n",
      "Step 457/1000, Loss: 0.2407563328742981, Validation Loss: 0.2879948914051056\n",
      "Step 458/1000, Loss: 0.2405349165201187, Validation Loss: 0.2877449691295624\n",
      "Step 459/1000, Loss: 0.24031390249729156, Validation Loss: 0.287495493888855\n",
      "Step 460/1000, Loss: 0.2400931864976883, Validation Loss: 0.2872465252876282\n",
      "Step 461/1000, Loss: 0.23987281322479248, Validation Loss: 0.2869980037212372\n",
      "Step 462/1000, Loss: 0.23965273797512054, Validation Loss: 0.286749929189682\n",
      "Step 463/1000, Loss: 0.23943297564983368, Validation Loss: 0.286502480506897\n",
      "Step 464/1000, Loss: 0.23921363055706024, Validation Loss: 0.28625547885894775\n",
      "Step 465/1000, Loss: 0.2389945387840271, Validation Loss: 0.2860089838504791\n",
      "Step 466/1000, Loss: 0.23877577483654022, Validation Loss: 0.2857629358768463\n",
      "Step 467/1000, Loss: 0.23855732381343842, Validation Loss: 0.2855173945426941\n",
      "Step 468/1000, Loss: 0.23833921551704407, Validation Loss: 0.2852723300457001\n",
      "Step 469/1000, Loss: 0.23812143504619598, Validation Loss: 0.28502777218818665\n",
      "Step 470/1000, Loss: 0.23790395259857178, Validation Loss: 0.28478366136550903\n",
      "Step 471/1000, Loss: 0.23768678307533264, Validation Loss: 0.2845400869846344\n",
      "Step 472/1000, Loss: 0.23746994137763977, Validation Loss: 0.2842969596385956\n",
      "Step 473/1000, Loss: 0.23725344240665436, Validation Loss: 0.28405433893203735\n",
      "Step 474/1000, Loss: 0.23703722655773163, Validation Loss: 0.28381216526031494\n",
      "Step 475/1000, Loss: 0.23682135343551636, Validation Loss: 0.2835704982280731\n",
      "Step 476/1000, Loss: 0.23660574853420258, Validation Loss: 0.2833292484283447\n",
      "Step 477/1000, Loss: 0.23639045655727386, Validation Loss: 0.2830885350704193\n",
      "Step 478/1000, Loss: 0.2361755222082138, Validation Loss: 0.2828482687473297\n",
      "Step 479/1000, Loss: 0.23596088588237762, Validation Loss: 0.2826085686683655\n",
      "Step 480/1000, Loss: 0.2357465773820877, Validation Loss: 0.2823692262172699\n",
      "Step 481/1000, Loss: 0.23553256690502167, Validation Loss: 0.2821303904056549\n",
      "Step 482/1000, Loss: 0.2353188395500183, Validation Loss: 0.2818920314311981\n",
      "Step 483/1000, Loss: 0.23510541021823883, Validation Loss: 0.2816541790962219\n",
      "Step 484/1000, Loss: 0.2348923236131668, Validation Loss: 0.28141671419143677\n",
      "Step 485/1000, Loss: 0.23467952013015747, Validation Loss: 0.281179815530777\n",
      "Step 486/1000, Loss: 0.2344670593738556, Validation Loss: 0.2809433043003082\n",
      "Step 487/1000, Loss: 0.234254851937294, Validation Loss: 0.2807072699069977\n",
      "Step 488/1000, Loss: 0.23404298722743988, Validation Loss: 0.28047171235084534\n",
      "Step 489/1000, Loss: 0.23383143544197083, Validation Loss: 0.2802366316318512\n",
      "Step 490/1000, Loss: 0.23362012207508087, Validation Loss: 0.28000202775001526\n",
      "Step 491/1000, Loss: 0.23340918123722076, Validation Loss: 0.27976787090301514\n",
      "Step 492/1000, Loss: 0.23319852352142334, Validation Loss: 0.27953416109085083\n",
      "Step 493/1000, Loss: 0.2329881340265274, Validation Loss: 0.2793009281158447\n",
      "Step 494/1000, Loss: 0.23277810215950012, Validation Loss: 0.27906814217567444\n",
      "Step 495/1000, Loss: 0.23256827890872955, Validation Loss: 0.2788357436656952\n",
      "Step 496/1000, Loss: 0.23235882818698883, Validation Loss: 0.2786038815975189\n",
      "Step 497/1000, Loss: 0.2321496605873108, Validation Loss: 0.2783724069595337\n",
      "Step 498/1000, Loss: 0.23194073140621185, Validation Loss: 0.27814143896102905\n",
      "Step 499/1000, Loss: 0.23173214495182037, Validation Loss: 0.27791088819503784\n",
      "Step 500/1000, Loss: 0.23152385652065277, Validation Loss: 0.27768081426620483\n",
      "Step 501/1000, Loss: 0.23131583631038666, Validation Loss: 0.27745121717453003\n",
      "Step 502/1000, Loss: 0.2311081439256668, Validation Loss: 0.27733656764030457\n",
      "Step 503/1000, Loss: 0.23100441694259644, Validation Loss: 0.2772221267223358\n",
      "Step 504/1000, Loss: 0.23090077936649323, Validation Loss: 0.2771078050136566\n",
      "Step 505/1000, Loss: 0.23079730570316315, Validation Loss: 0.27699360251426697\n",
      "Step 506/1000, Loss: 0.23069387674331665, Validation Loss: 0.27687960863113403\n",
      "Step 507/1000, Loss: 0.2305905520915985, Validation Loss: 0.27676576375961304\n",
      "Step 508/1000, Loss: 0.23048731684684753, Validation Loss: 0.2766520380973816\n",
      "Step 509/1000, Loss: 0.2303842157125473, Validation Loss: 0.2765384316444397\n",
      "Step 510/1000, Loss: 0.23028120398521423, Validation Loss: 0.27642500400543213\n",
      "Step 511/1000, Loss: 0.23017825186252594, Validation Loss: 0.2763116955757141\n",
      "Step 512/1000, Loss: 0.2300753891468048, Validation Loss: 0.27619850635528564\n",
      "Step 513/1000, Loss: 0.22997263073921204, Validation Loss: 0.2760855555534363\n",
      "Step 514/1000, Loss: 0.2298699915409088, Validation Loss: 0.2759726941585541\n",
      "Step 515/1000, Loss: 0.22976742684841156, Validation Loss: 0.2758599519729614\n",
      "Step 516/1000, Loss: 0.2296649068593979, Validation Loss: 0.27574729919433594\n",
      "Step 517/1000, Loss: 0.22956249117851257, Validation Loss: 0.2756347954273224\n",
      "Step 518/1000, Loss: 0.22946013510227203, Validation Loss: 0.27552247047424316\n",
      "Step 519/1000, Loss: 0.22935788333415985, Validation Loss: 0.2754102945327759\n",
      "Step 520/1000, Loss: 0.22925572097301483, Validation Loss: 0.275298148393631\n",
      "Step 521/1000, Loss: 0.2291536182165146, Validation Loss: 0.2751862406730652\n",
      "Step 522/1000, Loss: 0.2290516197681427, Validation Loss: 0.2750743627548218\n",
      "Step 523/1000, Loss: 0.22894969582557678, Validation Loss: 0.2749626636505127\n",
      "Step 524/1000, Loss: 0.22884783148765564, Validation Loss: 0.2748510539531708\n",
      "Step 525/1000, Loss: 0.22874604165554047, Validation Loss: 0.2747395932674408\n",
      "Step 526/1000, Loss: 0.22864437103271484, Validation Loss: 0.27462825179100037\n",
      "Step 527/1000, Loss: 0.2285427302122116, Validation Loss: 0.2745169997215271\n",
      "Step 528/1000, Loss: 0.22844119369983673, Validation Loss: 0.2744058668613434\n",
      "Step 529/1000, Loss: 0.22833971679210663, Validation Loss: 0.2742948532104492\n",
      "Step 530/1000, Loss: 0.22823837399482727, Validation Loss: 0.2741839587688446\n",
      "Step 531/1000, Loss: 0.2281370311975479, Validation Loss: 0.27407318353652954\n",
      "Step 532/1000, Loss: 0.22803577780723572, Validation Loss: 0.27396252751350403\n",
      "Step 533/1000, Loss: 0.2279345840215683, Validation Loss: 0.27385199069976807\n",
      "Step 534/1000, Loss: 0.22783350944519043, Validation Loss: 0.27374154329299927\n",
      "Step 535/1000, Loss: 0.22773247957229614, Validation Loss: 0.27363118529319763\n",
      "Step 536/1000, Loss: 0.22763150930404663, Validation Loss: 0.27352094650268555\n",
      "Step 537/1000, Loss: 0.22753068804740906, Validation Loss: 0.2734109163284302\n",
      "Step 538/1000, Loss: 0.22742986679077148, Validation Loss: 0.2733009159564972\n",
      "Step 539/1000, Loss: 0.2273291051387787, Validation Loss: 0.27319106459617615\n",
      "Step 540/1000, Loss: 0.22722849249839783, Validation Loss: 0.27308133244514465\n",
      "Step 541/1000, Loss: 0.22712790966033936, Validation Loss: 0.2729716897010803\n",
      "Step 542/1000, Loss: 0.22702738642692566, Validation Loss: 0.2728620767593384\n",
      "Step 543/1000, Loss: 0.22692695260047913, Validation Loss: 0.272752583026886\n",
      "Step 544/1000, Loss: 0.22682657837867737, Validation Loss: 0.27264323830604553\n",
      "Step 545/1000, Loss: 0.22672627866268158, Validation Loss: 0.27253398299217224\n",
      "Step 546/1000, Loss: 0.22662605345249176, Validation Loss: 0.2724248766899109\n",
      "Step 547/1000, Loss: 0.22652588784694672, Validation Loss: 0.2723158299922943\n",
      "Step 548/1000, Loss: 0.22642585635185242, Validation Loss: 0.27220696210861206\n",
      "Step 549/1000, Loss: 0.22632580995559692, Validation Loss: 0.2720981538295746\n",
      "Step 550/1000, Loss: 0.22622588276863098, Validation Loss: 0.2719894349575043\n",
      "Step 551/1000, Loss: 0.22612600028514862, Validation Loss: 0.2718808054924011\n",
      "Step 552/1000, Loss: 0.22602619230747223, Validation Loss: 0.27177220582962036\n",
      "Step 553/1000, Loss: 0.225926473736763, Validation Loss: 0.2716638147830963\n",
      "Step 554/1000, Loss: 0.22582677006721497, Validation Loss: 0.27155551314353943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 555/1000, Loss: 0.22572720050811768, Validation Loss: 0.2714473009109497\n",
      "Step 556/1000, Loss: 0.22562766075134277, Validation Loss: 0.2713392376899719\n",
      "Step 557/1000, Loss: 0.22552824020385742, Validation Loss: 0.27123120427131653\n",
      "Step 558/1000, Loss: 0.22542884945869446, Validation Loss: 0.27112334966659546\n",
      "Step 559/1000, Loss: 0.22532951831817627, Validation Loss: 0.2710154950618744\n",
      "Step 560/1000, Loss: 0.22523029148578644, Validation Loss: 0.27090781927108765\n",
      "Step 561/1000, Loss: 0.22513112425804138, Validation Loss: 0.27080029249191284\n",
      "Step 562/1000, Loss: 0.2250320315361023, Validation Loss: 0.27069273591041565\n",
      "Step 563/1000, Loss: 0.2249329835176468, Validation Loss: 0.2705853581428528\n",
      "Step 564/1000, Loss: 0.22483401000499725, Validation Loss: 0.27047809958457947\n",
      "Step 565/1000, Loss: 0.22473512589931488, Validation Loss: 0.27037087082862854\n",
      "Step 566/1000, Loss: 0.2246362864971161, Validation Loss: 0.27026376128196716\n",
      "Step 567/1000, Loss: 0.22453752160072327, Validation Loss: 0.27015677094459534\n",
      "Step 568/1000, Loss: 0.2244388312101364, Validation Loss: 0.27004989981651306\n",
      "Step 569/1000, Loss: 0.22434023022651672, Validation Loss: 0.26994314789772034\n",
      "Step 570/1000, Loss: 0.22424162924289703, Validation Loss: 0.2698364555835724\n",
      "Step 571/1000, Loss: 0.2241431623697281, Validation Loss: 0.2697299122810364\n",
      "Step 572/1000, Loss: 0.22404474020004272, Validation Loss: 0.26962336897850037\n",
      "Step 573/1000, Loss: 0.22394637763500214, Validation Loss: 0.2695170044898987\n",
      "Step 574/1000, Loss: 0.22384805977344513, Validation Loss: 0.2694106996059418\n",
      "Step 575/1000, Loss: 0.22374987602233887, Validation Loss: 0.2693045437335968\n",
      "Step 576/1000, Loss: 0.2236517369747162, Validation Loss: 0.2691984176635742\n",
      "Step 577/1000, Loss: 0.2235536277294159, Validation Loss: 0.2690924108028412\n",
      "Step 578/1000, Loss: 0.22345562279224396, Validation Loss: 0.26898661255836487\n",
      "Step 579/1000, Loss: 0.2233576625585556, Validation Loss: 0.26888078451156616\n",
      "Step 580/1000, Loss: 0.22325973212718964, Validation Loss: 0.2687750458717346\n",
      "Step 581/1000, Loss: 0.2231619656085968, Validation Loss: 0.2686694860458374\n",
      "Step 582/1000, Loss: 0.22306418418884277, Validation Loss: 0.26856398582458496\n",
      "Step 583/1000, Loss: 0.2229665219783783, Validation Loss: 0.2684585452079773\n",
      "Step 584/1000, Loss: 0.2228688895702362, Validation Loss: 0.26835325360298157\n",
      "Step 585/1000, Loss: 0.2227713167667389, Validation Loss: 0.2682480812072754\n",
      "Step 586/1000, Loss: 0.22267387807369232, Validation Loss: 0.268142968416214\n",
      "Step 587/1000, Loss: 0.22257640957832336, Validation Loss: 0.2680380344390869\n",
      "Step 588/1000, Loss: 0.22247906029224396, Validation Loss: 0.26793310046195984\n",
      "Step 589/1000, Loss: 0.22238177061080933, Validation Loss: 0.2678282558917999\n",
      "Step 590/1000, Loss: 0.22228452563285828, Validation Loss: 0.26772356033325195\n",
      "Step 591/1000, Loss: 0.22218739986419678, Validation Loss: 0.26761892437934875\n",
      "Step 592/1000, Loss: 0.22209030389785767, Validation Loss: 0.2675143778324127\n",
      "Step 593/1000, Loss: 0.22199323773384094, Validation Loss: 0.26740995049476624\n",
      "Step 594/1000, Loss: 0.22189629077911377, Validation Loss: 0.2673056423664093\n",
      "Step 595/1000, Loss: 0.22179938852787018, Validation Loss: 0.26720142364501953\n",
      "Step 596/1000, Loss: 0.22170253098011017, Validation Loss: 0.2670973241329193\n",
      "Step 597/1000, Loss: 0.22160577774047852, Validation Loss: 0.2669932544231415\n",
      "Step 598/1000, Loss: 0.22150908410549164, Validation Loss: 0.2668893039226532\n",
      "Step 599/1000, Loss: 0.22141247987747192, Validation Loss: 0.2667854428291321\n",
      "Step 600/1000, Loss: 0.2213158756494522, Validation Loss: 0.2666817009449005\n",
      "Step 601/1000, Loss: 0.22121936082839966, Validation Loss: 0.2665780484676361\n",
      "Step 602/1000, Loss: 0.22112290561199188, Validation Loss: 0.2665262818336487\n",
      "Step 603/1000, Loss: 0.22107470035552979, Validation Loss: 0.26647451519966125\n",
      "Step 604/1000, Loss: 0.22102653980255127, Validation Loss: 0.26642274856567383\n",
      "Step 605/1000, Loss: 0.22097840905189514, Validation Loss: 0.26637110114097595\n",
      "Step 606/1000, Loss: 0.220930278301239, Validation Loss: 0.2663194239139557\n",
      "Step 607/1000, Loss: 0.22088216245174408, Validation Loss: 0.2662677764892578\n",
      "Step 608/1000, Loss: 0.22083407640457153, Validation Loss: 0.2662162482738495\n",
      "Step 609/1000, Loss: 0.22078603506088257, Validation Loss: 0.2661646604537964\n",
      "Step 610/1000, Loss: 0.22073794901371002, Validation Loss: 0.26611313223838806\n",
      "Step 611/1000, Loss: 0.22068993747234344, Validation Loss: 0.2660616338253021\n",
      "Step 612/1000, Loss: 0.22064195573329926, Validation Loss: 0.2660101354122162\n",
      "Step 613/1000, Loss: 0.22059392929077148, Validation Loss: 0.2659587562084198\n",
      "Step 614/1000, Loss: 0.22054600715637207, Validation Loss: 0.26590731739997864\n",
      "Step 615/1000, Loss: 0.22049804031848907, Validation Loss: 0.26585590839385986\n",
      "Step 616/1000, Loss: 0.22045008838176727, Validation Loss: 0.2658045291900635\n",
      "Step 617/1000, Loss: 0.22040215134620667, Validation Loss: 0.26575326919555664\n",
      "Step 618/1000, Loss: 0.22035427391529083, Validation Loss: 0.26570194959640503\n",
      "Step 619/1000, Loss: 0.220306396484375, Validation Loss: 0.2656506597995758\n",
      "Step 620/1000, Loss: 0.22025850415229797, Validation Loss: 0.2655993103981018\n",
      "Step 621/1000, Loss: 0.22021065652370453, Validation Loss: 0.26554811000823975\n",
      "Step 622/1000, Loss: 0.22016280889511108, Validation Loss: 0.2654969096183777\n",
      "Step 623/1000, Loss: 0.22011499106884003, Validation Loss: 0.2654457688331604\n",
      "Step 624/1000, Loss: 0.22006720304489136, Validation Loss: 0.2653946280479431\n",
      "Step 625/1000, Loss: 0.22001942992210388, Validation Loss: 0.26534348726272583\n",
      "Step 626/1000, Loss: 0.21997162699699402, Validation Loss: 0.26529234647750854\n",
      "Step 627/1000, Loss: 0.21992391347885132, Validation Loss: 0.26524123549461365\n",
      "Step 628/1000, Loss: 0.21987617015838623, Validation Loss: 0.2651902139186859\n",
      "Step 629/1000, Loss: 0.21982845664024353, Validation Loss: 0.2651391923427582\n",
      "Step 630/1000, Loss: 0.21978075802326202, Validation Loss: 0.26508820056915283\n",
      "Step 631/1000, Loss: 0.21973305940628052, Validation Loss: 0.2650371789932251\n",
      "Step 632/1000, Loss: 0.2196853756904602, Validation Loss: 0.2649862468242645\n",
      "Step 633/1000, Loss: 0.21963773667812347, Validation Loss: 0.26493531465530396\n",
      "Step 634/1000, Loss: 0.21959011256694794, Validation Loss: 0.26488444209098816\n",
      "Step 635/1000, Loss: 0.2195424884557724, Validation Loss: 0.26483359932899475\n",
      "Step 636/1000, Loss: 0.21949484944343567, Validation Loss: 0.26478275656700134\n",
      "Step 637/1000, Loss: 0.2194472700357437, Validation Loss: 0.26473191380500793\n",
      "Step 638/1000, Loss: 0.21939970552921295, Validation Loss: 0.2646810710430145\n",
      "Step 639/1000, Loss: 0.21935215592384338, Validation Loss: 0.2646302878856659\n",
      "Step 640/1000, Loss: 0.21930459141731262, Validation Loss: 0.26457953453063965\n",
      "Step 641/1000, Loss: 0.21925702691078186, Validation Loss: 0.2645287811756134\n",
      "Step 642/1000, Loss: 0.21920952200889587, Validation Loss: 0.26447805762290955\n",
      "Step 643/1000, Loss: 0.21916204690933228, Validation Loss: 0.2644273340702057\n",
      "Step 644/1000, Loss: 0.21911455690860748, Validation Loss: 0.264376699924469\n",
      "Step 645/1000, Loss: 0.21906708180904388, Validation Loss: 0.2643260359764099\n",
      "Step 646/1000, Loss: 0.2190195918083191, Validation Loss: 0.264275461435318\n",
      "Step 647/1000, Loss: 0.21897216141223907, Validation Loss: 0.2642247676849365\n",
      "Step 648/1000, Loss: 0.21892468631267548, Validation Loss: 0.2641741931438446\n",
      "Step 649/1000, Loss: 0.21887728571891785, Validation Loss: 0.2641236484050751\n",
      "Step 650/1000, Loss: 0.21882988512516022, Validation Loss: 0.26407313346862793\n",
      "Step 651/1000, Loss: 0.21878249943256378, Validation Loss: 0.2640226483345032\n",
      "Step 652/1000, Loss: 0.21873514354228973, Validation Loss: 0.2639721632003784\n",
      "Step 653/1000, Loss: 0.21868778765201569, Validation Loss: 0.2639216184616089\n",
      "Step 654/1000, Loss: 0.21864041686058044, Validation Loss: 0.2638711929321289\n",
      "Step 655/1000, Loss: 0.21859309077262878, Validation Loss: 0.2638207674026489\n",
      "Step 656/1000, Loss: 0.2185457944869995, Validation Loss: 0.26377037167549133\n",
      "Step 657/1000, Loss: 0.21849851310253143, Validation Loss: 0.26371994614601135\n",
      "Step 658/1000, Loss: 0.21845124661922455, Validation Loss: 0.26366958022117615\n",
      "Step 659/1000, Loss: 0.21840399503707886, Validation Loss: 0.2636192739009857\n",
      "Step 660/1000, Loss: 0.21835672855377197, Validation Loss: 0.2635689675807953\n",
      "Step 661/1000, Loss: 0.21830949187278748, Validation Loss: 0.26351866126060486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 662/1000, Loss: 0.21826228499412537, Validation Loss: 0.2634683847427368\n",
      "Step 663/1000, Loss: 0.21821506321430206, Validation Loss: 0.26341813802719116\n",
      "Step 664/1000, Loss: 0.21816790103912354, Validation Loss: 0.2633679509162903\n",
      "Step 665/1000, Loss: 0.2181207686662674, Validation Loss: 0.263317734003067\n",
      "Step 666/1000, Loss: 0.21807359158992767, Validation Loss: 0.2632676362991333\n",
      "Step 667/1000, Loss: 0.21802644431591034, Validation Loss: 0.2632174789905548\n",
      "Step 668/1000, Loss: 0.2179793119430542, Validation Loss: 0.2631673514842987\n",
      "Step 669/1000, Loss: 0.21793219447135925, Validation Loss: 0.2631171643733978\n",
      "Step 670/1000, Loss: 0.2178850620985031, Validation Loss: 0.2630670666694641\n",
      "Step 671/1000, Loss: 0.21783795952796936, Validation Loss: 0.2630169689655304\n",
      "Step 672/1000, Loss: 0.2177909016609192, Validation Loss: 0.2629668712615967\n",
      "Step 673/1000, Loss: 0.21774382889270782, Validation Loss: 0.26291680335998535\n",
      "Step 674/1000, Loss: 0.21769681572914124, Validation Loss: 0.262866735458374\n",
      "Step 675/1000, Loss: 0.21764975786209106, Validation Loss: 0.26281675696372986\n",
      "Step 676/1000, Loss: 0.21760275959968567, Validation Loss: 0.2627667784690857\n",
      "Step 677/1000, Loss: 0.21755576133728027, Validation Loss: 0.26271679997444153\n",
      "Step 678/1000, Loss: 0.21750874817371368, Validation Loss: 0.26266688108444214\n",
      "Step 679/1000, Loss: 0.21746176481246948, Validation Loss: 0.26261696219444275\n",
      "Step 680/1000, Loss: 0.21741478145122528, Validation Loss: 0.26256707310676575\n",
      "Step 681/1000, Loss: 0.21736784279346466, Validation Loss: 0.26251718401908875\n",
      "Step 682/1000, Loss: 0.21732090413570404, Validation Loss: 0.2624673545360565\n",
      "Step 683/1000, Loss: 0.2172739952802658, Validation Loss: 0.2624174654483795\n",
      "Step 684/1000, Loss: 0.21722707152366638, Validation Loss: 0.2623676657676697\n",
      "Step 685/1000, Loss: 0.21718016266822815, Validation Loss: 0.26231783628463745\n",
      "Step 686/1000, Loss: 0.2171332687139511, Validation Loss: 0.26226806640625\n",
      "Step 687/1000, Loss: 0.21708638966083527, Validation Loss: 0.26221829652786255\n",
      "Step 688/1000, Loss: 0.2170395404100418, Validation Loss: 0.2621685266494751\n",
      "Step 689/1000, Loss: 0.21699273586273193, Validation Loss: 0.2621188461780548\n",
      "Step 690/1000, Loss: 0.21694590151309967, Validation Loss: 0.2620691657066345\n",
      "Step 691/1000, Loss: 0.21689915657043457, Validation Loss: 0.2620195150375366\n",
      "Step 692/1000, Loss: 0.2168523520231247, Validation Loss: 0.2619698643684387\n",
      "Step 693/1000, Loss: 0.216805562376976, Validation Loss: 0.2619202733039856\n",
      "Step 694/1000, Loss: 0.2167588174343109, Validation Loss: 0.2618706226348877\n",
      "Step 695/1000, Loss: 0.21671204268932343, Validation Loss: 0.26182109117507935\n",
      "Step 696/1000, Loss: 0.2166653424501419, Validation Loss: 0.2617715895175934\n",
      "Step 697/1000, Loss: 0.2166185826063156, Validation Loss: 0.26172202825546265\n",
      "Step 698/1000, Loss: 0.21657192707061768, Validation Loss: 0.26167258620262146\n",
      "Step 699/1000, Loss: 0.21652524173259735, Validation Loss: 0.2616230845451355\n",
      "Step 700/1000, Loss: 0.21647855639457703, Validation Loss: 0.2615736126899719\n",
      "Step 701/1000, Loss: 0.21643194556236267, Validation Loss: 0.26152411103248596\n",
      "Step 702/1000, Loss: 0.21638526022434235, Validation Loss: 0.26149943470954895\n",
      "Step 703/1000, Loss: 0.21636195480823517, Validation Loss: 0.26147472858428955\n",
      "Step 704/1000, Loss: 0.2163386195898056, Validation Loss: 0.26145005226135254\n",
      "Step 705/1000, Loss: 0.21631531417369843, Validation Loss: 0.2614253759384155\n",
      "Step 706/1000, Loss: 0.21629203855991364, Validation Loss: 0.26140066981315613\n",
      "Step 707/1000, Loss: 0.21626873314380646, Validation Loss: 0.26137596368789673\n",
      "Step 708/1000, Loss: 0.21624542772769928, Validation Loss: 0.26135125756263733\n",
      "Step 709/1000, Loss: 0.2162221223115921, Validation Loss: 0.2613266110420227\n",
      "Step 710/1000, Loss: 0.2161988615989685, Validation Loss: 0.2613019347190857\n",
      "Step 711/1000, Loss: 0.21617552638053894, Validation Loss: 0.2612772583961487\n",
      "Step 712/1000, Loss: 0.21615226566791534, Validation Loss: 0.26125261187553406\n",
      "Step 713/1000, Loss: 0.21612900495529175, Validation Loss: 0.26122793555259705\n",
      "Step 714/1000, Loss: 0.21610571444034576, Validation Loss: 0.2612032890319824\n",
      "Step 715/1000, Loss: 0.21608248353004456, Validation Loss: 0.2611786425113678\n",
      "Step 716/1000, Loss: 0.21605919301509857, Validation Loss: 0.26115402579307556\n",
      "Step 717/1000, Loss: 0.21603594720363617, Validation Loss: 0.2611294090747833\n",
      "Step 718/1000, Loss: 0.21601268649101257, Validation Loss: 0.2611047923564911\n",
      "Step 719/1000, Loss: 0.21598942577838898, Validation Loss: 0.26108020544052124\n",
      "Step 720/1000, Loss: 0.21596617996692657, Validation Loss: 0.2610556483268738\n",
      "Step 721/1000, Loss: 0.21594294905662537, Validation Loss: 0.26103103160858154\n",
      "Step 722/1000, Loss: 0.21591970324516296, Validation Loss: 0.2610063850879669\n",
      "Step 723/1000, Loss: 0.21589645743370056, Validation Loss: 0.26098179817199707\n",
      "Step 724/1000, Loss: 0.21587324142456055, Validation Loss: 0.2609572112560272\n",
      "Step 725/1000, Loss: 0.21585004031658173, Validation Loss: 0.2609326243400574\n",
      "Step 726/1000, Loss: 0.21582680940628052, Validation Loss: 0.26090800762176514\n",
      "Step 727/1000, Loss: 0.2158035784959793, Validation Loss: 0.26088348031044006\n",
      "Step 728/1000, Loss: 0.21578039228916168, Validation Loss: 0.2608588635921478\n",
      "Step 729/1000, Loss: 0.21575716137886047, Validation Loss: 0.260834276676178\n",
      "Step 730/1000, Loss: 0.21573393046855927, Validation Loss: 0.2608097493648529\n",
      "Step 731/1000, Loss: 0.21571074426174164, Validation Loss: 0.26078516244888306\n",
      "Step 732/1000, Loss: 0.215687558054924, Validation Loss: 0.260760635137558\n",
      "Step 733/1000, Loss: 0.2156643271446228, Validation Loss: 0.26073604822158813\n",
      "Step 734/1000, Loss: 0.2156411111354828, Validation Loss: 0.26071152091026306\n",
      "Step 735/1000, Loss: 0.21561791002750397, Validation Loss: 0.2606870234012604\n",
      "Step 736/1000, Loss: 0.21559470891952515, Validation Loss: 0.2606625258922577\n",
      "Step 737/1000, Loss: 0.21557149291038513, Validation Loss: 0.2606379985809326\n",
      "Step 738/1000, Loss: 0.2155483067035675, Validation Loss: 0.26061350107192993\n",
      "Step 739/1000, Loss: 0.21552512049674988, Validation Loss: 0.26058894395828247\n",
      "Step 740/1000, Loss: 0.21550188958644867, Validation Loss: 0.2605644166469574\n",
      "Step 741/1000, Loss: 0.21547871828079224, Validation Loss: 0.2605398893356323\n",
      "Step 742/1000, Loss: 0.215455561876297, Validation Loss: 0.26051539182662964\n",
      "Step 743/1000, Loss: 0.21543234586715698, Validation Loss: 0.2604908347129822\n",
      "Step 744/1000, Loss: 0.21540920436382294, Validation Loss: 0.2604663670063019\n",
      "Step 745/1000, Loss: 0.21538598835468292, Validation Loss: 0.2604418992996216\n",
      "Step 746/1000, Loss: 0.2153628170490265, Validation Loss: 0.2604173719882965\n",
      "Step 747/1000, Loss: 0.21533964574337006, Validation Loss: 0.2603929340839386\n",
      "Step 748/1000, Loss: 0.2153165191411972, Validation Loss: 0.2603684365749359\n",
      "Step 749/1000, Loss: 0.21529333293437958, Validation Loss: 0.260343998670578\n",
      "Step 750/1000, Loss: 0.21527016162872314, Validation Loss: 0.2603195607662201\n",
      "Step 751/1000, Loss: 0.2152469903230667, Validation Loss: 0.2602950632572174\n",
      "Step 752/1000, Loss: 0.21522381901741028, Validation Loss: 0.2602706253528595\n",
      "Step 753/1000, Loss: 0.21520067751407623, Validation Loss: 0.2602461278438568\n",
      "Step 754/1000, Loss: 0.2151775360107422, Validation Loss: 0.2602216601371765\n",
      "Step 755/1000, Loss: 0.21515439450740814, Validation Loss: 0.2601972222328186\n",
      "Step 756/1000, Loss: 0.2151312381029129, Validation Loss: 0.2601727843284607\n",
      "Step 757/1000, Loss: 0.21510812640190125, Validation Loss: 0.2601483166217804\n",
      "Step 758/1000, Loss: 0.2150849848985672, Validation Loss: 0.2601238787174225\n",
      "Step 759/1000, Loss: 0.21506182849407196, Validation Loss: 0.2600994110107422\n",
      "Step 760/1000, Loss: 0.21503868699073792, Validation Loss: 0.2600749731063843\n",
      "Step 761/1000, Loss: 0.21501557528972626, Validation Loss: 0.26005056500434875\n",
      "Step 762/1000, Loss: 0.21499241888523102, Validation Loss: 0.26002615690231323\n",
      "Step 763/1000, Loss: 0.21496933698654175, Validation Loss: 0.2600017189979553\n",
      "Step 764/1000, Loss: 0.2149461805820465, Validation Loss: 0.2599772810935974\n",
      "Step 765/1000, Loss: 0.21492305397987366, Validation Loss: 0.2599529027938843\n",
      "Step 766/1000, Loss: 0.214899942278862, Validation Loss: 0.25992852449417114\n",
      "Step 767/1000, Loss: 0.21487683057785034, Validation Loss: 0.25990408658981323\n",
      "Step 768/1000, Loss: 0.21485371887683868, Validation Loss: 0.2598797082901001\n",
      "Step 769/1000, Loss: 0.21483056247234344, Validation Loss: 0.2598553001880646\n",
      "Step 770/1000, Loss: 0.21480749547481537, Validation Loss: 0.25983095169067383\n",
      "Step 771/1000, Loss: 0.2147843837738037, Validation Loss: 0.2598065733909607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 772/1000, Loss: 0.21476125717163086, Validation Loss: 0.25978219509124756\n",
      "Step 773/1000, Loss: 0.2147381752729416, Validation Loss: 0.2597578167915344\n",
      "Step 774/1000, Loss: 0.21471509337425232, Validation Loss: 0.2597334384918213\n",
      "Step 775/1000, Loss: 0.21469199657440186, Validation Loss: 0.25970908999443054\n",
      "Step 776/1000, Loss: 0.2146688997745514, Validation Loss: 0.2596847116947174\n",
      "Step 777/1000, Loss: 0.21464583277702332, Validation Loss: 0.2596603333950043\n",
      "Step 778/1000, Loss: 0.21462272107601166, Validation Loss: 0.2596360146999359\n",
      "Step 779/1000, Loss: 0.21459966897964478, Validation Loss: 0.2596116364002228\n",
      "Step 780/1000, Loss: 0.21457655727863312, Validation Loss: 0.2595873177051544\n",
      "Step 781/1000, Loss: 0.21455349028110504, Validation Loss: 0.2595629096031189\n",
      "Step 782/1000, Loss: 0.21453039348125458, Validation Loss: 0.2595386505126953\n",
      "Step 783/1000, Loss: 0.2145073413848877, Validation Loss: 0.25951430201530457\n",
      "Step 784/1000, Loss: 0.2144842892885208, Validation Loss: 0.25948992371559143\n",
      "Step 785/1000, Loss: 0.21446119248867035, Validation Loss: 0.2594655752182007\n",
      "Step 786/1000, Loss: 0.21443809568881989, Validation Loss: 0.25944119691848755\n",
      "Step 787/1000, Loss: 0.214415043592453, Validation Loss: 0.2594168782234192\n",
      "Step 788/1000, Loss: 0.21439197659492493, Validation Loss: 0.25939247012138367\n",
      "Step 789/1000, Loss: 0.21436892449855804, Validation Loss: 0.2593681812286377\n",
      "Step 790/1000, Loss: 0.21434587240219116, Validation Loss: 0.25934386253356934\n",
      "Step 791/1000, Loss: 0.21432280540466309, Validation Loss: 0.2593194842338562\n",
      "Step 792/1000, Loss: 0.2142997682094574, Validation Loss: 0.2592952251434326\n",
      "Step 793/1000, Loss: 0.21427671611309052, Validation Loss: 0.25927096605300903\n",
      "Step 794/1000, Loss: 0.21425367891788483, Validation Loss: 0.2592466473579407\n",
      "Step 795/1000, Loss: 0.21423064172267914, Validation Loss: 0.2592223286628723\n",
      "Step 796/1000, Loss: 0.21420758962631226, Validation Loss: 0.25919806957244873\n",
      "Step 797/1000, Loss: 0.21418455243110657, Validation Loss: 0.25917378067970276\n",
      "Step 798/1000, Loss: 0.21416153013706207, Validation Loss: 0.2591494917869568\n",
      "Step 799/1000, Loss: 0.21413850784301758, Validation Loss: 0.2591252326965332\n",
      "Step 800/1000, Loss: 0.2141154706478119, Validation Loss: 0.2591009736061096\n",
      "Step 801/1000, Loss: 0.2140924632549286, Validation Loss: 0.25907671451568604\n",
      "Step 802/1000, Loss: 0.2140694260597229, Validation Loss: 0.25906458497047424\n",
      "Step 803/1000, Loss: 0.21405793726444244, Validation Loss: 0.25905242562294006\n",
      "Step 804/1000, Loss: 0.214046448469162, Validation Loss: 0.25904029607772827\n",
      "Step 805/1000, Loss: 0.21403492987155914, Validation Loss: 0.2590281665325165\n",
      "Step 806/1000, Loss: 0.21402345597743988, Validation Loss: 0.25901609659194946\n",
      "Step 807/1000, Loss: 0.21401193737983704, Validation Loss: 0.2590039372444153\n",
      "Step 808/1000, Loss: 0.2140004187822342, Validation Loss: 0.2589917778968811\n",
      "Step 809/1000, Loss: 0.21398892998695374, Validation Loss: 0.2589796781539917\n",
      "Step 810/1000, Loss: 0.2139774113893509, Validation Loss: 0.2589675784111023\n",
      "Step 811/1000, Loss: 0.21396590769290924, Validation Loss: 0.2589554488658905\n",
      "Step 812/1000, Loss: 0.2139544039964676, Validation Loss: 0.2589433491230011\n",
      "Step 813/1000, Loss: 0.21394288539886475, Validation Loss: 0.2589312195777893\n",
      "Step 814/1000, Loss: 0.2139313966035843, Validation Loss: 0.2589191496372223\n",
      "Step 815/1000, Loss: 0.21391989290714264, Validation Loss: 0.2589070498943329\n",
      "Step 816/1000, Loss: 0.2139083743095398, Validation Loss: 0.2588949501514435\n",
      "Step 817/1000, Loss: 0.21389688551425934, Validation Loss: 0.2588828206062317\n",
      "Step 818/1000, Loss: 0.21388541162014008, Validation Loss: 0.2588707208633423\n",
      "Step 819/1000, Loss: 0.21387390792369843, Validation Loss: 0.2588585615158081\n",
      "Step 820/1000, Loss: 0.21386241912841797, Validation Loss: 0.2588464617729187\n",
      "Step 821/1000, Loss: 0.2138509452342987, Validation Loss: 0.2588343620300293\n",
      "Step 822/1000, Loss: 0.21383944153785706, Validation Loss: 0.2588222920894623\n",
      "Step 823/1000, Loss: 0.2138279229402542, Validation Loss: 0.2588101625442505\n",
      "Step 824/1000, Loss: 0.21381643414497375, Validation Loss: 0.25879812240600586\n",
      "Step 825/1000, Loss: 0.2138049453496933, Validation Loss: 0.25878605246543884\n",
      "Step 826/1000, Loss: 0.21379345655441284, Validation Loss: 0.2587739825248718\n",
      "Step 827/1000, Loss: 0.2137819528579712, Validation Loss: 0.25876182317733765\n",
      "Step 828/1000, Loss: 0.21377043426036835, Validation Loss: 0.258749783039093\n",
      "Step 829/1000, Loss: 0.21375896036624908, Validation Loss: 0.258737713098526\n",
      "Step 830/1000, Loss: 0.21374748647212982, Validation Loss: 0.2587256133556366\n",
      "Step 831/1000, Loss: 0.21373598277568817, Validation Loss: 0.2587135434150696\n",
      "Step 832/1000, Loss: 0.21372446417808533, Validation Loss: 0.2587013840675354\n",
      "Step 833/1000, Loss: 0.21371299028396606, Validation Loss: 0.2586892545223236\n",
      "Step 834/1000, Loss: 0.21370148658752441, Validation Loss: 0.2586771249771118\n",
      "Step 835/1000, Loss: 0.21368998289108276, Validation Loss: 0.25866496562957764\n",
      "Step 836/1000, Loss: 0.2136785238981247, Validation Loss: 0.25865286588668823\n",
      "Step 837/1000, Loss: 0.21366702020168304, Validation Loss: 0.25864076614379883\n",
      "Step 838/1000, Loss: 0.2136555016040802, Validation Loss: 0.25862863659858704\n",
      "Step 839/1000, Loss: 0.21364401280879974, Validation Loss: 0.25861653685569763\n",
      "Step 840/1000, Loss: 0.21363255381584167, Validation Loss: 0.25860440731048584\n",
      "Step 841/1000, Loss: 0.21362106502056122, Validation Loss: 0.25859227776527405\n",
      "Step 842/1000, Loss: 0.21360957622528076, Validation Loss: 0.25858017802238464\n",
      "Step 843/1000, Loss: 0.2135980725288391, Validation Loss: 0.25856804847717285\n",
      "Step 844/1000, Loss: 0.21358659863471985, Validation Loss: 0.25855597853660583\n",
      "Step 845/1000, Loss: 0.2135750949382782, Validation Loss: 0.25854387879371643\n",
      "Step 846/1000, Loss: 0.21356360614299774, Validation Loss: 0.258531779050827\n",
      "Step 847/1000, Loss: 0.2135521024465561, Validation Loss: 0.25851961970329285\n",
      "Step 848/1000, Loss: 0.21354062855243683, Validation Loss: 0.25850751996040344\n",
      "Step 849/1000, Loss: 0.21352912485599518, Validation Loss: 0.25849542021751404\n",
      "Step 850/1000, Loss: 0.2135176658630371, Validation Loss: 0.25848332047462463\n",
      "Step 851/1000, Loss: 0.21350617706775665, Validation Loss: 0.25847122073173523\n",
      "Step 852/1000, Loss: 0.2134946882724762, Validation Loss: 0.25845909118652344\n",
      "Step 853/1000, Loss: 0.21348321437835693, Validation Loss: 0.2584470510482788\n",
      "Step 854/1000, Loss: 0.21347174048423767, Validation Loss: 0.25843489170074463\n",
      "Step 855/1000, Loss: 0.2134602665901184, Validation Loss: 0.2584227919578552\n",
      "Step 856/1000, Loss: 0.21344877779483795, Validation Loss: 0.2584106922149658\n",
      "Step 857/1000, Loss: 0.21343731880187988, Validation Loss: 0.25839856266975403\n",
      "Step 858/1000, Loss: 0.21342584490776062, Validation Loss: 0.258386492729187\n",
      "Step 859/1000, Loss: 0.21341434121131897, Validation Loss: 0.2583743929862976\n",
      "Step 860/1000, Loss: 0.2134028971195221, Validation Loss: 0.2583622634410858\n",
      "Step 861/1000, Loss: 0.21339139342308044, Validation Loss: 0.2583501935005188\n",
      "Step 862/1000, Loss: 0.2133798897266388, Validation Loss: 0.2583381235599518\n",
      "Step 863/1000, Loss: 0.21336843073368073, Validation Loss: 0.25832599401474\n",
      "Step 864/1000, Loss: 0.21335694193840027, Validation Loss: 0.258313924074173\n",
      "Step 865/1000, Loss: 0.213345468044281, Validation Loss: 0.2583017945289612\n",
      "Step 866/1000, Loss: 0.21333397924900055, Validation Loss: 0.2582896947860718\n",
      "Step 867/1000, Loss: 0.21332252025604248, Validation Loss: 0.25827765464782715\n",
      "Step 868/1000, Loss: 0.21331103146076202, Validation Loss: 0.25826552510261536\n",
      "Step 869/1000, Loss: 0.21329952776432037, Validation Loss: 0.25825345516204834\n",
      "Step 870/1000, Loss: 0.2132880538702011, Validation Loss: 0.2582413852214813\n",
      "Step 871/1000, Loss: 0.21327659487724304, Validation Loss: 0.25822925567626953\n",
      "Step 872/1000, Loss: 0.2132650911808014, Validation Loss: 0.2582172155380249\n",
      "Step 873/1000, Loss: 0.21325361728668213, Validation Loss: 0.2582050859928131\n",
      "Step 874/1000, Loss: 0.21324215829372406, Validation Loss: 0.2581930458545685\n",
      "Step 875/1000, Loss: 0.2132306545972824, Validation Loss: 0.2581809461116791\n",
      "Step 876/1000, Loss: 0.21321919560432434, Validation Loss: 0.25816890597343445\n",
      "Step 877/1000, Loss: 0.21320775151252747, Validation Loss: 0.25815683603286743\n",
      "Step 878/1000, Loss: 0.21319624781608582, Validation Loss: 0.2581447958946228\n",
      "Step 879/1000, Loss: 0.21318477392196655, Validation Loss: 0.2581326961517334\n",
      "Step 880/1000, Loss: 0.21317331492900848, Validation Loss: 0.258120596408844\n",
      "Step 881/1000, Loss: 0.21316181123256683, Validation Loss: 0.258108526468277\n",
      "Step 882/1000, Loss: 0.21315032243728638, Validation Loss: 0.25809645652770996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 883/1000, Loss: 0.21313883364200592, Validation Loss: 0.25808435678482056\n",
      "Step 884/1000, Loss: 0.21312737464904785, Validation Loss: 0.2580723464488983\n",
      "Step 885/1000, Loss: 0.21311591565608978, Validation Loss: 0.2580602467060089\n",
      "Step 886/1000, Loss: 0.21310444176197052, Validation Loss: 0.2580481469631195\n",
      "Step 887/1000, Loss: 0.21309296786785126, Validation Loss: 0.2580361068248749\n",
      "Step 888/1000, Loss: 0.2130814492702484, Validation Loss: 0.2580240070819855\n",
      "Step 889/1000, Loss: 0.21306999027729034, Validation Loss: 0.25801193714141846\n",
      "Step 890/1000, Loss: 0.21305853128433228, Validation Loss: 0.25799989700317383\n",
      "Step 891/1000, Loss: 0.21304701268672943, Validation Loss: 0.2579878568649292\n",
      "Step 892/1000, Loss: 0.21303556859493256, Validation Loss: 0.2579757869243622\n",
      "Step 893/1000, Loss: 0.2130240947008133, Validation Loss: 0.25796371698379517\n",
      "Step 894/1000, Loss: 0.21301259100437164, Validation Loss: 0.25795167684555054\n",
      "Step 895/1000, Loss: 0.21300114691257477, Validation Loss: 0.25793957710266113\n",
      "Step 896/1000, Loss: 0.2129896879196167, Validation Loss: 0.2579275667667389\n",
      "Step 897/1000, Loss: 0.21297819912433624, Validation Loss: 0.2579154074192047\n",
      "Step 898/1000, Loss: 0.21296672523021698, Validation Loss: 0.2579033076763153\n",
      "Step 899/1000, Loss: 0.21295525133609772, Validation Loss: 0.2578912079334259\n",
      "Step 900/1000, Loss: 0.21294376254081726, Validation Loss: 0.2578791379928589\n",
      "Step 901/1000, Loss: 0.2129323035478592, Validation Loss: 0.2578669786453247\n",
      "Step 902/1000, Loss: 0.21292082965373993, Validation Loss: 0.2578609585762024\n",
      "Step 903/1000, Loss: 0.2129150927066803, Validation Loss: 0.2578548789024353\n",
      "Step 904/1000, Loss: 0.21290934085845947, Validation Loss: 0.2578487992286682\n",
      "Step 905/1000, Loss: 0.21290357410907745, Validation Loss: 0.2578427195549011\n",
      "Step 906/1000, Loss: 0.21289782226085663, Validation Loss: 0.2578366696834564\n",
      "Step 907/1000, Loss: 0.21289211511611938, Validation Loss: 0.25783059000968933\n",
      "Step 908/1000, Loss: 0.21288636326789856, Validation Loss: 0.25782451033592224\n",
      "Step 909/1000, Loss: 0.21288062632083893, Validation Loss: 0.25781843066215515\n",
      "Step 910/1000, Loss: 0.2128748595714569, Validation Loss: 0.25781235098838806\n",
      "Step 911/1000, Loss: 0.21286910772323608, Validation Loss: 0.2578062415122986\n",
      "Step 912/1000, Loss: 0.21286337077617645, Validation Loss: 0.2578001618385315\n",
      "Step 913/1000, Loss: 0.21285761892795563, Validation Loss: 0.2577941119670868\n",
      "Step 914/1000, Loss: 0.2128518670797348, Validation Loss: 0.2577880322933197\n",
      "Step 915/1000, Loss: 0.21284613013267517, Validation Loss: 0.2577819526195526\n",
      "Step 916/1000, Loss: 0.21284036338329315, Validation Loss: 0.2577758729457855\n",
      "Step 917/1000, Loss: 0.21283462643623352, Validation Loss: 0.2577698230743408\n",
      "Step 918/1000, Loss: 0.2128288894891739, Validation Loss: 0.25776374340057373\n",
      "Step 919/1000, Loss: 0.21282313764095306, Validation Loss: 0.25775763392448425\n",
      "Step 920/1000, Loss: 0.21281737089157104, Validation Loss: 0.2577515244483948\n",
      "Step 921/1000, Loss: 0.21281161904335022, Validation Loss: 0.2577454745769501\n",
      "Step 922/1000, Loss: 0.2128058820962906, Validation Loss: 0.25773942470550537\n",
      "Step 923/1000, Loss: 0.21280016005039215, Validation Loss: 0.2577333450317383\n",
      "Step 924/1000, Loss: 0.21279436349868774, Validation Loss: 0.2577272355556488\n",
      "Step 925/1000, Loss: 0.2127886265516281, Validation Loss: 0.2577211856842041\n",
      "Step 926/1000, Loss: 0.21278288960456848, Validation Loss: 0.257715106010437\n",
      "Step 927/1000, Loss: 0.21277712285518646, Validation Loss: 0.2577090263366699\n",
      "Step 928/1000, Loss: 0.21277137100696564, Validation Loss: 0.25770291686058044\n",
      "Step 929/1000, Loss: 0.21276560425758362, Validation Loss: 0.25769689679145813\n",
      "Step 930/1000, Loss: 0.21275991201400757, Validation Loss: 0.25769081711769104\n",
      "Step 931/1000, Loss: 0.21275413036346436, Validation Loss: 0.25768473744392395\n",
      "Step 932/1000, Loss: 0.21274837851524353, Validation Loss: 0.25767868757247925\n",
      "Step 933/1000, Loss: 0.2127426117658615, Validation Loss: 0.25767263770103455\n",
      "Step 934/1000, Loss: 0.21273687481880188, Validation Loss: 0.25766652822494507\n",
      "Step 935/1000, Loss: 0.21273115277290344, Validation Loss: 0.257660448551178\n",
      "Step 936/1000, Loss: 0.21272535622119904, Validation Loss: 0.2576543986797333\n",
      "Step 937/1000, Loss: 0.2127196192741394, Validation Loss: 0.2576483190059662\n",
      "Step 938/1000, Loss: 0.21271388232707977, Validation Loss: 0.2576422393321991\n",
      "Step 939/1000, Loss: 0.21270811557769775, Validation Loss: 0.257636159658432\n",
      "Step 940/1000, Loss: 0.21270236372947693, Validation Loss: 0.2576300799846649\n",
      "Step 941/1000, Loss: 0.2126966267824173, Validation Loss: 0.2576240301132202\n",
      "Step 942/1000, Loss: 0.21269090473651886, Validation Loss: 0.2576179802417755\n",
      "Step 943/1000, Loss: 0.21268513798713684, Validation Loss: 0.2576119005680084\n",
      "Step 944/1000, Loss: 0.21267937123775482, Validation Loss: 0.25760582089424133\n",
      "Step 945/1000, Loss: 0.2126736342906952, Validation Loss: 0.257599800825119\n",
      "Step 946/1000, Loss: 0.21266788244247437, Validation Loss: 0.25759369134902954\n",
      "Step 947/1000, Loss: 0.21266214549541473, Validation Loss: 0.25758764147758484\n",
      "Step 948/1000, Loss: 0.2126563936471939, Validation Loss: 0.25758153200149536\n",
      "Step 949/1000, Loss: 0.2126506268978119, Validation Loss: 0.25757551193237305\n",
      "Step 950/1000, Loss: 0.21264491975307465, Validation Loss: 0.25756946206092834\n",
      "Step 951/1000, Loss: 0.21263915300369263, Validation Loss: 0.25756341218948364\n",
      "Step 952/1000, Loss: 0.2126334011554718, Validation Loss: 0.25755733251571655\n",
      "Step 953/1000, Loss: 0.2126276195049286, Validation Loss: 0.25755131244659424\n",
      "Step 954/1000, Loss: 0.21262191236019135, Validation Loss: 0.25754523277282715\n",
      "Step 955/1000, Loss: 0.21261614561080933, Validation Loss: 0.25753915309906006\n",
      "Step 956/1000, Loss: 0.2126103639602661, Validation Loss: 0.25753307342529297\n",
      "Step 957/1000, Loss: 0.2126046121120453, Validation Loss: 0.25752705335617065\n",
      "Step 958/1000, Loss: 0.21259889006614685, Validation Loss: 0.25752100348472595\n",
      "Step 959/1000, Loss: 0.21259312331676483, Validation Loss: 0.25751495361328125\n",
      "Step 960/1000, Loss: 0.212587371468544, Validation Loss: 0.2575088441371918\n",
      "Step 961/1000, Loss: 0.212581604719162, Validation Loss: 0.25750282406806946\n",
      "Step 962/1000, Loss: 0.21257586777210236, Validation Loss: 0.25749680399894714\n",
      "Step 963/1000, Loss: 0.21257011592388153, Validation Loss: 0.25749072432518005\n",
      "Step 964/1000, Loss: 0.2125643491744995, Validation Loss: 0.25748464465141296\n",
      "Step 965/1000, Loss: 0.2125585824251175, Validation Loss: 0.25747862458229065\n",
      "Step 966/1000, Loss: 0.21255283057689667, Validation Loss: 0.25747254490852356\n",
      "Step 967/1000, Loss: 0.21254709362983704, Validation Loss: 0.25746649503707886\n",
      "Step 968/1000, Loss: 0.2125413417816162, Validation Loss: 0.25746041536331177\n",
      "Step 969/1000, Loss: 0.2125355452299118, Validation Loss: 0.25745439529418945\n",
      "Step 970/1000, Loss: 0.21252980828285217, Validation Loss: 0.25744831562042236\n",
      "Step 971/1000, Loss: 0.21252407133579254, Validation Loss: 0.25744229555130005\n",
      "Step 972/1000, Loss: 0.21251827478408813, Validation Loss: 0.25743618607521057\n",
      "Step 973/1000, Loss: 0.21251250803470612, Validation Loss: 0.25743016600608826\n",
      "Step 974/1000, Loss: 0.21250677108764648, Validation Loss: 0.25742408633232117\n",
      "Step 975/1000, Loss: 0.21250101923942566, Validation Loss: 0.25741806626319885\n",
      "Step 976/1000, Loss: 0.21249523758888245, Validation Loss: 0.25741198658943176\n",
      "Step 977/1000, Loss: 0.21248950064182281, Validation Loss: 0.25740596652030945\n",
      "Step 978/1000, Loss: 0.2124837338924408, Validation Loss: 0.25739988684654236\n",
      "Step 979/1000, Loss: 0.21247798204421997, Validation Loss: 0.25739380717277527\n",
      "Step 980/1000, Loss: 0.21247220039367676, Validation Loss: 0.25738775730133057\n",
      "Step 981/1000, Loss: 0.21246644854545593, Validation Loss: 0.2573816776275635\n",
      "Step 982/1000, Loss: 0.2124606817960739, Validation Loss: 0.25737565755844116\n",
      "Step 983/1000, Loss: 0.2124549299478531, Validation Loss: 0.2573695778846741\n",
      "Step 984/1000, Loss: 0.21244916319847107, Validation Loss: 0.257363498210907\n",
      "Step 985/1000, Loss: 0.21244339644908905, Validation Loss: 0.25735747814178467\n",
      "Step 986/1000, Loss: 0.21243764460086823, Validation Loss: 0.2573513984680176\n",
      "Step 987/1000, Loss: 0.2124318778514862, Validation Loss: 0.25734537839889526\n",
      "Step 988/1000, Loss: 0.2124261111021042, Validation Loss: 0.2573392987251282\n",
      "Step 989/1000, Loss: 0.21242035925388336, Validation Loss: 0.25733324885368347\n",
      "Step 990/1000, Loss: 0.21241459250450134, Validation Loss: 0.2573271691799164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 991/1000, Loss: 0.21240882575511932, Validation Loss: 0.25732114911079407\n",
      "Step 992/1000, Loss: 0.2124030739068985, Validation Loss: 0.25731509923934937\n",
      "Step 993/1000, Loss: 0.21239730715751648, Validation Loss: 0.25730904936790466\n",
      "Step 994/1000, Loss: 0.21239157021045685, Validation Loss: 0.25730299949645996\n",
      "Step 995/1000, Loss: 0.21238580346107483, Validation Loss: 0.25729694962501526\n",
      "Step 996/1000, Loss: 0.2123800814151764, Validation Loss: 0.25729089975357056\n",
      "Step 997/1000, Loss: 0.21237431466579437, Validation Loss: 0.25728487968444824\n",
      "Step 998/1000, Loss: 0.21236859261989594, Validation Loss: 0.2572788596153259\n",
      "Step 999/1000, Loss: 0.2123628556728363, Validation Loss: 0.25727272033691406\n",
      "Step 1000/1000, Loss: 0.21235708892345428, Validation Loss: 0.25726667046546936\n",
      "Step 1/1000, Loss: 0.3244418799877167, Validation Loss: 0.6476433277130127\n",
      "Step 2/1000, Loss: 0.31811287999153137, Validation Loss: 0.6444289088249207\n",
      "Step 3/1000, Loss: 0.31247231364250183, Validation Loss: 0.6392870545387268\n",
      "Step 4/1000, Loss: 0.3074423670768738, Validation Loss: 0.6346644759178162\n",
      "Step 5/1000, Loss: 0.30292776226997375, Validation Loss: 0.6317157745361328\n",
      "Step 6/1000, Loss: 0.2988298535346985, Validation Loss: 0.6292390823364258\n",
      "Step 7/1000, Loss: 0.29505908489227295, Validation Loss: 0.6267459988594055\n",
      "Step 8/1000, Loss: 0.2915410101413727, Validation Loss: 0.6242194175720215\n",
      "Step 9/1000, Loss: 0.2882152795791626, Validation Loss: 0.6218317747116089\n",
      "Step 10/1000, Loss: 0.28503528237342834, Validation Loss: 0.6198273301124573\n",
      "Step 11/1000, Loss: 0.28196796774864197, Validation Loss: 0.6184545755386353\n",
      "Step 12/1000, Loss: 0.278992623090744, Validation Loss: 0.6179325580596924\n",
      "Step 13/1000, Loss: 0.27609795331954956, Validation Loss: 0.6184412837028503\n",
      "Step 14/1000, Loss: 0.27327924966812134, Validation Loss: 0.6201203465461731\n",
      "Step 15/1000, Loss: 0.2705356478691101, Validation Loss: 0.6230637431144714\n",
      "Step 16/1000, Loss: 0.26786789298057556, Validation Loss: 0.6273072361946106\n",
      "Step 17/1000, Loss: 0.2652767598628998, Validation Loss: 0.6328102350234985\n",
      "Step 18/1000, Loss: 0.2627617120742798, Validation Loss: 0.639435887336731\n",
      "Step 19/1000, Loss: 0.26032012701034546, Validation Loss: 0.6469286680221558\n",
      "Step 20/1000, Loss: 0.25794702768325806, Validation Loss: 0.6549013257026672\n",
      "Step 21/1000, Loss: 0.2556348443031311, Validation Loss: 0.6628429293632507\n",
      "Step 22/1000, Loss: 0.25337421894073486, Validation Loss: 0.6701579093933105\n",
      "Step 23/1000, Loss: 0.2511545419692993, Validation Loss: 0.6762416958808899\n",
      "Step 24/1000, Loss: 0.24896538257598877, Validation Loss: 0.6805906891822815\n",
      "Step 25/1000, Loss: 0.24679739773273468, Validation Loss: 0.682904064655304\n",
      "Step 26/1000, Loss: 0.24464398622512817, Validation Loss: 0.6831182241439819\n",
      "Step 27/1000, Loss: 0.24250182509422302, Validation Loss: 0.6813706159591675\n",
      "Step 28/1000, Loss: 0.24037085473537445, Validation Loss: 0.6779441833496094\n",
      "Step 29/1000, Loss: 0.2382531315088272, Validation Loss: 0.6732261180877686\n",
      "Step 30/1000, Loss: 0.2361518293619156, Validation Loss: 0.6676716208457947\n",
      "Step 31/1000, Loss: 0.23406952619552612, Validation Loss: 0.6617739796638489\n",
      "Step 32/1000, Loss: 0.23200783133506775, Validation Loss: 0.6560291647911072\n",
      "Step 33/1000, Loss: 0.22996635735034943, Validation Loss: 0.6508970260620117\n",
      "Step 34/1000, Loss: 0.22794313728809357, Validation Loss: 0.6467648148536682\n",
      "Step 35/1000, Loss: 0.22593507170677185, Validation Loss: 0.6439195871353149\n",
      "Step 36/1000, Loss: 0.22393858432769775, Validation Loss: 0.6425352692604065\n",
      "Step 37/1000, Loss: 0.22195053100585938, Validation Loss: 0.6426743865013123\n",
      "Step 38/1000, Loss: 0.21996869146823883, Validation Loss: 0.6442970037460327\n",
      "Step 39/1000, Loss: 0.2179921269416809, Validation Loss: 0.6472756862640381\n",
      "Step 40/1000, Loss: 0.21602104604244232, Validation Loss: 0.6514049768447876\n",
      "Step 41/1000, Loss: 0.21405646204948425, Validation Loss: 0.6564152836799622\n",
      "Step 42/1000, Loss: 0.2120998501777649, Validation Loss: 0.6619849801063538\n",
      "Step 43/1000, Loss: 0.21015247702598572, Validation Loss: 0.6677591800689697\n",
      "Step 44/1000, Loss: 0.2082151174545288, Validation Loss: 0.673370361328125\n",
      "Step 45/1000, Loss: 0.20628783106803894, Validation Loss: 0.678468644618988\n",
      "Step 46/1000, Loss: 0.2043699324131012, Validation Loss: 0.6827531456947327\n",
      "Step 47/1000, Loss: 0.20246022939682007, Validation Loss: 0.6859984993934631\n",
      "Step 48/1000, Loss: 0.20055727660655975, Validation Loss: 0.6880784630775452\n",
      "Step 49/1000, Loss: 0.19865991175174713, Validation Loss: 0.6889735460281372\n",
      "Step 50/1000, Loss: 0.19676737487316132, Validation Loss: 0.6887701749801636\n",
      "Step 51/1000, Loss: 0.19487951695919037, Validation Loss: 0.6876473426818848\n",
      "Step 52/1000, Loss: 0.19299668073654175, Validation Loss: 0.6858538389205933\n",
      "Step 53/1000, Loss: 0.19111941754817963, Validation Loss: 0.6836808323860168\n",
      "Step 54/1000, Loss: 0.18924826383590698, Validation Loss: 0.6814332008361816\n",
      "Step 55/1000, Loss: 0.18738354742527008, Validation Loss: 0.6793994903564453\n",
      "Step 56/1000, Loss: 0.18552511930465698, Validation Loss: 0.6778265237808228\n",
      "Step 57/1000, Loss: 0.18367257714271545, Validation Loss: 0.6768993139266968\n",
      "Step 58/1000, Loss: 0.18182529509067535, Validation Loss: 0.6767284274101257\n",
      "Step 59/1000, Loss: 0.1799827516078949, Validation Loss: 0.6773427724838257\n",
      "Step 60/1000, Loss: 0.17814461886882782, Validation Loss: 0.6786925792694092\n",
      "Step 61/1000, Loss: 0.1763109415769577, Validation Loss: 0.6806565523147583\n",
      "Step 62/1000, Loss: 0.17448195815086365, Validation Loss: 0.6830552220344543\n",
      "Step 63/1000, Loss: 0.17265814542770386, Validation Loss: 0.6856687068939209\n",
      "Step 64/1000, Loss: 0.17083978652954102, Validation Loss: 0.6882579922676086\n",
      "Step 65/1000, Loss: 0.16902722418308258, Validation Loss: 0.6905875205993652\n",
      "Step 66/1000, Loss: 0.1672205626964569, Validation Loss: 0.6924466490745544\n",
      "Step 67/1000, Loss: 0.16541969776153564, Validation Loss: 0.6936694979667664\n",
      "Step 68/1000, Loss: 0.16362449526786804, Validation Loss: 0.6941481232643127\n",
      "Step 69/1000, Loss: 0.16183486580848694, Validation Loss: 0.6938380599021912\n",
      "Step 70/1000, Loss: 0.16005073487758636, Validation Loss: 0.6927589178085327\n",
      "Step 71/1000, Loss: 0.15827219188213348, Validation Loss: 0.6909831166267395\n",
      "Step 72/1000, Loss: 0.15649938583374023, Validation Loss: 0.6886233687400818\n",
      "Step 73/1000, Loss: 0.15473252534866333, Validation Loss: 0.6858153343200684\n",
      "Step 74/1000, Loss: 0.15297174453735352, Validation Loss: 0.6826999187469482\n",
      "Step 75/1000, Loss: 0.15121722221374512, Validation Loss: 0.6794061660766602\n",
      "Step 76/1000, Loss: 0.14946898818016052, Validation Loss: 0.6760370135307312\n",
      "Step 77/1000, Loss: 0.14772705733776093, Validation Loss: 0.6726605296134949\n",
      "Step 78/1000, Loss: 0.14599141478538513, Validation Loss: 0.669304609298706\n",
      "Step 79/1000, Loss: 0.14426210522651672, Validation Loss: 0.6659579873085022\n",
      "Step 80/1000, Loss: 0.1425391286611557, Validation Loss: 0.6625723242759705\n",
      "Step 81/1000, Loss: 0.1408226639032364, Validation Loss: 0.6590717434883118\n",
      "Step 82/1000, Loss: 0.13911278545856476, Validation Loss: 0.6553636193275452\n",
      "Step 83/1000, Loss: 0.13740965723991394, Validation Loss: 0.651350200176239\n",
      "Step 84/1000, Loss: 0.1357133537530899, Validation Loss: 0.6469395160675049\n",
      "Step 85/1000, Loss: 0.13402391970157623, Validation Loss: 0.6420568823814392\n",
      "Step 86/1000, Loss: 0.13234135508537292, Validation Loss: 0.6366521120071411\n",
      "Step 87/1000, Loss: 0.13066565990447998, Validation Loss: 0.6307039856910706\n",
      "Step 88/1000, Loss: 0.1289968341588974, Validation Loss: 0.6242203712463379\n",
      "Step 89/1000, Loss: 0.12733489274978638, Validation Loss: 0.6172363758087158\n",
      "Step 90/1000, Loss: 0.1256798803806305, Validation Loss: 0.6098074316978455\n",
      "Step 91/1000, Loss: 0.12403184920549393, Validation Loss: 0.6020029187202454\n",
      "Step 92/1000, Loss: 0.12239081412553787, Validation Loss: 0.5938957333564758\n",
      "Step 93/1000, Loss: 0.12075676023960114, Validation Loss: 0.5855559706687927\n",
      "Step 94/1000, Loss: 0.1191297098994255, Validation Loss: 0.5770431160926819\n",
      "Step 95/1000, Loss: 0.11750958114862442, Validation Loss: 0.5684019923210144\n",
      "Step 96/1000, Loss: 0.11589636653661728, Validation Loss: 0.5596602559089661\n",
      "Step 97/1000, Loss: 0.1142900139093399, Validation Loss: 0.550829291343689\n",
      "Step 98/1000, Loss: 0.11269047856330872, Validation Loss: 0.5419069528579712\n",
      "Step 99/1000, Loss: 0.11109776794910431, Validation Loss: 0.5328838229179382\n",
      "Step 100/1000, Loss: 0.1095118373632431, Validation Loss: 0.5237499475479126\n",
      "Step 101/1000, Loss: 0.1079326868057251, Validation Loss: 0.5144978761672974\n",
      "Step 102/1000, Loss: 0.10636032372713089, Validation Loss: 0.5097951889038086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 103/1000, Loss: 0.1055769994854927, Validation Loss: 0.5050041079521179\n",
      "Step 104/1000, Loss: 0.10479588806629181, Validation Loss: 0.5001338124275208\n",
      "Step 105/1000, Loss: 0.10401694476604462, Validation Loss: 0.49519506096839905\n",
      "Step 106/1000, Loss: 0.10324007272720337, Validation Loss: 0.4902012348175049\n",
      "Step 107/1000, Loss: 0.10246524959802628, Validation Loss: 0.4851674437522888\n",
      "Step 108/1000, Loss: 0.10169242322444916, Validation Loss: 0.48010969161987305\n",
      "Step 109/1000, Loss: 0.10092156380414963, Validation Loss: 0.4750438928604126\n",
      "Step 110/1000, Loss: 0.10015261173248291, Validation Loss: 0.46998533606529236\n",
      "Step 111/1000, Loss: 0.09938555210828781, Validation Loss: 0.46494850516319275\n",
      "Step 112/1000, Loss: 0.09862038493156433, Validation Loss: 0.4599459767341614\n",
      "Step 113/1000, Loss: 0.09785706549882889, Validation Loss: 0.45498931407928467\n",
      "Step 114/1000, Loss: 0.09709560126066208, Validation Loss: 0.4500870406627655\n",
      "Step 115/1000, Loss: 0.09633592516183853, Validation Loss: 0.44524574279785156\n",
      "Step 116/1000, Loss: 0.09557810425758362, Validation Loss: 0.4404706656932831\n",
      "Step 117/1000, Loss: 0.09482213109731674, Validation Loss: 0.43576452136039734\n",
      "Step 118/1000, Loss: 0.09406797587871552, Validation Loss: 0.4311286509037018\n",
      "Step 119/1000, Loss: 0.09331563115119934, Validation Loss: 0.42656323313713074\n",
      "Step 120/1000, Loss: 0.092565156519413, Validation Loss: 0.4220673739910126\n",
      "Step 121/1000, Loss: 0.09181651473045349, Validation Loss: 0.41764017939567566\n",
      "Step 122/1000, Loss: 0.09106975793838501, Validation Loss: 0.4132801294326782\n",
      "Step 123/1000, Loss: 0.09032488614320755, Validation Loss: 0.40898627042770386\n",
      "Step 124/1000, Loss: 0.0895819142460823, Validation Loss: 0.4047575294971466\n",
      "Step 125/1000, Loss: 0.08884089440107346, Validation Loss: 0.40059351921081543\n",
      "Step 126/1000, Loss: 0.0881018117070198, Validation Loss: 0.39649471640586853\n",
      "Step 127/1000, Loss: 0.08736471086740494, Validation Loss: 0.3924613296985626\n",
      "Step 128/1000, Loss: 0.08662961423397064, Validation Loss: 0.38849472999572754\n",
      "Step 129/1000, Loss: 0.08589652180671692, Validation Loss: 0.3845962584018707\n",
      "Step 130/1000, Loss: 0.08516548573970795, Validation Loss: 0.3807675838470459\n",
      "Step 131/1000, Loss: 0.08443653583526611, Validation Loss: 0.3770099878311157\n",
      "Step 132/1000, Loss: 0.08370966464281082, Validation Loss: 0.3733249306678772\n",
      "Step 133/1000, Loss: 0.08298491686582565, Validation Loss: 0.3697130084037781\n",
      "Step 134/1000, Loss: 0.0822623148560524, Validation Loss: 0.36617475748062134\n",
      "Step 135/1000, Loss: 0.08154187351465225, Validation Loss: 0.3627101182937622\n",
      "Step 136/1000, Loss: 0.0808236226439476, Validation Loss: 0.3593178689479828\n",
      "Step 137/1000, Loss: 0.08010760694742203, Validation Loss: 0.35599687695503235\n",
      "Step 138/1000, Loss: 0.07939380407333374, Validation Loss: 0.35274457931518555\n",
      "Step 139/1000, Loss: 0.07868225872516632, Validation Loss: 0.3495582938194275\n",
      "Step 140/1000, Loss: 0.07797297090291977, Validation Loss: 0.34643468260765076\n",
      "Step 141/1000, Loss: 0.07726596295833588, Validation Loss: 0.3433702290058136\n",
      "Step 142/1000, Loss: 0.07656124234199524, Validation Loss: 0.3403608500957489\n",
      "Step 143/1000, Loss: 0.07585883140563965, Validation Loss: 0.33740296959877014\n",
      "Step 144/1000, Loss: 0.0751587375998497, Validation Loss: 0.33449268341064453\n",
      "Step 145/1000, Loss: 0.0744609609246254, Validation Loss: 0.3316265344619751\n",
      "Step 146/1000, Loss: 0.07376549392938614, Validation Loss: 0.32880112528800964\n",
      "Step 147/1000, Loss: 0.07307235151529312, Validation Loss: 0.32601386308670044\n",
      "Step 148/1000, Loss: 0.07238155603408813, Validation Loss: 0.3232622742652893\n",
      "Step 149/1000, Loss: 0.071693055331707, Validation Loss: 0.32054394483566284\n",
      "Step 150/1000, Loss: 0.0710068792104721, Validation Loss: 0.3178572654724121\n",
      "Step 151/1000, Loss: 0.07032303512096405, Validation Loss: 0.3152002990245819\n",
      "Step 152/1000, Loss: 0.06964150816202164, Validation Loss: 0.31257161498069763\n",
      "Step 153/1000, Loss: 0.06896228343248367, Validation Loss: 0.30996981263160706\n",
      "Step 154/1000, Loss: 0.06828535348176956, Validation Loss: 0.3073938190937042\n",
      "Step 155/1000, Loss: 0.0676107406616211, Validation Loss: 0.3048417866230011\n",
      "Step 156/1000, Loss: 0.06693843007087708, Validation Loss: 0.3023127019405365\n",
      "Step 157/1000, Loss: 0.06626839190721512, Validation Loss: 0.29980456829071045\n",
      "Step 158/1000, Loss: 0.06560065597295761, Validation Loss: 0.2973158061504364\n",
      "Step 159/1000, Loss: 0.06493521481752396, Validation Loss: 0.2948446273803711\n",
      "Step 160/1000, Loss: 0.06427206844091415, Validation Loss: 0.2923890948295593\n",
      "Step 161/1000, Loss: 0.06361120939254761, Validation Loss: 0.2899473309516907\n",
      "Step 162/1000, Loss: 0.06295265257358551, Validation Loss: 0.28751757740974426\n",
      "Step 163/1000, Loss: 0.06229640543460846, Validation Loss: 0.28509822487831116\n",
      "Step 164/1000, Loss: 0.061642494052648544, Validation Loss: 0.2826877236366272\n",
      "Step 165/1000, Loss: 0.06099090725183487, Validation Loss: 0.2802850604057312\n",
      "Step 166/1000, Loss: 0.06034170091152191, Validation Loss: 0.27788954973220825\n",
      "Step 167/1000, Loss: 0.059694889932870865, Validation Loss: 0.2755008339881897\n",
      "Step 168/1000, Loss: 0.05905049666762352, Validation Loss: 0.27311965823173523\n",
      "Step 169/1000, Loss: 0.05840858072042465, Validation Loss: 0.2707465887069702\n",
      "Step 170/1000, Loss: 0.057769160717725754, Validation Loss: 0.26838359236717224\n",
      "Step 171/1000, Loss: 0.0571322925388813, Validation Loss: 0.26603254675865173\n",
      "Step 172/1000, Loss: 0.05649804323911667, Validation Loss: 0.26369553804397583\n",
      "Step 173/1000, Loss: 0.055866435170173645, Validation Loss: 0.2613753378391266\n",
      "Step 174/1000, Loss: 0.05523756146430969, Validation Loss: 0.25907447934150696\n",
      "Step 175/1000, Loss: 0.05461147800087929, Validation Loss: 0.25679534673690796\n",
      "Step 176/1000, Loss: 0.0539882555603981, Validation Loss: 0.25454089045524597\n",
      "Step 177/1000, Loss: 0.053367987275123596, Validation Loss: 0.2523130178451538\n",
      "Step 178/1000, Loss: 0.05275075510144234, Validation Loss: 0.25011420249938965\n",
      "Step 179/1000, Loss: 0.0521366223692894, Validation Loss: 0.2479463368654251\n",
      "Step 180/1000, Loss: 0.05152571201324463, Validation Loss: 0.24581119418144226\n",
      "Step 181/1000, Loss: 0.05091811344027519, Validation Loss: 0.24371057748794556\n",
      "Step 182/1000, Loss: 0.05031392723321915, Validation Loss: 0.24164600670337677\n",
      "Step 183/1000, Loss: 0.049713246524333954, Validation Loss: 0.23961909115314484\n",
      "Step 184/1000, Loss: 0.049116212874650955, Validation Loss: 0.23763112723827362\n",
      "Step 185/1000, Loss: 0.04852287471294403, Validation Loss: 0.23568326234817505\n",
      "Step 186/1000, Loss: 0.04793340340256691, Validation Loss: 0.23377661406993866\n",
      "Step 187/1000, Loss: 0.047347862273454666, Validation Loss: 0.2319120466709137\n",
      "Step 188/1000, Loss: 0.04676640406250954, Validation Loss: 0.23009014129638672\n",
      "Step 189/1000, Loss: 0.046189114451408386, Validation Loss: 0.22831110656261444\n",
      "Step 190/1000, Loss: 0.045616112649440765, Validation Loss: 0.22657504677772522\n",
      "Step 191/1000, Loss: 0.04504750669002533, Validation Loss: 0.22488120198249817\n",
      "Step 192/1000, Loss: 0.04448340833187103, Validation Loss: 0.22322896122932434\n",
      "Step 193/1000, Loss: 0.04392391815781593, Validation Loss: 0.2216169536113739\n",
      "Step 194/1000, Loss: 0.043369147926568985, Validation Loss: 0.22004389762878418\n",
      "Step 195/1000, Loss: 0.042819198220968246, Validation Loss: 0.2185080498456955\n",
      "Step 196/1000, Loss: 0.042274173349142075, Validation Loss: 0.21700766682624817\n",
      "Step 197/1000, Loss: 0.04173414409160614, Validation Loss: 0.2155407816171646\n",
      "Step 198/1000, Loss: 0.04119924455881119, Validation Loss: 0.2141055315732956\n",
      "Step 199/1000, Loss: 0.040669504553079605, Validation Loss: 0.21269994974136353\n",
      "Step 200/1000, Loss: 0.04014504700899124, Validation Loss: 0.21132245659828186\n",
      "Step 201/1000, Loss: 0.03962593525648117, Validation Loss: 0.2099713534116745\n",
      "Step 202/1000, Loss: 0.03911224752664566, Validation Loss: 0.20930539071559906\n",
      "Step 203/1000, Loss: 0.03885772451758385, Validation Loss: 0.20864573121070862\n",
      "Step 204/1000, Loss: 0.03860510140657425, Validation Loss: 0.2079923450946808\n",
      "Step 205/1000, Loss: 0.038354311138391495, Validation Loss: 0.20734484493732452\n",
      "Step 206/1000, Loss: 0.0381053201854229, Validation Loss: 0.20670317113399506\n",
      "Step 207/1000, Loss: 0.03785808011889458, Validation Loss: 0.20606715977191925\n",
      "Step 208/1000, Loss: 0.03761255741119385, Validation Loss: 0.20543642342090607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 209/1000, Loss: 0.037368740886449814, Validation Loss: 0.2048107236623764\n",
      "Step 210/1000, Loss: 0.0371265783905983, Validation Loss: 0.20418983697891235\n",
      "Step 211/1000, Loss: 0.03688603639602661, Validation Loss: 0.2035733163356781\n",
      "Step 212/1000, Loss: 0.036647114902734756, Validation Loss: 0.20296071469783783\n",
      "Step 213/1000, Loss: 0.036409784108400345, Validation Loss: 0.2023514062166214\n",
      "Step 214/1000, Loss: 0.036174021661281586, Validation Loss: 0.20174500346183777\n",
      "Step 215/1000, Loss: 0.03593980520963669, Validation Loss: 0.20114107429981232\n",
      "Step 216/1000, Loss: 0.03570712357759476, Validation Loss: 0.20053908228874207\n",
      "Step 217/1000, Loss: 0.0354759581387043, Validation Loss: 0.19993871450424194\n",
      "Step 218/1000, Loss: 0.03524630516767502, Validation Loss: 0.19933977723121643\n",
      "Step 219/1000, Loss: 0.03501814603805542, Validation Loss: 0.19874192774295807\n",
      "Step 220/1000, Loss: 0.03479146584868431, Validation Loss: 0.19814519584178925\n",
      "Step 221/1000, Loss: 0.0345662422478199, Validation Loss: 0.19754932820796967\n",
      "Step 222/1000, Loss: 0.03434250131249428, Validation Loss: 0.19695445895195007\n",
      "Step 223/1000, Loss: 0.03412019461393356, Validation Loss: 0.19636063277721405\n",
      "Step 224/1000, Loss: 0.03389932960271835, Validation Loss: 0.19576776027679443\n",
      "Step 225/1000, Loss: 0.03367990255355835, Validation Loss: 0.19517599046230316\n",
      "Step 226/1000, Loss: 0.03346188738942146, Validation Loss: 0.19458533823490143\n",
      "Step 227/1000, Loss: 0.033245306462049484, Validation Loss: 0.19399599730968475\n",
      "Step 228/1000, Loss: 0.03303011134266853, Validation Loss: 0.19340787827968597\n",
      "Step 229/1000, Loss: 0.0328163281083107, Validation Loss: 0.19282123446464539\n",
      "Step 230/1000, Loss: 0.03260394185781479, Validation Loss: 0.19223597645759583\n",
      "Step 231/1000, Loss: 0.032392941415309906, Validation Loss: 0.19165225327014923\n",
      "Step 232/1000, Loss: 0.032183319330215454, Validation Loss: 0.1910700500011444\n",
      "Step 233/1000, Loss: 0.031975071877241135, Validation Loss: 0.1904895156621933\n",
      "Step 234/1000, Loss: 0.03176818788051605, Validation Loss: 0.1899106204509735\n",
      "Step 235/1000, Loss: 0.031562671065330505, Validation Loss: 0.18933320045471191\n",
      "Step 236/1000, Loss: 0.0313585065305233, Validation Loss: 0.1887575089931488\n",
      "Step 237/1000, Loss: 0.03115568682551384, Validation Loss: 0.18818345665931702\n",
      "Step 238/1000, Loss: 0.030954213812947273, Validation Loss: 0.18761099874973297\n",
      "Step 239/1000, Loss: 0.030754070729017258, Validation Loss: 0.18704010546207428\n",
      "Step 240/1000, Loss: 0.030555259436368942, Validation Loss: 0.1864708513021469\n",
      "Step 241/1000, Loss: 0.03035777248442173, Validation Loss: 0.18590325117111206\n",
      "Step 242/1000, Loss: 0.030161594972014427, Validation Loss: 0.18533740937709808\n",
      "Step 243/1000, Loss: 0.029966743662953377, Validation Loss: 0.18477311730384827\n",
      "Step 244/1000, Loss: 0.02977318875491619, Validation Loss: 0.18421055376529694\n",
      "Step 245/1000, Loss: 0.029580935835838318, Validation Loss: 0.1836499124765396\n",
      "Step 246/1000, Loss: 0.029389970004558563, Validation Loss: 0.18309105932712555\n",
      "Step 247/1000, Loss: 0.029200298711657524, Validation Loss: 0.18253427743911743\n",
      "Step 248/1000, Loss: 0.029011907055974007, Validation Loss: 0.18197956681251526\n",
      "Step 249/1000, Loss: 0.028824785724282265, Validation Loss: 0.18142691254615784\n",
      "Step 250/1000, Loss: 0.028638944029808044, Validation Loss: 0.18087667226791382\n",
      "Step 251/1000, Loss: 0.02845435030758381, Validation Loss: 0.1803286373615265\n",
      "Step 252/1000, Loss: 0.0282710213214159, Validation Loss: 0.17978300154209137\n",
      "Step 253/1000, Loss: 0.028088942170143127, Validation Loss: 0.17923973500728607\n",
      "Step 254/1000, Loss: 0.02790810540318489, Validation Loss: 0.1786990612745285\n",
      "Step 255/1000, Loss: 0.027728494256734848, Validation Loss: 0.17816083133220673\n",
      "Step 256/1000, Loss: 0.027550121769309044, Validation Loss: 0.1776251643896103\n",
      "Step 257/1000, Loss: 0.027372973039746284, Validation Loss: 0.17709216475486755\n",
      "Step 258/1000, Loss: 0.027197038754820824, Validation Loss: 0.1765618622303009\n",
      "Step 259/1000, Loss: 0.02702232263982296, Validation Loss: 0.17603422701358795\n",
      "Step 260/1000, Loss: 0.026848798617720604, Validation Loss: 0.17550942301750183\n",
      "Step 261/1000, Loss: 0.02667648158967495, Validation Loss: 0.1749875247478485\n",
      "Step 262/1000, Loss: 0.026505354791879654, Validation Loss: 0.1744685173034668\n",
      "Step 263/1000, Loss: 0.02633540891110897, Validation Loss: 0.17395271360874176\n",
      "Step 264/1000, Loss: 0.026166649535298347, Validation Loss: 0.17344017326831818\n",
      "Step 265/1000, Loss: 0.025999050587415695, Validation Loss: 0.17293083667755127\n",
      "Step 266/1000, Loss: 0.02583262138068676, Validation Loss: 0.1724247932434082\n",
      "Step 267/1000, Loss: 0.025667354464530945, Validation Loss: 0.17192213237285614\n",
      "Step 268/1000, Loss: 0.02550322934985161, Validation Loss: 0.1714230477809906\n",
      "Step 269/1000, Loss: 0.025340259075164795, Validation Loss: 0.17092742025852203\n",
      "Step 270/1000, Loss: 0.025178421288728714, Validation Loss: 0.1704353243112564\n",
      "Step 271/1000, Loss: 0.02501772530376911, Validation Loss: 0.16994698345661163\n",
      "Step 272/1000, Loss: 0.0248581375926733, Validation Loss: 0.1694623827934265\n",
      "Step 273/1000, Loss: 0.024699680507183075, Validation Loss: 0.16898150742053986\n",
      "Step 274/1000, Loss: 0.02454233169555664, Validation Loss: 0.1685042530298233\n",
      "Step 275/1000, Loss: 0.02438608929514885, Validation Loss: 0.1680310070514679\n",
      "Step 276/1000, Loss: 0.024230938404798508, Validation Loss: 0.16756169497966766\n",
      "Step 277/1000, Loss: 0.02407689578831196, Validation Loss: 0.1670963317155838\n",
      "Step 278/1000, Loss: 0.023923922330141068, Validation Loss: 0.1666349619626999\n",
      "Step 279/1000, Loss: 0.02377203479409218, Validation Loss: 0.1661776602268219\n",
      "Step 280/1000, Loss: 0.023621216416358948, Validation Loss: 0.16572441160678864\n",
      "Step 281/1000, Loss: 0.023471469059586525, Validation Loss: 0.1652754545211792\n",
      "Step 282/1000, Loss: 0.02332277223467827, Validation Loss: 0.16483062505722046\n",
      "Step 283/1000, Loss: 0.023175138980150223, Validation Loss: 0.16439011693000793\n",
      "Step 284/1000, Loss: 0.023028546944260597, Validation Loss: 0.16395394504070282\n",
      "Step 285/1000, Loss: 0.022882983088493347, Validation Loss: 0.16352209448814392\n",
      "Step 286/1000, Loss: 0.022738469764590263, Validation Loss: 0.16309477388858795\n",
      "Step 287/1000, Loss: 0.022594980895519257, Validation Loss: 0.16267189383506775\n",
      "Step 288/1000, Loss: 0.022452501580119133, Validation Loss: 0.16225354373455048\n",
      "Step 289/1000, Loss: 0.022311046719551086, Validation Loss: 0.16183967888355255\n",
      "Step 290/1000, Loss: 0.02217058464884758, Validation Loss: 0.16143040359020233\n",
      "Step 291/1000, Loss: 0.022031137719750404, Validation Loss: 0.16102570295333862\n",
      "Step 292/1000, Loss: 0.021892676129937172, Validation Loss: 0.16062577068805695\n",
      "Step 293/1000, Loss: 0.02175520546734333, Validation Loss: 0.1602303683757782\n",
      "Step 294/1000, Loss: 0.02161872759461403, Validation Loss: 0.1598397195339203\n",
      "Step 295/1000, Loss: 0.02148321084678173, Validation Loss: 0.15945377945899963\n",
      "Step 296/1000, Loss: 0.021348673850297928, Validation Loss: 0.1590726375579834\n",
      "Step 297/1000, Loss: 0.021215097978711128, Validation Loss: 0.15869633853435516\n",
      "Step 298/1000, Loss: 0.021082479506731033, Validation Loss: 0.15832480788230896\n",
      "Step 299/1000, Loss: 0.020950810983777046, Validation Loss: 0.15795810520648956\n",
      "Step 300/1000, Loss: 0.02082008495926857, Validation Loss: 0.15759620070457458\n",
      "Step 301/1000, Loss: 0.020690303295850754, Validation Loss: 0.1572391837835312\n",
      "Step 302/1000, Loss: 0.020561449229717255, Validation Loss: 0.15706300735473633\n",
      "Step 303/1000, Loss: 0.02049742452800274, Validation Loss: 0.15688888728618622\n",
      "Step 304/1000, Loss: 0.020433709025382996, Validation Loss: 0.15671676397323608\n",
      "Step 305/1000, Loss: 0.020370300859212875, Validation Loss: 0.15654665231704712\n",
      "Step 306/1000, Loss: 0.020307188853621483, Validation Loss: 0.1563783884048462\n",
      "Step 307/1000, Loss: 0.020244348794221878, Validation Loss: 0.1562119424343109\n",
      "Step 308/1000, Loss: 0.020181791856884956, Validation Loss: 0.1560472548007965\n",
      "Step 309/1000, Loss: 0.02011949196457863, Validation Loss: 0.1558842957019806\n",
      "Step 310/1000, Loss: 0.020057443529367447, Validation Loss: 0.15572300553321838\n",
      "Step 311/1000, Loss: 0.01999565213918686, Validation Loss: 0.15556339919567108\n",
      "Step 312/1000, Loss: 0.019934112206101418, Validation Loss: 0.15540535748004913\n",
      "Step 313/1000, Loss: 0.019872792065143585, Validation Loss: 0.15524882078170776\n",
      "Step 314/1000, Loss: 0.019811715930700302, Validation Loss: 0.15509368479251862\n",
      "Step 315/1000, Loss: 0.01975085958838463, Validation Loss: 0.1549399495124817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 316/1000, Loss: 0.01969023048877716, Validation Loss: 0.15478774905204773\n",
      "Step 317/1000, Loss: 0.019629817456007004, Validation Loss: 0.15463681519031525\n",
      "Step 318/1000, Loss: 0.019569629803299904, Validation Loss: 0.15448716282844543\n",
      "Step 319/1000, Loss: 0.01950964517891407, Validation Loss: 0.1543387621641159\n",
      "Step 320/1000, Loss: 0.019449880346655846, Validation Loss: 0.15419167280197144\n",
      "Step 321/1000, Loss: 0.01939031295478344, Validation Loss: 0.15404580533504486\n",
      "Step 322/1000, Loss: 0.019330959767103195, Validation Loss: 0.15390117466449738\n",
      "Step 323/1000, Loss: 0.019271811470389366, Validation Loss: 0.15375782549381256\n",
      "Step 324/1000, Loss: 0.01921285130083561, Validation Loss: 0.15361569821834564\n",
      "Step 325/1000, Loss: 0.019154103472828865, Validation Loss: 0.153474822640419\n",
      "Step 326/1000, Loss: 0.019095545634627342, Validation Loss: 0.15333516895771027\n",
      "Step 327/1000, Loss: 0.019037185236811638, Validation Loss: 0.15319673717021942\n",
      "Step 328/1000, Loss: 0.018979022279381752, Validation Loss: 0.15305966138839722\n",
      "Step 329/1000, Loss: 0.018921049311757088, Validation Loss: 0.1529237926006317\n",
      "Step 330/1000, Loss: 0.018863268196582794, Validation Loss: 0.15278922021389008\n",
      "Step 331/1000, Loss: 0.018805675208568573, Validation Loss: 0.15265585482120514\n",
      "Step 332/1000, Loss: 0.018748272210359573, Validation Loss: 0.15252377092838287\n",
      "Step 333/1000, Loss: 0.018691057339310646, Validation Loss: 0.1523929089307785\n",
      "Step 334/1000, Loss: 0.01863403618335724, Validation Loss: 0.15226313471794128\n",
      "Step 335/1000, Loss: 0.01857719197869301, Validation Loss: 0.15213462710380554\n",
      "Step 336/1000, Loss: 0.018520547077059746, Validation Loss: 0.15200738608837128\n",
      "Step 337/1000, Loss: 0.01846407912671566, Validation Loss: 0.15188124775886536\n",
      "Step 338/1000, Loss: 0.0184077899903059, Validation Loss: 0.15175627171993256\n",
      "Step 339/1000, Loss: 0.01835169456899166, Validation Loss: 0.15163254737854004\n",
      "Step 340/1000, Loss: 0.018295779824256897, Validation Loss: 0.15150991082191467\n",
      "Step 341/1000, Loss: 0.018240049481391907, Validation Loss: 0.15138840675354004\n",
      "Step 342/1000, Loss: 0.018184499815106392, Validation Loss: 0.15126807987689972\n",
      "Step 343/1000, Loss: 0.018129121512174606, Validation Loss: 0.15114876627922058\n",
      "Step 344/1000, Loss: 0.01807394064962864, Validation Loss: 0.15103064477443695\n",
      "Step 345/1000, Loss: 0.018018921837210655, Validation Loss: 0.15091358125209808\n",
      "Step 346/1000, Loss: 0.01796410046517849, Validation Loss: 0.15079756081104279\n",
      "Step 347/1000, Loss: 0.017909448593854904, Validation Loss: 0.15068265795707703\n",
      "Step 348/1000, Loss: 0.017854977399110794, Validation Loss: 0.15056882798671722\n",
      "Step 349/1000, Loss: 0.01780068501830101, Validation Loss: 0.15045598149299622\n",
      "Step 350/1000, Loss: 0.0177465733140707, Validation Loss: 0.15034429728984833\n",
      "Step 351/1000, Loss: 0.017692627385258675, Validation Loss: 0.15023364126682281\n",
      "Step 352/1000, Loss: 0.01763886772096157, Validation Loss: 0.15012392401695251\n",
      "Step 353/1000, Loss: 0.017585285007953644, Validation Loss: 0.15001527965068817\n",
      "Step 354/1000, Loss: 0.017531879246234894, Validation Loss: 0.14990758895874023\n",
      "Step 355/1000, Loss: 0.017478644847869873, Validation Loss: 0.14980094134807587\n",
      "Step 356/1000, Loss: 0.01742558740079403, Validation Loss: 0.14969530701637268\n",
      "Step 357/1000, Loss: 0.01737270876765251, Validation Loss: 0.1495905965566635\n",
      "Step 358/1000, Loss: 0.017320003360509872, Validation Loss: 0.14948686957359314\n",
      "Step 359/1000, Loss: 0.017267465591430664, Validation Loss: 0.14938415586948395\n",
      "Step 360/1000, Loss: 0.01721511036157608, Validation Loss: 0.14928244054317474\n",
      "Step 361/1000, Loss: 0.01716291904449463, Validation Loss: 0.1491817682981491\n",
      "Step 362/1000, Loss: 0.017110910266637802, Validation Loss: 0.14908193051815033\n",
      "Step 363/1000, Loss: 0.01705906353890896, Validation Loss: 0.14898303151130676\n",
      "Step 364/1000, Loss: 0.01700739376246929, Validation Loss: 0.14888520538806915\n",
      "Step 365/1000, Loss: 0.0169559046626091, Validation Loss: 0.1487882435321808\n",
      "Step 366/1000, Loss: 0.016904575750231743, Validation Loss: 0.1486922651529312\n",
      "Step 367/1000, Loss: 0.016853420063853264, Validation Loss: 0.1485971212387085\n",
      "Step 368/1000, Loss: 0.016802435740828514, Validation Loss: 0.1485028713941574\n",
      "Step 369/1000, Loss: 0.016751626506447792, Validation Loss: 0.14840954542160034\n",
      "Step 370/1000, Loss: 0.016700979322195053, Validation Loss: 0.14831706881523132\n",
      "Step 371/1000, Loss: 0.016650499776005745, Validation Loss: 0.14822550117969513\n",
      "Step 372/1000, Loss: 0.016600200906395912, Validation Loss: 0.14813481271266937\n",
      "Step 373/1000, Loss: 0.01655006781220436, Validation Loss: 0.14804500341415405\n",
      "Step 374/1000, Loss: 0.01650010049343109, Validation Loss: 0.1479560136795044\n",
      "Step 375/1000, Loss: 0.016450297087430954, Validation Loss: 0.14786796271800995\n",
      "Step 376/1000, Loss: 0.016400661319494247, Validation Loss: 0.14778074622154236\n",
      "Step 377/1000, Loss: 0.01635119877755642, Validation Loss: 0.14769434928894043\n",
      "Step 378/1000, Loss: 0.016301898285746574, Validation Loss: 0.14760886132717133\n",
      "Step 379/1000, Loss: 0.01625276543200016, Validation Loss: 0.1475241482257843\n",
      "Step 380/1000, Loss: 0.01620379276573658, Validation Loss: 0.14744026958942413\n",
      "Step 381/1000, Loss: 0.016154995188117027, Validation Loss: 0.14735712110996246\n",
      "Step 382/1000, Loss: 0.016106359660625458, Validation Loss: 0.1472748965024948\n",
      "Step 383/1000, Loss: 0.01605788618326187, Validation Loss: 0.14719340205192566\n",
      "Step 384/1000, Loss: 0.016009585931897163, Validation Loss: 0.14711271226406097\n",
      "Step 385/1000, Loss: 0.01596144400537014, Validation Loss: 0.14703282713890076\n",
      "Step 386/1000, Loss: 0.01591346599161625, Validation Loss: 0.1469537317752838\n",
      "Step 387/1000, Loss: 0.01586565002799034, Validation Loss: 0.146875262260437\n",
      "Step 388/1000, Loss: 0.015817992389202118, Validation Loss: 0.14679768681526184\n",
      "Step 389/1000, Loss: 0.015770504251122475, Validation Loss: 0.1467207670211792\n",
      "Step 390/1000, Loss: 0.015723174437880516, Validation Loss: 0.1466447114944458\n",
      "Step 391/1000, Loss: 0.01567601040005684, Validation Loss: 0.1465693563222885\n",
      "Step 392/1000, Loss: 0.015629006549715996, Validation Loss: 0.14649465680122375\n",
      "Step 393/1000, Loss: 0.015582170337438583, Validation Loss: 0.1464206725358963\n",
      "Step 394/1000, Loss: 0.015535485930740833, Validation Loss: 0.14634735882282257\n",
      "Step 395/1000, Loss: 0.015488963574171066, Validation Loss: 0.1462748646736145\n",
      "Step 396/1000, Loss: 0.015442603267729282, Validation Loss: 0.14620299637317657\n",
      "Step 397/1000, Loss: 0.015396405011415482, Validation Loss: 0.14613182842731476\n",
      "Step 398/1000, Loss: 0.015350360423326492, Validation Loss: 0.14606136083602905\n",
      "Step 399/1000, Loss: 0.015304473228752613, Validation Loss: 0.1459914594888687\n",
      "Step 400/1000, Loss: 0.015258749015629292, Validation Loss: 0.14592234790325165\n",
      "Step 401/1000, Loss: 0.015213181264698505, Validation Loss: 0.14585374295711517\n",
      "Step 402/1000, Loss: 0.015167775563895702, Validation Loss: 0.14581981301307678\n",
      "Step 403/1000, Loss: 0.01514513697475195, Validation Loss: 0.14578616619110107\n",
      "Step 404/1000, Loss: 0.0151225496083498, Validation Loss: 0.14575277268886566\n",
      "Step 405/1000, Loss: 0.015100013464689255, Validation Loss: 0.14571964740753174\n",
      "Step 406/1000, Loss: 0.015077518299221992, Validation Loss: 0.14568683505058289\n",
      "Step 407/1000, Loss: 0.015055078081786633, Validation Loss: 0.14565417170524597\n",
      "Step 408/1000, Loss: 0.015032676048576832, Validation Loss: 0.14562179148197174\n",
      "Step 409/1000, Loss: 0.015010319650173187, Validation Loss: 0.1455896496772766\n",
      "Step 410/1000, Loss: 0.0149880051612854, Validation Loss: 0.14555776119232178\n",
      "Step 411/1000, Loss: 0.014965726062655449, Validation Loss: 0.14552603662014008\n",
      "Step 412/1000, Loss: 0.014943481422960758, Validation Loss: 0.14549463987350464\n",
      "Step 413/1000, Loss: 0.014921288006007671, Validation Loss: 0.14546334743499756\n",
      "Step 414/1000, Loss: 0.01489911787211895, Validation Loss: 0.14543233811855316\n",
      "Step 415/1000, Loss: 0.014876988716423512, Validation Loss: 0.1454014629125595\n",
      "Step 416/1000, Loss: 0.014854896813631058, Validation Loss: 0.14537079632282257\n",
      "Step 417/1000, Loss: 0.014832845889031887, Validation Loss: 0.1453402042388916\n",
      "Step 418/1000, Loss: 0.01481082383543253, Validation Loss: 0.14530979096889496\n",
      "Step 419/1000, Loss: 0.014788838103413582, Validation Loss: 0.1452796310186386\n",
      "Step 420/1000, Loss: 0.014766882173717022, Validation Loss: 0.14524953067302704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 421/1000, Loss: 0.014744962565600872, Validation Loss: 0.145219624042511\n",
      "Step 422/1000, Loss: 0.014723074622452259, Validation Loss: 0.1451898217201233\n",
      "Step 423/1000, Loss: 0.014701222069561481, Validation Loss: 0.14516007900238037\n",
      "Step 424/1000, Loss: 0.014679394662380219, Validation Loss: 0.1451304703950882\n",
      "Step 425/1000, Loss: 0.014657599851489067, Validation Loss: 0.14510101079940796\n",
      "Step 426/1000, Loss: 0.014635849744081497, Validation Loss: 0.14507173001766205\n",
      "Step 427/1000, Loss: 0.014614117331802845, Validation Loss: 0.1450425386428833\n",
      "Step 428/1000, Loss: 0.014592424035072327, Validation Loss: 0.1450134664773941\n",
      "Step 429/1000, Loss: 0.014570754021406174, Validation Loss: 0.1449843794107437\n",
      "Step 430/1000, Loss: 0.01454912405461073, Validation Loss: 0.14495554566383362\n",
      "Step 431/1000, Loss: 0.014527522958815098, Validation Loss: 0.14492680132389069\n",
      "Step 432/1000, Loss: 0.014505944214761257, Validation Loss: 0.1448982059955597\n",
      "Step 433/1000, Loss: 0.014484409242868423, Validation Loss: 0.14486970007419586\n",
      "Step 434/1000, Loss: 0.014462892897427082, Validation Loss: 0.1448412537574768\n",
      "Step 435/1000, Loss: 0.0144414147362113, Validation Loss: 0.1448129266500473\n",
      "Step 436/1000, Loss: 0.014419960789382458, Validation Loss: 0.1447848081588745\n",
      "Step 437/1000, Loss: 0.014398538507521152, Validation Loss: 0.1447567641735077\n",
      "Step 438/1000, Loss: 0.014377152547240257, Validation Loss: 0.14472882449626923\n",
      "Step 439/1000, Loss: 0.014355788938701153, Validation Loss: 0.1447010487318039\n",
      "Step 440/1000, Loss: 0.014334460720419884, Validation Loss: 0.14467337727546692\n",
      "Step 441/1000, Loss: 0.0143131660297513, Validation Loss: 0.1446458101272583\n",
      "Step 442/1000, Loss: 0.014291897416114807, Validation Loss: 0.14461831748485565\n",
      "Step 443/1000, Loss: 0.014270663261413574, Validation Loss: 0.14459101855754852\n",
      "Step 444/1000, Loss: 0.014249446801841259, Validation Loss: 0.14456383883953094\n",
      "Step 445/1000, Loss: 0.014228269457817078, Validation Loss: 0.1445368081331253\n",
      "Step 446/1000, Loss: 0.014207126572728157, Validation Loss: 0.1445099264383316\n",
      "Step 447/1000, Loss: 0.014186015352606773, Validation Loss: 0.1444830596446991\n",
      "Step 448/1000, Loss: 0.014164923690259457, Validation Loss: 0.1444563865661621\n",
      "Step 449/1000, Loss: 0.014143870212137699, Validation Loss: 0.14442981779575348\n",
      "Step 450/1000, Loss: 0.01412284281104803, Validation Loss: 0.144403338432312\n",
      "Step 451/1000, Loss: 0.014101848006248474, Validation Loss: 0.14437690377235413\n",
      "Step 452/1000, Loss: 0.014080877415835857, Validation Loss: 0.14435067772865295\n",
      "Step 453/1000, Loss: 0.014059944078326225, Validation Loss: 0.14432455599308014\n",
      "Step 454/1000, Loss: 0.014039038680493832, Validation Loss: 0.14429853856563568\n",
      "Step 455/1000, Loss: 0.014018160291016102, Validation Loss: 0.144272580742836\n",
      "Step 456/1000, Loss: 0.013997322879731655, Validation Loss: 0.14424678683280945\n",
      "Step 457/1000, Loss: 0.013976503163576126, Validation Loss: 0.1442210078239441\n",
      "Step 458/1000, Loss: 0.013955718837678432, Validation Loss: 0.14419537782669067\n",
      "Step 459/1000, Loss: 0.013934959657490253, Validation Loss: 0.14416983723640442\n",
      "Step 460/1000, Loss: 0.01391423400491476, Validation Loss: 0.14414437115192413\n",
      "Step 461/1000, Loss: 0.01389353722333908, Validation Loss: 0.14411908388137817\n",
      "Step 462/1000, Loss: 0.01387287862598896, Validation Loss: 0.14409388601779938\n",
      "Step 463/1000, Loss: 0.013852240517735481, Validation Loss: 0.14406877756118774\n",
      "Step 464/1000, Loss: 0.013831640593707561, Validation Loss: 0.14404363930225372\n",
      "Step 465/1000, Loss: 0.01381106860935688, Validation Loss: 0.14401869475841522\n",
      "Step 466/1000, Loss: 0.013790520839393139, Validation Loss: 0.1439938247203827\n",
      "Step 467/1000, Loss: 0.013770011253654957, Validation Loss: 0.14396905899047852\n",
      "Step 468/1000, Loss: 0.013749528676271439, Validation Loss: 0.1439443677663803\n",
      "Step 469/1000, Loss: 0.013729074969887733, Validation Loss: 0.14391975104808807\n",
      "Step 470/1000, Loss: 0.013708650134503841, Validation Loss: 0.14389528334140778\n",
      "Step 471/1000, Loss: 0.013688264414668083, Validation Loss: 0.14387090504169464\n",
      "Step 472/1000, Loss: 0.013667899183928967, Validation Loss: 0.14384660124778748\n",
      "Step 473/1000, Loss: 0.013647571206092834, Validation Loss: 0.1438223123550415\n",
      "Step 474/1000, Loss: 0.013627266511321068, Validation Loss: 0.14379823207855225\n",
      "Step 475/1000, Loss: 0.013606992550194263, Validation Loss: 0.14377421140670776\n",
      "Step 476/1000, Loss: 0.01358675304800272, Validation Loss: 0.1437501609325409\n",
      "Step 477/1000, Loss: 0.013566550798714161, Validation Loss: 0.14372630417346954\n",
      "Step 478/1000, Loss: 0.013546365313231945, Validation Loss: 0.14370256662368774\n",
      "Step 479/1000, Loss: 0.013526218943297863, Validation Loss: 0.14367888867855072\n",
      "Step 480/1000, Loss: 0.013506101444363594, Validation Loss: 0.14365527033805847\n",
      "Step 481/1000, Loss: 0.013486011885106564, Validation Loss: 0.14363175630569458\n",
      "Step 482/1000, Loss: 0.01346595399081707, Validation Loss: 0.14360836148262024\n",
      "Step 483/1000, Loss: 0.01344592310488224, Validation Loss: 0.1435849368572235\n",
      "Step 484/1000, Loss: 0.013425924815237522, Validation Loss: 0.14356166124343872\n",
      "Step 485/1000, Loss: 0.013405958190560341, Validation Loss: 0.14353840053081512\n",
      "Step 486/1000, Loss: 0.013386021368205547, Validation Loss: 0.14351530373096466\n",
      "Step 487/1000, Loss: 0.013366114348173141, Validation Loss: 0.14349228143692017\n",
      "Step 488/1000, Loss: 0.013346240855753422, Validation Loss: 0.14346933364868164\n",
      "Step 489/1000, Loss: 0.013326392509043217, Validation Loss: 0.1434464007616043\n",
      "Step 490/1000, Loss: 0.013306574895977974, Validation Loss: 0.1434236317873001\n",
      "Step 491/1000, Loss: 0.013286790810525417, Validation Loss: 0.1434008777141571\n",
      "Step 492/1000, Loss: 0.0132670346647501, Validation Loss: 0.14337822794914246\n",
      "Step 493/1000, Loss: 0.013247310183942318, Validation Loss: 0.1433556228876114\n",
      "Step 494/1000, Loss: 0.013227618299424648, Validation Loss: 0.14333312213420868\n",
      "Step 495/1000, Loss: 0.013207950629293919, Validation Loss: 0.14331063628196716\n",
      "Step 496/1000, Loss: 0.013188314624130726, Validation Loss: 0.14328832924365997\n",
      "Step 497/1000, Loss: 0.013168713077902794, Validation Loss: 0.14326605200767517\n",
      "Step 498/1000, Loss: 0.013149137608706951, Validation Loss: 0.14324378967285156\n",
      "Step 499/1000, Loss: 0.01312959659844637, Validation Loss: 0.1432216614484787\n",
      "Step 500/1000, Loss: 0.013110087253153324, Validation Loss: 0.14319956302642822\n",
      "Step 501/1000, Loss: 0.013090608641505241, Validation Loss: 0.14317750930786133\n",
      "Step 502/1000, Loss: 0.013071156106889248, Validation Loss: 0.14316654205322266\n",
      "Step 503/1000, Loss: 0.013061441481113434, Validation Loss: 0.14315560460090637\n",
      "Step 504/1000, Loss: 0.013051739893853664, Validation Loss: 0.14314472675323486\n",
      "Step 505/1000, Loss: 0.013042042963206768, Validation Loss: 0.14313377439975739\n",
      "Step 506/1000, Loss: 0.013032358139753342, Validation Loss: 0.14312289655208588\n",
      "Step 507/1000, Loss: 0.013022671453654766, Validation Loss: 0.14311206340789795\n",
      "Step 508/1000, Loss: 0.013013001531362534, Validation Loss: 0.14310131967067719\n",
      "Step 509/1000, Loss: 0.013003338128328323, Validation Loss: 0.1430906057357788\n",
      "Step 510/1000, Loss: 0.01299368031322956, Validation Loss: 0.14307978749275208\n",
      "Step 511/1000, Loss: 0.012984023429453373, Validation Loss: 0.14306896924972534\n",
      "Step 512/1000, Loss: 0.012974381446838379, Validation Loss: 0.14305834472179413\n",
      "Step 513/1000, Loss: 0.012964736670255661, Validation Loss: 0.14304767549037933\n",
      "Step 514/1000, Loss: 0.012955105863511562, Validation Loss: 0.14303703606128693\n",
      "Step 515/1000, Loss: 0.012945474125444889, Validation Loss: 0.1430264264345169\n",
      "Step 516/1000, Loss: 0.012935852631926537, Validation Loss: 0.14301583170890808\n",
      "Step 517/1000, Loss: 0.012926233932375908, Validation Loss: 0.14300532639026642\n",
      "Step 518/1000, Loss: 0.012916624546051025, Validation Loss: 0.1429947018623352\n",
      "Step 519/1000, Loss: 0.012907015159726143, Validation Loss: 0.14298419654369354\n",
      "Step 520/1000, Loss: 0.012897414155304432, Validation Loss: 0.14297357201576233\n",
      "Step 521/1000, Loss: 0.012887821532785892, Validation Loss: 0.14296308159828186\n",
      "Step 522/1000, Loss: 0.012878220528364182, Validation Loss: 0.14295253157615662\n",
      "Step 523/1000, Loss: 0.012868637219071388, Validation Loss: 0.14294208586215973\n",
      "Step 524/1000, Loss: 0.012859055772423744, Validation Loss: 0.14293161034584045\n",
      "Step 525/1000, Loss: 0.012849478051066399, Validation Loss: 0.14292117953300476\n",
      "Step 526/1000, Loss: 0.0128399059176445, Validation Loss: 0.14291076362133026\n",
      "Step 527/1000, Loss: 0.012830340303480625, Validation Loss: 0.14290037751197815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 528/1000, Loss: 0.012820773757994175, Validation Loss: 0.14288994669914246\n",
      "Step 529/1000, Loss: 0.012811213731765747, Validation Loss: 0.14287956058979034\n",
      "Step 530/1000, Loss: 0.01280166395008564, Validation Loss: 0.14286914467811584\n",
      "Step 531/1000, Loss: 0.012792116031050682, Validation Loss: 0.14285868406295776\n",
      "Step 532/1000, Loss: 0.012782569043338299, Validation Loss: 0.14284837245941162\n",
      "Step 533/1000, Loss: 0.012773037888109684, Validation Loss: 0.14283795654773712\n",
      "Step 534/1000, Loss: 0.012763497419655323, Validation Loss: 0.1428276151418686\n",
      "Step 535/1000, Loss: 0.012753968127071857, Validation Loss: 0.14281721413135529\n",
      "Step 536/1000, Loss: 0.012744439765810966, Validation Loss: 0.14280688762664795\n",
      "Step 537/1000, Loss: 0.012734918855130672, Validation Loss: 0.1427965760231018\n",
      "Step 538/1000, Loss: 0.012725399807095528, Validation Loss: 0.14278623461723328\n",
      "Step 539/1000, Loss: 0.01271588634699583, Validation Loss: 0.14277595281600952\n",
      "Step 540/1000, Loss: 0.012706381268799305, Validation Loss: 0.14276564121246338\n",
      "Step 541/1000, Loss: 0.012696878053247929, Validation Loss: 0.14275535941123962\n",
      "Step 542/1000, Loss: 0.012687375769019127, Validation Loss: 0.1427449882030487\n",
      "Step 543/1000, Loss: 0.012677887454628944, Validation Loss: 0.14273467659950256\n",
      "Step 544/1000, Loss: 0.012668397277593613, Validation Loss: 0.14272445440292358\n",
      "Step 545/1000, Loss: 0.012658912688493729, Validation Loss: 0.14271414279937744\n",
      "Step 546/1000, Loss: 0.012649435549974442, Validation Loss: 0.14270392060279846\n",
      "Step 547/1000, Loss: 0.012639956548810005, Validation Loss: 0.1426936835050583\n",
      "Step 548/1000, Loss: 0.012630491517484188, Validation Loss: 0.1426835060119629\n",
      "Step 549/1000, Loss: 0.012621021829545498, Validation Loss: 0.14267326891422272\n",
      "Step 550/1000, Loss: 0.012611554935574532, Validation Loss: 0.14266301691532135\n",
      "Step 551/1000, Loss: 0.012602102011442184, Validation Loss: 0.14265285432338715\n",
      "Step 552/1000, Loss: 0.012592649087309837, Validation Loss: 0.14264266192913055\n",
      "Step 553/1000, Loss: 0.012583200819790363, Validation Loss: 0.14263245463371277\n",
      "Step 554/1000, Loss: 0.012573757208883762, Validation Loss: 0.14262230694293976\n",
      "Step 555/1000, Loss: 0.012564321048557758, Validation Loss: 0.14261218905448914\n",
      "Step 556/1000, Loss: 0.012554885819554329, Validation Loss: 0.1426021158695221\n",
      "Step 557/1000, Loss: 0.012545457109808922, Validation Loss: 0.1425919532775879\n",
      "Step 558/1000, Loss: 0.012536032125353813, Validation Loss: 0.14258192479610443\n",
      "Step 559/1000, Loss: 0.012526609934866428, Validation Loss: 0.1425718069076538\n",
      "Step 560/1000, Loss: 0.01251719705760479, Validation Loss: 0.14256176352500916\n",
      "Step 561/1000, Loss: 0.01250778790563345, Validation Loss: 0.1425517201423645\n",
      "Step 562/1000, Loss: 0.012498383410274982, Validation Loss: 0.14254170656204224\n",
      "Step 563/1000, Loss: 0.012488984502851963, Validation Loss: 0.14253167808055878\n",
      "Step 564/1000, Loss: 0.012479589320719242, Validation Loss: 0.1425216943025589\n",
      "Step 565/1000, Loss: 0.012470193207263947, Validation Loss: 0.14251171052455902\n",
      "Step 566/1000, Loss: 0.012460811994969845, Validation Loss: 0.14250171184539795\n",
      "Step 567/1000, Loss: 0.012451434507966042, Validation Loss: 0.14249172806739807\n",
      "Step 568/1000, Loss: 0.012442061677575111, Validation Loss: 0.1424817442893982\n",
      "Step 569/1000, Loss: 0.012432688847184181, Validation Loss: 0.1424717754125595\n",
      "Step 570/1000, Loss: 0.012423329055309296, Validation Loss: 0.14246179163455963\n",
      "Step 571/1000, Loss: 0.012413963675498962, Validation Loss: 0.1424519568681717\n",
      "Step 572/1000, Loss: 0.012404607608914375, Validation Loss: 0.14244207739830017\n",
      "Step 573/1000, Loss: 0.012395258992910385, Validation Loss: 0.14243215322494507\n",
      "Step 574/1000, Loss: 0.012385915033519268, Validation Loss: 0.14242224395275116\n",
      "Step 575/1000, Loss: 0.0123765729367733, Validation Loss: 0.14241234958171844\n",
      "Step 576/1000, Loss: 0.01236723642796278, Validation Loss: 0.14240244030952454\n",
      "Step 577/1000, Loss: 0.012357908301055431, Validation Loss: 0.14239254593849182\n",
      "Step 578/1000, Loss: 0.012348582036793232, Validation Loss: 0.14238271117210388\n",
      "Step 579/1000, Loss: 0.01233926322311163, Validation Loss: 0.14237281680107117\n",
      "Step 580/1000, Loss: 0.01232994720339775, Validation Loss: 0.14236293733119965\n",
      "Step 581/1000, Loss: 0.012320639565587044, Validation Loss: 0.1423531174659729\n",
      "Step 582/1000, Loss: 0.01231133472174406, Validation Loss: 0.14234335720539093\n",
      "Step 583/1000, Loss: 0.012302031740546227, Validation Loss: 0.14233356714248657\n",
      "Step 584/1000, Loss: 0.012292738072574139, Validation Loss: 0.1423238068819046\n",
      "Step 585/1000, Loss: 0.012283450923860073, Validation Loss: 0.14231395721435547\n",
      "Step 586/1000, Loss: 0.012274160049855709, Validation Loss: 0.1423041671514511\n",
      "Step 587/1000, Loss: 0.012264884077012539, Validation Loss: 0.14229437708854675\n",
      "Step 588/1000, Loss: 0.012255609035491943, Validation Loss: 0.14228463172912598\n",
      "Step 589/1000, Loss: 0.012246341444551945, Validation Loss: 0.14227484166622162\n",
      "Step 590/1000, Loss: 0.012237072922289371, Validation Loss: 0.14226512610912323\n",
      "Step 591/1000, Loss: 0.012227813713252544, Validation Loss: 0.14225538074970245\n",
      "Step 592/1000, Loss: 0.012218562886118889, Validation Loss: 0.14224572479724884\n",
      "Step 593/1000, Loss: 0.01220931950956583, Validation Loss: 0.14223603904247284\n",
      "Step 594/1000, Loss: 0.012200075201690197, Validation Loss: 0.14222630858421326\n",
      "Step 595/1000, Loss: 0.01219084020704031, Validation Loss: 0.14221662282943726\n",
      "Step 596/1000, Loss: 0.012181603349745274, Validation Loss: 0.14220689237117767\n",
      "Step 597/1000, Loss: 0.012172376736998558, Validation Loss: 0.14219725131988525\n",
      "Step 598/1000, Loss: 0.012163151055574417, Validation Loss: 0.14218762516975403\n",
      "Step 599/1000, Loss: 0.01215393841266632, Validation Loss: 0.14217795431613922\n",
      "Step 600/1000, Loss: 0.012144723907113075, Validation Loss: 0.14216828346252441\n",
      "Step 601/1000, Loss: 0.012135517783463001, Validation Loss: 0.1421586573123932\n",
      "Step 602/1000, Loss: 0.012126314453780651, Validation Loss: 0.14215384423732758\n",
      "Step 603/1000, Loss: 0.012121719308197498, Validation Loss: 0.14214901626110077\n",
      "Step 604/1000, Loss: 0.012117128819227219, Validation Loss: 0.14214426279067993\n",
      "Step 605/1000, Loss: 0.012112523429095745, Validation Loss: 0.1421395242214203\n",
      "Step 606/1000, Loss: 0.012107926420867443, Validation Loss: 0.14213471114635468\n",
      "Step 607/1000, Loss: 0.012103334069252014, Validation Loss: 0.14212997257709503\n",
      "Step 608/1000, Loss: 0.012098738923668861, Validation Loss: 0.1421252340078354\n",
      "Step 609/1000, Loss: 0.012094144709408283, Validation Loss: 0.14212043583393097\n",
      "Step 610/1000, Loss: 0.012089553289115429, Validation Loss: 0.14211563766002655\n",
      "Step 611/1000, Loss: 0.012084964662790298, Validation Loss: 0.1421108841896057\n",
      "Step 612/1000, Loss: 0.01208037231117487, Validation Loss: 0.14210611581802368\n",
      "Step 613/1000, Loss: 0.012075780890882015, Validation Loss: 0.14210137724876404\n",
      "Step 614/1000, Loss: 0.012071190401911736, Validation Loss: 0.14209657907485962\n",
      "Step 615/1000, Loss: 0.012066597118973732, Validation Loss: 0.14209182560443878\n",
      "Step 616/1000, Loss: 0.012062004767358303, Validation Loss: 0.14208702743053436\n",
      "Step 617/1000, Loss: 0.012057413347065449, Validation Loss: 0.14208227396011353\n",
      "Step 618/1000, Loss: 0.012052822858095169, Validation Loss: 0.1420774608850479\n",
      "Step 619/1000, Loss: 0.012048235163092613, Validation Loss: 0.14207275211811066\n",
      "Step 620/1000, Loss: 0.012043642811477184, Validation Loss: 0.1420680433511734\n",
      "Step 621/1000, Loss: 0.012039047665894032, Validation Loss: 0.14206326007843018\n",
      "Step 622/1000, Loss: 0.012034463696181774, Validation Loss: 0.14205849170684814\n",
      "Step 623/1000, Loss: 0.01202987227588892, Validation Loss: 0.1420537233352661\n",
      "Step 624/1000, Loss: 0.012025284580886364, Validation Loss: 0.14204901456832886\n",
      "Step 625/1000, Loss: 0.01202069129794836, Validation Loss: 0.1420442759990692\n",
      "Step 626/1000, Loss: 0.01201610267162323, Validation Loss: 0.14203956723213196\n",
      "Step 627/1000, Loss: 0.012011514976620674, Validation Loss: 0.14203493297100067\n",
      "Step 628/1000, Loss: 0.012006929144263268, Validation Loss: 0.1420302540063858\n",
      "Step 629/1000, Loss: 0.012002337723970413, Validation Loss: 0.14202550053596497\n",
      "Step 630/1000, Loss: 0.011997749097645283, Validation Loss: 0.14202074706554413\n",
      "Step 631/1000, Loss: 0.011993162333965302, Validation Loss: 0.14201608300209045\n",
      "Step 632/1000, Loss: 0.011988569982349873, Validation Loss: 0.14201140403747559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 633/1000, Loss: 0.011983984149992466, Validation Loss: 0.14200666546821594\n",
      "Step 634/1000, Loss: 0.011979389935731888, Validation Loss: 0.14200197160243988\n",
      "Step 635/1000, Loss: 0.011974801309406757, Validation Loss: 0.14199726283550262\n",
      "Step 636/1000, Loss: 0.011970213614404202, Validation Loss: 0.14199258387088776\n",
      "Step 637/1000, Loss: 0.011965620331466198, Validation Loss: 0.1419878900051117\n",
      "Step 638/1000, Loss: 0.011961031705141068, Validation Loss: 0.14198321104049683\n",
      "Step 639/1000, Loss: 0.011956444010138512, Validation Loss: 0.14197856187820435\n",
      "Step 640/1000, Loss: 0.011951851658523083, Validation Loss: 0.14197389781475067\n",
      "Step 641/1000, Loss: 0.011947260238230228, Validation Loss: 0.14196930825710297\n",
      "Step 642/1000, Loss: 0.011942671611905098, Validation Loss: 0.1419646292924881\n",
      "Step 643/1000, Loss: 0.011938086710870266, Validation Loss: 0.1419599950313568\n",
      "Step 644/1000, Loss: 0.011933502741158009, Validation Loss: 0.14195528626441956\n",
      "Step 645/1000, Loss: 0.011928913183510303, Validation Loss: 0.14195065200328827\n",
      "Step 646/1000, Loss: 0.0119243199005723, Validation Loss: 0.14194592833518982\n",
      "Step 647/1000, Loss: 0.011919730342924595, Validation Loss: 0.14194130897521973\n",
      "Step 648/1000, Loss: 0.01191513892263174, Validation Loss: 0.14193668961524963\n",
      "Step 649/1000, Loss: 0.011910551227629185, Validation Loss: 0.14193199574947357\n",
      "Step 650/1000, Loss: 0.011905958876013756, Validation Loss: 0.1419273465871811\n",
      "Step 651/1000, Loss: 0.011901368387043476, Validation Loss: 0.14192265272140503\n",
      "Step 652/1000, Loss: 0.01189678069204092, Validation Loss: 0.14191800355911255\n",
      "Step 653/1000, Loss: 0.01189219020307064, Validation Loss: 0.14191335439682007\n",
      "Step 654/1000, Loss: 0.011887606233358383, Validation Loss: 0.1419086903333664\n",
      "Step 655/1000, Loss: 0.011883015744388103, Validation Loss: 0.1419040858745575\n",
      "Step 656/1000, Loss: 0.011878425255417824, Validation Loss: 0.1418994665145874\n",
      "Step 657/1000, Loss: 0.01187383383512497, Validation Loss: 0.14189477264881134\n",
      "Step 658/1000, Loss: 0.011869250796735287, Validation Loss: 0.14189012348651886\n",
      "Step 659/1000, Loss: 0.011864665895700455, Validation Loss: 0.14188547432422638\n",
      "Step 660/1000, Loss: 0.01186007633805275, Validation Loss: 0.1418808102607727\n",
      "Step 661/1000, Loss: 0.011855490505695343, Validation Loss: 0.14187616109848022\n",
      "Step 662/1000, Loss: 0.011850904673337936, Validation Loss: 0.1418716162443161\n",
      "Step 663/1000, Loss: 0.01184631697833538, Validation Loss: 0.141866996884346\n",
      "Step 664/1000, Loss: 0.011841724626719952, Validation Loss: 0.1418624371290207\n",
      "Step 665/1000, Loss: 0.011837138794362545, Validation Loss: 0.14185777306556702\n",
      "Step 666/1000, Loss: 0.011832543648779392, Validation Loss: 0.14185316860675812\n",
      "Step 667/1000, Loss: 0.011827962473034859, Validation Loss: 0.14184853434562683\n",
      "Step 668/1000, Loss: 0.011823375709354877, Validation Loss: 0.14184392988681793\n",
      "Step 669/1000, Loss: 0.01181878987699747, Validation Loss: 0.14183932542800903\n",
      "Step 670/1000, Loss: 0.01181420125067234, Validation Loss: 0.14183470606803894\n",
      "Step 671/1000, Loss: 0.011809613555669785, Validation Loss: 0.14183010160923004\n",
      "Step 672/1000, Loss: 0.0118050342425704, Validation Loss: 0.14182551205158234\n",
      "Step 673/1000, Loss: 0.011800444684922695, Validation Loss: 0.14182086288928986\n",
      "Step 674/1000, Loss: 0.011795863509178162, Validation Loss: 0.14181627333164215\n",
      "Step 675/1000, Loss: 0.01179127674549818, Validation Loss: 0.14181168377399445\n",
      "Step 676/1000, Loss: 0.011786692775785923, Validation Loss: 0.14180715382099152\n",
      "Step 677/1000, Loss: 0.01178211160004139, Validation Loss: 0.14180250465869904\n",
      "Step 678/1000, Loss: 0.011777527630329132, Validation Loss: 0.14179790019989014\n",
      "Step 679/1000, Loss: 0.01177294086664915, Validation Loss: 0.141793355345726\n",
      "Step 680/1000, Loss: 0.011768355965614319, Validation Loss: 0.14178884029388428\n",
      "Step 681/1000, Loss: 0.011763770133256912, Validation Loss: 0.141784206032753\n",
      "Step 682/1000, Loss: 0.01175918709486723, Validation Loss: 0.14177967607975006\n",
      "Step 683/1000, Loss: 0.011754607781767845, Validation Loss: 0.14177511632442474\n",
      "Step 684/1000, Loss: 0.011750023812055588, Validation Loss: 0.14177055656909943\n",
      "Step 685/1000, Loss: 0.011745437048375607, Validation Loss: 0.14176605641841888\n",
      "Step 686/1000, Loss: 0.011740857735276222, Validation Loss: 0.14176155626773834\n",
      "Step 687/1000, Loss: 0.011736277490854263, Validation Loss: 0.141757071018219\n",
      "Step 688/1000, Loss: 0.011731690727174282, Validation Loss: 0.14175258576869965\n",
      "Step 689/1000, Loss: 0.011727111414074898, Validation Loss: 0.1417481154203415\n",
      "Step 690/1000, Loss: 0.01172252744436264, Validation Loss: 0.14174364507198334\n",
      "Step 691/1000, Loss: 0.011717953719198704, Validation Loss: 0.14173908531665802\n",
      "Step 692/1000, Loss: 0.011713368818163872, Validation Loss: 0.14173449575901031\n",
      "Step 693/1000, Loss: 0.011708787642419338, Validation Loss: 0.14172996580600739\n",
      "Step 694/1000, Loss: 0.011704210191965103, Validation Loss: 0.14172539114952087\n",
      "Step 695/1000, Loss: 0.011699630878865719, Validation Loss: 0.14172086119651794\n",
      "Step 696/1000, Loss: 0.011695045977830887, Validation Loss: 0.1417163461446762\n",
      "Step 697/1000, Loss: 0.0116904741153121, Validation Loss: 0.14171181619167328\n",
      "Step 698/1000, Loss: 0.011685891076922417, Validation Loss: 0.14170731604099274\n",
      "Step 699/1000, Loss: 0.011681310832500458, Validation Loss: 0.14170274138450623\n",
      "Step 700/1000, Loss: 0.011676731519401073, Validation Loss: 0.14169827103614807\n",
      "Step 701/1000, Loss: 0.011672151274979115, Validation Loss: 0.1416938304901123\n",
      "Step 702/1000, Loss: 0.011667572893202305, Validation Loss: 0.14169152081012726\n",
      "Step 703/1000, Loss: 0.0116652837023139, Validation Loss: 0.14168930053710938\n",
      "Step 704/1000, Loss: 0.011662994511425495, Validation Loss: 0.1416870504617691\n",
      "Step 705/1000, Loss: 0.011660711839795113, Validation Loss: 0.14168477058410645\n",
      "Step 706/1000, Loss: 0.011658412404358387, Validation Loss: 0.14168252050876617\n",
      "Step 707/1000, Loss: 0.011656126007437706, Validation Loss: 0.1416802704334259\n",
      "Step 708/1000, Loss: 0.011653832159936428, Validation Loss: 0.14167805016040802\n",
      "Step 709/1000, Loss: 0.011651542037725449, Validation Loss: 0.14167578518390656\n",
      "Step 710/1000, Loss: 0.011649255640804768, Validation Loss: 0.14167356491088867\n",
      "Step 711/1000, Loss: 0.011646958068013191, Validation Loss: 0.14167135953903198\n",
      "Step 712/1000, Loss: 0.011644668877124786, Validation Loss: 0.1416691392660141\n",
      "Step 713/1000, Loss: 0.011642375029623508, Validation Loss: 0.14166699349880219\n",
      "Step 714/1000, Loss: 0.01164008118212223, Validation Loss: 0.1416647881269455\n",
      "Step 715/1000, Loss: 0.011637788265943527, Validation Loss: 0.1416626125574112\n",
      "Step 716/1000, Loss: 0.011635497212409973, Validation Loss: 0.1416604220867157\n",
      "Step 717/1000, Loss: 0.01163320243358612, Validation Loss: 0.14165818691253662\n",
      "Step 718/1000, Loss: 0.011630904860794544, Validation Loss: 0.14165598154067993\n",
      "Step 719/1000, Loss: 0.011628610081970692, Validation Loss: 0.14165376126766205\n",
      "Step 720/1000, Loss: 0.011626316234469414, Validation Loss: 0.14165160059928894\n",
      "Step 721/1000, Loss: 0.011624022386968136, Validation Loss: 0.14164939522743225\n",
      "Step 722/1000, Loss: 0.011621723882853985, Validation Loss: 0.14164720475673676\n",
      "Step 723/1000, Loss: 0.011619423516094685, Validation Loss: 0.14164501428604126\n",
      "Step 724/1000, Loss: 0.011617121286690235, Validation Loss: 0.14164285361766815\n",
      "Step 725/1000, Loss: 0.011614827439188957, Validation Loss: 0.14164066314697266\n",
      "Step 726/1000, Loss: 0.011612525209784508, Validation Loss: 0.14163851737976074\n",
      "Step 727/1000, Loss: 0.011610225774347782, Validation Loss: 0.14163631200790405\n",
      "Step 728/1000, Loss: 0.01160793099552393, Validation Loss: 0.14163409173488617\n",
      "Step 729/1000, Loss: 0.011605627834796906, Validation Loss: 0.1416318714618683\n",
      "Step 730/1000, Loss: 0.01160332653671503, Validation Loss: 0.14162969589233398\n",
      "Step 731/1000, Loss: 0.011601025238633156, Validation Loss: 0.1416274756193161\n",
      "Step 732/1000, Loss: 0.011598723009228706, Validation Loss: 0.1416252702474594\n",
      "Step 733/1000, Loss: 0.01159642357379198, Validation Loss: 0.14162304997444153\n",
      "Step 734/1000, Loss: 0.011594117619097233, Validation Loss: 0.14162087440490723\n",
      "Step 735/1000, Loss: 0.01159182284027338, Validation Loss: 0.14161871373653412\n",
      "Step 736/1000, Loss: 0.01158951036632061, Validation Loss: 0.141616553068161\n",
      "Step 737/1000, Loss: 0.011587203480303288, Validation Loss: 0.14161431789398193\n",
      "Step 738/1000, Loss: 0.011584900319576263, Validation Loss: 0.14161205291748047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 739/1000, Loss: 0.011582596227526665, Validation Loss: 0.1416098177433014\n",
      "Step 740/1000, Loss: 0.01158029306679964, Validation Loss: 0.1416076272726059\n",
      "Step 741/1000, Loss: 0.011577987112104893, Validation Loss: 0.14160536229610443\n",
      "Step 742/1000, Loss: 0.011575679294764996, Validation Loss: 0.14160315692424774\n",
      "Step 743/1000, Loss: 0.011573371477425098, Validation Loss: 0.14160096645355225\n",
      "Step 744/1000, Loss: 0.0115710673853755, Validation Loss: 0.14159876108169556\n",
      "Step 745/1000, Loss: 0.01156875491142273, Validation Loss: 0.14159660041332245\n",
      "Step 746/1000, Loss: 0.011566445231437683, Validation Loss: 0.14159439504146576\n",
      "Step 747/1000, Loss: 0.011564142070710659, Validation Loss: 0.14159224927425385\n",
      "Step 748/1000, Loss: 0.011561824008822441, Validation Loss: 0.14159002900123596\n",
      "Step 749/1000, Loss: 0.011559517122805119, Validation Loss: 0.14158786833286285\n",
      "Step 750/1000, Loss: 0.011557205580174923, Validation Loss: 0.14158573746681213\n",
      "Step 751/1000, Loss: 0.011554891243577003, Validation Loss: 0.14158351719379425\n",
      "Step 752/1000, Loss: 0.011552573181688786, Validation Loss: 0.14158134162425995\n",
      "Step 753/1000, Loss: 0.011550258845090866, Validation Loss: 0.14157924056053162\n",
      "Step 754/1000, Loss: 0.011547948233783245, Validation Loss: 0.14157705008983612\n",
      "Step 755/1000, Loss: 0.011545632034540176, Validation Loss: 0.14157487452030182\n",
      "Step 756/1000, Loss: 0.01154332235455513, Validation Loss: 0.1415727734565735\n",
      "Step 757/1000, Loss: 0.011541005223989487, Validation Loss: 0.14157061278820038\n",
      "Step 758/1000, Loss: 0.011538690887391567, Validation Loss: 0.14156846702098846\n",
      "Step 759/1000, Loss: 0.011536375619471073, Validation Loss: 0.14156633615493774\n",
      "Step 760/1000, Loss: 0.011534059420228004, Validation Loss: 0.14156419038772583\n",
      "Step 761/1000, Loss: 0.01153174601495266, Validation Loss: 0.1415620744228363\n",
      "Step 762/1000, Loss: 0.011529424227774143, Validation Loss: 0.1415599137544632\n",
      "Step 763/1000, Loss: 0.01152710895985365, Validation Loss: 0.14155781269073486\n",
      "Step 764/1000, Loss: 0.011524791829288006, Validation Loss: 0.14155560731887817\n",
      "Step 765/1000, Loss: 0.011522475630044937, Validation Loss: 0.14155349135398865\n",
      "Step 766/1000, Loss: 0.011520156636834145, Validation Loss: 0.14155125617980957\n",
      "Step 767/1000, Loss: 0.011517835780978203, Validation Loss: 0.14154917001724243\n",
      "Step 768/1000, Loss: 0.011515517719089985, Validation Loss: 0.1415470391511917\n",
      "Step 769/1000, Loss: 0.011513198725879192, Validation Loss: 0.1415448784828186\n",
      "Step 770/1000, Loss: 0.011510876007378101, Validation Loss: 0.1415427327156067\n",
      "Step 771/1000, Loss: 0.011508559808135033, Validation Loss: 0.14154057204723358\n",
      "Step 772/1000, Loss: 0.011506233364343643, Validation Loss: 0.1415383517742157\n",
      "Step 773/1000, Loss: 0.011503919027745724, Validation Loss: 0.14153625071048737\n",
      "Step 774/1000, Loss: 0.011501585133373737, Validation Loss: 0.14153407514095306\n",
      "Step 775/1000, Loss: 0.011499268934130669, Validation Loss: 0.14153194427490234\n",
      "Step 776/1000, Loss: 0.011496941559016705, Validation Loss: 0.14152981340885162\n",
      "Step 777/1000, Loss: 0.011494619771838188, Validation Loss: 0.14152772724628448\n",
      "Step 778/1000, Loss: 0.011492293328046799, Validation Loss: 0.14152558147907257\n",
      "Step 779/1000, Loss: 0.011489964090287685, Validation Loss: 0.14152349531650543\n",
      "Step 780/1000, Loss: 0.011487642303109169, Validation Loss: 0.14152143895626068\n",
      "Step 781/1000, Loss: 0.011485313065350056, Validation Loss: 0.14151932299137115\n",
      "Step 782/1000, Loss: 0.011482983827590942, Validation Loss: 0.14151719212532043\n",
      "Step 783/1000, Loss: 0.011480662040412426, Validation Loss: 0.14151506125926971\n",
      "Step 784/1000, Loss: 0.011478330940008163, Validation Loss: 0.1415129154920578\n",
      "Step 785/1000, Loss: 0.011476005427539349, Validation Loss: 0.14151078462600708\n",
      "Step 786/1000, Loss: 0.01147367525845766, Validation Loss: 0.14150866866111755\n",
      "Step 787/1000, Loss: 0.011471349745988846, Validation Loss: 0.14150656759738922\n",
      "Step 788/1000, Loss: 0.011469020508229733, Validation Loss: 0.1415044665336609\n",
      "Step 789/1000, Loss: 0.01146668940782547, Validation Loss: 0.14150242507457733\n",
      "Step 790/1000, Loss: 0.011464361101388931, Validation Loss: 0.141500324010849\n",
      "Step 791/1000, Loss: 0.011462030932307243, Validation Loss: 0.14149823784828186\n",
      "Step 792/1000, Loss: 0.011459700763225555, Validation Loss: 0.1414962112903595\n",
      "Step 793/1000, Loss: 0.011457369662821293, Validation Loss: 0.14149411022663116\n",
      "Step 794/1000, Loss: 0.011455042287707329, Validation Loss: 0.1414920538663864\n",
      "Step 795/1000, Loss: 0.011452710255980492, Validation Loss: 0.14149004220962524\n",
      "Step 796/1000, Loss: 0.011450374498963356, Validation Loss: 0.1414879709482193\n",
      "Step 797/1000, Loss: 0.011448044329881668, Validation Loss: 0.14148589968681335\n",
      "Step 798/1000, Loss: 0.011445713229477406, Validation Loss: 0.14148379862308502\n",
      "Step 799/1000, Loss: 0.011443382129073143, Validation Loss: 0.14148180186748505\n",
      "Step 800/1000, Loss: 0.011441046372056007, Validation Loss: 0.1414797306060791\n",
      "Step 801/1000, Loss: 0.011438710615038872, Validation Loss: 0.14147762954235077\n",
      "Step 802/1000, Loss: 0.011436371132731438, Validation Loss: 0.14147664606571198\n",
      "Step 803/1000, Loss: 0.011435206048190594, Validation Loss: 0.1414756327867508\n",
      "Step 804/1000, Loss: 0.011434043757617474, Validation Loss: 0.14147457480430603\n",
      "Step 805/1000, Loss: 0.011432875879108906, Validation Loss: 0.14147347211837769\n",
      "Step 806/1000, Loss: 0.011431707069277763, Validation Loss: 0.14147242903709412\n",
      "Step 807/1000, Loss: 0.011430536396801472, Validation Loss: 0.14147143065929413\n",
      "Step 808/1000, Loss: 0.011429371312260628, Validation Loss: 0.14147032797336578\n",
      "Step 809/1000, Loss: 0.011428199708461761, Validation Loss: 0.1414692997932434\n",
      "Step 810/1000, Loss: 0.011427029967308044, Validation Loss: 0.14146825671195984\n",
      "Step 811/1000, Loss: 0.011425858363509178, Validation Loss: 0.14146719872951508\n",
      "Step 812/1000, Loss: 0.011424687691032887, Validation Loss: 0.1414661854505539\n",
      "Step 813/1000, Loss: 0.011423518881201744, Validation Loss: 0.14146514236927032\n",
      "Step 814/1000, Loss: 0.011422346346080303, Validation Loss: 0.14146414399147034\n",
      "Step 815/1000, Loss: 0.011421174742281437, Validation Loss: 0.14146314561367035\n",
      "Step 816/1000, Loss: 0.011420001275837421, Validation Loss: 0.14146217703819275\n",
      "Step 817/1000, Loss: 0.011418836191296577, Validation Loss: 0.14146119356155396\n",
      "Step 818/1000, Loss: 0.011417658999562263, Validation Loss: 0.14146022498607635\n",
      "Step 819/1000, Loss: 0.011416484601795673, Validation Loss: 0.14145927131175995\n",
      "Step 820/1000, Loss: 0.011415315791964531, Validation Loss: 0.14145830273628235\n",
      "Step 821/1000, Loss: 0.011414137668907642, Validation Loss: 0.14145733416080475\n",
      "Step 822/1000, Loss: 0.011412961408495903, Validation Loss: 0.14145642518997192\n",
      "Step 823/1000, Loss: 0.011411783285439014, Validation Loss: 0.14145539700984955\n",
      "Step 824/1000, Loss: 0.011410608887672424, Validation Loss: 0.14145441353321075\n",
      "Step 825/1000, Loss: 0.011409434489905834, Validation Loss: 0.14145341515541077\n",
      "Step 826/1000, Loss: 0.011408255435526371, Validation Loss: 0.1414523869752884\n",
      "Step 827/1000, Loss: 0.01140708290040493, Validation Loss: 0.1414513736963272\n",
      "Step 828/1000, Loss: 0.011405904777348042, Validation Loss: 0.14145033061504364\n",
      "Step 829/1000, Loss: 0.011404729448258877, Validation Loss: 0.14144931733608246\n",
      "Step 830/1000, Loss: 0.01140354759991169, Validation Loss: 0.14144830405712128\n",
      "Step 831/1000, Loss: 0.011402372270822525, Validation Loss: 0.1414472758769989\n",
      "Step 832/1000, Loss: 0.011401195079088211, Validation Loss: 0.14144626259803772\n",
      "Step 833/1000, Loss: 0.011400016956031322, Validation Loss: 0.14144527912139893\n",
      "Step 834/1000, Loss: 0.011398839764297009, Validation Loss: 0.14144429564476013\n",
      "Step 835/1000, Loss: 0.011397662572562695, Validation Loss: 0.14144331216812134\n",
      "Step 836/1000, Loss: 0.011396476067602634, Validation Loss: 0.14144234359264374\n",
      "Step 837/1000, Loss: 0.011395297013223171, Validation Loss: 0.14144137501716614\n",
      "Step 838/1000, Loss: 0.011394117958843708, Validation Loss: 0.14144036173820496\n",
      "Step 839/1000, Loss: 0.011392938904464245, Validation Loss: 0.14143936336040497\n",
      "Step 840/1000, Loss: 0.011391752399504185, Validation Loss: 0.14143840968608856\n",
      "Step 841/1000, Loss: 0.011390570551156998, Validation Loss: 0.14143739640712738\n",
      "Step 842/1000, Loss: 0.011389389634132385, Validation Loss: 0.1414363831281662\n",
      "Step 843/1000, Loss: 0.011388208717107773, Validation Loss: 0.14143534004688263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 844/1000, Loss: 0.011387024074792862, Validation Loss: 0.14143429696559906\n",
      "Step 845/1000, Loss: 0.011385842226445675, Validation Loss: 0.14143326878547668\n",
      "Step 846/1000, Loss: 0.01138465479016304, Validation Loss: 0.1414322555065155\n",
      "Step 847/1000, Loss: 0.011383471079170704, Validation Loss: 0.14143125712871552\n",
      "Step 848/1000, Loss: 0.011382287368178368, Validation Loss: 0.14143024384975433\n",
      "Step 849/1000, Loss: 0.011381101794540882, Validation Loss: 0.14142924547195435\n",
      "Step 850/1000, Loss: 0.011379910632967949, Validation Loss: 0.14142830669879913\n",
      "Step 851/1000, Loss: 0.011378727853298187, Validation Loss: 0.14142729341983795\n",
      "Step 852/1000, Loss: 0.011377543210983276, Validation Loss: 0.14142629504203796\n",
      "Step 853/1000, Loss: 0.011376352049410343, Validation Loss: 0.14142532646656036\n",
      "Step 854/1000, Loss: 0.011375165544450283, Validation Loss: 0.14142435789108276\n",
      "Step 855/1000, Loss: 0.011373979039490223, Validation Loss: 0.14142341911792755\n",
      "Step 856/1000, Loss: 0.011372788809239864, Validation Loss: 0.14142240583896637\n",
      "Step 857/1000, Loss: 0.011371604166924953, Validation Loss: 0.14142143726348877\n",
      "Step 858/1000, Loss: 0.01137041300535202, Validation Loss: 0.14142054319381714\n",
      "Step 859/1000, Loss: 0.011369221843779087, Validation Loss: 0.14141960442066193\n",
      "Step 860/1000, Loss: 0.011368034407496452, Validation Loss: 0.14141853153705597\n",
      "Step 861/1000, Loss: 0.01136684324592352, Validation Loss: 0.14141756296157837\n",
      "Step 862/1000, Loss: 0.011365653946995735, Validation Loss: 0.141416534781456\n",
      "Step 863/1000, Loss: 0.011364460922777653, Validation Loss: 0.1414155662059784\n",
      "Step 864/1000, Loss: 0.01136326789855957, Validation Loss: 0.1414145976305008\n",
      "Step 865/1000, Loss: 0.011362079530954361, Validation Loss: 0.1414136439561844\n",
      "Step 866/1000, Loss: 0.011360888369381428, Validation Loss: 0.1414126604795456\n",
      "Step 867/1000, Loss: 0.011359693482518196, Validation Loss: 0.14141172170639038\n",
      "Step 868/1000, Loss: 0.011358503252267838, Validation Loss: 0.14141078293323517\n",
      "Step 869/1000, Loss: 0.011357307434082031, Validation Loss: 0.14140979945659637\n",
      "Step 870/1000, Loss: 0.011356117203831673, Validation Loss: 0.14140886068344116\n",
      "Step 871/1000, Loss: 0.011354921385645866, Validation Loss: 0.14140787720680237\n",
      "Step 872/1000, Loss: 0.011353731155395508, Validation Loss: 0.14140695333480835\n",
      "Step 873/1000, Loss: 0.011352531611919403, Validation Loss: 0.14140598475933075\n",
      "Step 874/1000, Loss: 0.01135134045034647, Validation Loss: 0.14140506088733673\n",
      "Step 875/1000, Loss: 0.011350144632160664, Validation Loss: 0.14140409231185913\n",
      "Step 876/1000, Loss: 0.011348948813974857, Validation Loss: 0.1414031833410263\n",
      "Step 877/1000, Loss: 0.011347746476531029, Validation Loss: 0.1414022296667099\n",
      "Step 878/1000, Loss: 0.01134655624628067, Validation Loss: 0.14140138030052185\n",
      "Step 879/1000, Loss: 0.01134535949677229, Validation Loss: 0.14140045642852783\n",
      "Step 880/1000, Loss: 0.011344162747263908, Validation Loss: 0.14139948785305023\n",
      "Step 881/1000, Loss: 0.011342961341142654, Validation Loss: 0.1413985639810562\n",
      "Step 882/1000, Loss: 0.011341768316924572, Validation Loss: 0.1413976401090622\n",
      "Step 883/1000, Loss: 0.011340567842125893, Validation Loss: 0.1413966864347458\n",
      "Step 884/1000, Loss: 0.011339370161294937, Validation Loss: 0.14139577746391296\n",
      "Step 885/1000, Loss: 0.011338172480463982, Validation Loss: 0.14139482378959656\n",
      "Step 886/1000, Loss: 0.0113369757309556, Validation Loss: 0.14139391481876373\n",
      "Step 887/1000, Loss: 0.011335772462189198, Validation Loss: 0.14139299094676971\n",
      "Step 888/1000, Loss: 0.011334571056067944, Validation Loss: 0.1413920521736145\n",
      "Step 889/1000, Loss: 0.011333377100527287, Validation Loss: 0.14139112830162048\n",
      "Step 890/1000, Loss: 0.011332174763083458, Validation Loss: 0.14139017462730408\n",
      "Step 891/1000, Loss: 0.011330971494317055, Validation Loss: 0.14138928055763245\n",
      "Step 892/1000, Loss: 0.011329772882163525, Validation Loss: 0.14138837158679962\n",
      "Step 893/1000, Loss: 0.011328569613397121, Validation Loss: 0.14138741791248322\n",
      "Step 894/1000, Loss: 0.011327369138598442, Validation Loss: 0.14138656854629517\n",
      "Step 895/1000, Loss: 0.011326168663799763, Validation Loss: 0.14138571918010712\n",
      "Step 896/1000, Loss: 0.01132496353238821, Validation Loss: 0.1413847804069519\n",
      "Step 897/1000, Loss: 0.011323763988912106, Validation Loss: 0.14138391613960266\n",
      "Step 898/1000, Loss: 0.011322557926177979, Validation Loss: 0.14138302206993103\n",
      "Step 899/1000, Loss: 0.011321360245347023, Validation Loss: 0.1413821429014206\n",
      "Step 900/1000, Loss: 0.01132015697658062, Validation Loss: 0.14138121902942657\n",
      "Step 901/1000, Loss: 0.011318947188556194, Validation Loss: 0.14138023555278778\n",
      "Step 902/1000, Loss: 0.011317748576402664, Validation Loss: 0.14137980341911316\n",
      "Step 903/1000, Loss: 0.011317145079374313, Validation Loss: 0.14137929677963257\n",
      "Step 904/1000, Loss: 0.011316541582345963, Validation Loss: 0.14137884974479675\n",
      "Step 905/1000, Loss: 0.011315942741930485, Validation Loss: 0.14137835800647736\n",
      "Step 906/1000, Loss: 0.01131533831357956, Validation Loss: 0.14137794077396393\n",
      "Step 907/1000, Loss: 0.011314738541841507, Validation Loss: 0.1413774937391281\n",
      "Step 908/1000, Loss: 0.011314134113490582, Validation Loss: 0.14137700200080872\n",
      "Step 909/1000, Loss: 0.011313525959849358, Validation Loss: 0.1413765698671341\n",
      "Step 910/1000, Loss: 0.011312928050756454, Validation Loss: 0.14137610793113708\n",
      "Step 911/1000, Loss: 0.011312322691082954, Validation Loss: 0.14137563109397888\n",
      "Step 912/1000, Loss: 0.011311720125377178, Validation Loss: 0.14137525856494904\n",
      "Step 913/1000, Loss: 0.011311112903058529, Validation Loss: 0.14137479662895203\n",
      "Step 914/1000, Loss: 0.011310510337352753, Validation Loss: 0.14137430489063263\n",
      "Step 915/1000, Loss: 0.011309902183711529, Validation Loss: 0.1413739174604416\n",
      "Step 916/1000, Loss: 0.011309301480650902, Validation Loss: 0.14137350022792816\n",
      "Step 917/1000, Loss: 0.011308697052299976, Validation Loss: 0.14137302339076996\n",
      "Step 918/1000, Loss: 0.011308086104691029, Validation Loss: 0.14137257635593414\n",
      "Step 919/1000, Loss: 0.01130748726427555, Validation Loss: 0.14137212932109833\n",
      "Step 920/1000, Loss: 0.011306879110634327, Validation Loss: 0.1413717418909073\n",
      "Step 921/1000, Loss: 0.011306274682283401, Validation Loss: 0.14137127995491028\n",
      "Step 922/1000, Loss: 0.011305664665997028, Validation Loss: 0.14137089252471924\n",
      "Step 923/1000, Loss: 0.011305059306323528, Validation Loss: 0.1413704752922058\n",
      "Step 924/1000, Loss: 0.011304453015327454, Validation Loss: 0.14137008786201477\n",
      "Step 925/1000, Loss: 0.011303843930363655, Validation Loss: 0.14136967062950134\n",
      "Step 926/1000, Loss: 0.011303236708045006, Validation Loss: 0.14136919379234314\n",
      "Step 927/1000, Loss: 0.011302629485726357, Validation Loss: 0.14136873185634613\n",
      "Step 928/1000, Loss: 0.011302015744149685, Validation Loss: 0.1413682997226715\n",
      "Step 929/1000, Loss: 0.011301407590508461, Validation Loss: 0.1413678526878357\n",
      "Step 930/1000, Loss: 0.011300799436867237, Validation Loss: 0.1413673609495163\n",
      "Step 931/1000, Loss: 0.01130018848925829, Validation Loss: 0.14136692881584167\n",
      "Step 932/1000, Loss: 0.01129958126693964, Validation Loss: 0.14136648178100586\n",
      "Step 933/1000, Loss: 0.011298969388008118, Validation Loss: 0.14136600494384766\n",
      "Step 934/1000, Loss: 0.011298359371721745, Validation Loss: 0.14136555790901184\n",
      "Step 935/1000, Loss: 0.011297749355435371, Validation Loss: 0.14136509597301483\n",
      "Step 936/1000, Loss: 0.01129713375121355, Validation Loss: 0.1413646787405014\n",
      "Step 937/1000, Loss: 0.011296526528894901, Validation Loss: 0.1413642317056656\n",
      "Step 938/1000, Loss: 0.011295918375253677, Validation Loss: 0.1413637399673462\n",
      "Step 939/1000, Loss: 0.01129530556499958, Validation Loss: 0.14136332273483276\n",
      "Step 940/1000, Loss: 0.011294697411358356, Validation Loss: 0.14136287569999695\n",
      "Step 941/1000, Loss: 0.011294083669781685, Validation Loss: 0.14136244356632233\n",
      "Step 942/1000, Loss: 0.011293470859527588, Validation Loss: 0.1413620263338089\n",
      "Step 943/1000, Loss: 0.011292862705886364, Validation Loss: 0.14136148989200592\n",
      "Step 944/1000, Loss: 0.01129225268959999, Validation Loss: 0.1413610279560089\n",
      "Step 945/1000, Loss: 0.011291639879345894, Validation Loss: 0.14136061072349548\n",
      "Step 946/1000, Loss: 0.011291028931736946, Validation Loss: 0.14136019349098206\n",
      "Step 947/1000, Loss: 0.011290410533547401, Validation Loss: 0.14135979115962982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 948/1000, Loss: 0.01128979679197073, Validation Loss: 0.14135929942131042\n",
      "Step 949/1000, Loss: 0.01128918956965208, Validation Loss: 0.1413588970899582\n",
      "Step 950/1000, Loss: 0.011288574896752834, Validation Loss: 0.14135850965976715\n",
      "Step 951/1000, Loss: 0.011287963017821312, Validation Loss: 0.14135806262493134\n",
      "Step 952/1000, Loss: 0.011287348344922066, Validation Loss: 0.14135761559009552\n",
      "Step 953/1000, Loss: 0.011286738328635693, Validation Loss: 0.14135724306106567\n",
      "Step 954/1000, Loss: 0.011286123655736446, Validation Loss: 0.14135679602622986\n",
      "Step 955/1000, Loss: 0.011285508051514626, Validation Loss: 0.14135637879371643\n",
      "Step 956/1000, Loss: 0.01128489337861538, Validation Loss: 0.1413559913635254\n",
      "Step 957/1000, Loss: 0.01128427404910326, Validation Loss: 0.14135555922985077\n",
      "Step 958/1000, Loss: 0.011283661238849163, Validation Loss: 0.14135517179965973\n",
      "Step 959/1000, Loss: 0.011283048428595066, Validation Loss: 0.1413547545671463\n",
      "Step 960/1000, Loss: 0.011282437480986118, Validation Loss: 0.1413542926311493\n",
      "Step 961/1000, Loss: 0.011281819082796574, Validation Loss: 0.14135387539863586\n",
      "Step 962/1000, Loss: 0.011281203478574753, Validation Loss: 0.14135341346263885\n",
      "Step 963/1000, Loss: 0.011280586011707783, Validation Loss: 0.14135298132896423\n",
      "Step 964/1000, Loss: 0.011279972270131111, Validation Loss: 0.1413525640964508\n",
      "Step 965/1000, Loss: 0.01127935852855444, Validation Loss: 0.1413521021604538\n",
      "Step 966/1000, Loss: 0.01127874106168747, Validation Loss: 0.14135168492794037\n",
      "Step 967/1000, Loss: 0.011278120800852776, Validation Loss: 0.14135123789310455\n",
      "Step 968/1000, Loss: 0.011277505196630955, Validation Loss: 0.14135083556175232\n",
      "Step 969/1000, Loss: 0.01127688866108656, Validation Loss: 0.1413504183292389\n",
      "Step 970/1000, Loss: 0.011276270262897015, Validation Loss: 0.14134998619556427\n",
      "Step 971/1000, Loss: 0.011275654658675194, Validation Loss: 0.1413496434688568\n",
      "Step 972/1000, Loss: 0.011275038123130798, Validation Loss: 0.1413492113351822\n",
      "Step 973/1000, Loss: 0.011274418793618679, Validation Loss: 0.14134877920150757\n",
      "Step 974/1000, Loss: 0.011273804120719433, Validation Loss: 0.14134840667247772\n",
      "Step 975/1000, Loss: 0.01127318013459444, Validation Loss: 0.14134803414344788\n",
      "Step 976/1000, Loss: 0.011272567324340343, Validation Loss: 0.14134766161441803\n",
      "Step 977/1000, Loss: 0.011271943338215351, Validation Loss: 0.14134728908538818\n",
      "Step 978/1000, Loss: 0.011271328665316105, Validation Loss: 0.14134688675403595\n",
      "Step 979/1000, Loss: 0.011270702816545963, Validation Loss: 0.14134645462036133\n",
      "Step 980/1000, Loss: 0.011270085349678993, Validation Loss: 0.14134611189365387\n",
      "Step 981/1000, Loss: 0.0112694650888443, Validation Loss: 0.14134572446346283\n",
      "Step 982/1000, Loss: 0.011268844828009605, Validation Loss: 0.1413453072309494\n",
      "Step 983/1000, Loss: 0.01126822829246521, Validation Loss: 0.14134491980075836\n",
      "Step 984/1000, Loss: 0.011267605237662792, Validation Loss: 0.14134447276592255\n",
      "Step 985/1000, Loss: 0.011266978457570076, Validation Loss: 0.14134414494037628\n",
      "Step 986/1000, Loss: 0.011266358196735382, Validation Loss: 0.14134368300437927\n",
      "Step 987/1000, Loss: 0.011265737935900688, Validation Loss: 0.14134331047534943\n",
      "Step 988/1000, Loss: 0.011265115812420845, Validation Loss: 0.1413429230451584\n",
      "Step 989/1000, Loss: 0.011264498345553875, Validation Loss: 0.14134256541728973\n",
      "Step 990/1000, Loss: 0.011263868771493435, Validation Loss: 0.1413421779870987\n",
      "Step 991/1000, Loss: 0.011263246648013592, Validation Loss: 0.14134179055690765\n",
      "Step 992/1000, Loss: 0.011262624524533749, Validation Loss: 0.141341432929039\n",
      "Step 993/1000, Loss: 0.011262007988989353, Validation Loss: 0.14134107530117035\n",
      "Step 994/1000, Loss: 0.011261377483606339, Validation Loss: 0.14134065806865692\n",
      "Step 995/1000, Loss: 0.01126075442880392, Validation Loss: 0.1413402557373047\n",
      "Step 996/1000, Loss: 0.011260132305324078, Validation Loss: 0.14133988320827484\n",
      "Step 997/1000, Loss: 0.011259512044489384, Validation Loss: 0.14133955538272858\n",
      "Step 998/1000, Loss: 0.01125888992100954, Validation Loss: 0.14133913815021515\n",
      "Step 999/1000, Loss: 0.011258266866207123, Validation Loss: 0.1413387805223465\n",
      "Step 1000/1000, Loss: 0.011257639154791832, Validation Loss: 0.1413383185863495\n",
      "Step 1/1000, Loss: 1.4891462326049805, Validation Loss: 1.3108277320861816\n",
      "Step 2/1000, Loss: 1.4150816202163696, Validation Loss: 1.245156168937683\n",
      "Step 3/1000, Loss: 1.343299388885498, Validation Loss: 1.181911587715149\n",
      "Step 4/1000, Loss: 1.2737529277801514, Validation Loss: 1.121200442314148\n",
      "Step 5/1000, Loss: 1.206505537033081, Validation Loss: 1.0629850625991821\n",
      "Step 6/1000, Loss: 1.1416099071502686, Validation Loss: 1.007299780845642\n",
      "Step 7/1000, Loss: 1.079100489616394, Validation Loss: 0.9541701674461365\n",
      "Step 8/1000, Loss: 1.0189944505691528, Validation Loss: 0.9035724997520447\n",
      "Step 9/1000, Loss: 0.9612924456596375, Validation Loss: 0.855456531047821\n",
      "Step 10/1000, Loss: 0.9059810638427734, Validation Loss: 0.8097573518753052\n",
      "Step 11/1000, Loss: 0.8530362844467163, Validation Loss: 0.7664000988006592\n",
      "Step 12/1000, Loss: 0.8024224638938904, Validation Loss: 0.7252956628799438\n",
      "Step 13/1000, Loss: 0.7540942430496216, Validation Loss: 0.6863441467285156\n",
      "Step 14/1000, Loss: 0.7079984545707703, Validation Loss: 0.6494364142417908\n",
      "Step 15/1000, Loss: 0.6640762090682983, Validation Loss: 0.6144593954086304\n",
      "Step 16/1000, Loss: 0.6222670078277588, Validation Loss: 0.5813076496124268\n",
      "Step 17/1000, Loss: 0.5825135111808777, Validation Loss: 0.5498884916305542\n",
      "Step 18/1000, Loss: 0.5447640419006348, Validation Loss: 0.5201196670532227\n",
      "Step 19/1000, Loss: 0.5089716911315918, Validation Loss: 0.4919287860393524\n",
      "Step 20/1000, Loss: 0.4750939607620239, Validation Loss: 0.4652542769908905\n",
      "Step 21/1000, Loss: 0.4430925250053406, Validation Loss: 0.4400429427623749\n",
      "Step 22/1000, Loss: 0.4129319190979004, Validation Loss: 0.41624853014945984\n",
      "Step 23/1000, Loss: 0.3845787048339844, Validation Loss: 0.39383140206336975\n",
      "Step 24/1000, Loss: 0.3580009639263153, Validation Loss: 0.37275803089141846\n",
      "Step 25/1000, Loss: 0.3331678807735443, Validation Loss: 0.35300108790397644\n",
      "Step 26/1000, Loss: 0.31004929542541504, Validation Loss: 0.3345395028591156\n",
      "Step 27/1000, Loss: 0.28861531615257263, Validation Loss: 0.31735628843307495\n",
      "Step 28/1000, Loss: 0.2688348591327667, Validation Loss: 0.3014339208602905\n",
      "Step 29/1000, Loss: 0.2506730258464813, Validation Loss: 0.2867494225502014\n",
      "Step 30/1000, Loss: 0.2340908944606781, Validation Loss: 0.2732713520526886\n",
      "Step 31/1000, Loss: 0.21904659271240234, Validation Loss: 0.2609955370426178\n",
      "Step 32/1000, Loss: 0.20553742349147797, Validation Loss: 0.249918133020401\n",
      "Step 33/1000, Loss: 0.19356563687324524, Validation Loss: 0.23999884724617004\n",
      "Step 34/1000, Loss: 0.18309839069843292, Validation Loss: 0.23116804659366608\n",
      "Step 35/1000, Loss: 0.1740735024213791, Validation Loss: 0.22333265841007233\n",
      "Step 36/1000, Loss: 0.1664026826620102, Validation Loss: 0.21638190746307373\n",
      "Step 37/1000, Loss: 0.15997113287448883, Validation Loss: 0.2101915180683136\n",
      "Step 38/1000, Loss: 0.15463688969612122, Validation Loss: 0.20462794601917267\n",
      "Step 39/1000, Loss: 0.15023529529571533, Validation Loss: 0.19955448806285858\n",
      "Step 40/1000, Loss: 0.14659300446510315, Validation Loss: 0.19483797252178192\n",
      "Step 41/1000, Loss: 0.14354193210601807, Validation Loss: 0.1903546303510666\n",
      "Step 42/1000, Loss: 0.1409279704093933, Validation Loss: 0.1859942525625229\n",
      "Step 43/1000, Loss: 0.13861598074436188, Validation Loss: 0.1816626936197281\n",
      "Step 44/1000, Loss: 0.13649290800094604, Validation Loss: 0.17728354036808014\n",
      "Step 45/1000, Loss: 0.13446912169456482, Validation Loss: 0.17279714345932007\n",
      "Step 46/1000, Loss: 0.13247783482074738, Validation Loss: 0.16815915703773499\n",
      "Step 47/1000, Loss: 0.13047358393669128, Validation Loss: 0.16333946585655212\n",
      "Step 48/1000, Loss: 0.1284293532371521, Validation Loss: 0.15832585096359253\n",
      "Step 49/1000, Loss: 0.12633341550827026, Validation Loss: 0.15313340723514557\n",
      "Step 50/1000, Loss: 0.1241859644651413, Validation Loss: 0.14781169593334198\n",
      "Step 51/1000, Loss: 0.12199565768241882, Validation Loss: 0.1424359232187271\n",
      "Step 52/1000, Loss: 0.11977676302194595, Validation Loss: 0.13708820939064026\n",
      "Step 53/1000, Loss: 0.11754607409238815, Validation Loss: 0.1318446844816208\n",
      "Step 54/1000, Loss: 0.11532075703144073, Validation Loss: 0.12677142024040222\n",
      "Step 55/1000, Loss: 0.1131163015961647, Validation Loss: 0.12192710489034653\n",
      "Step 56/1000, Loss: 0.11094532907009125, Validation Loss: 0.11736753582954407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 57/1000, Loss: 0.10881951451301575, Validation Loss: 0.11314818263053894\n",
      "Step 58/1000, Loss: 0.1067488044500351, Validation Loss: 0.10932134091854095\n",
      "Step 59/1000, Loss: 0.10474000126123428, Validation Loss: 0.10593143850564957\n",
      "Step 60/1000, Loss: 0.10279810428619385, Validation Loss: 0.10301214456558228\n",
      "Step 61/1000, Loss: 0.10092800110578537, Validation Loss: 0.10058228671550751\n",
      "Step 62/1000, Loss: 0.0991363525390625, Validation Loss: 0.098641537129879\n",
      "Step 63/1000, Loss: 0.09743265062570572, Validation Loss: 0.09716503322124481\n",
      "Step 64/1000, Loss: 0.0958278626203537, Validation Loss: 0.09610390663146973\n",
      "Step 65/1000, Loss: 0.09433130919933319, Validation Loss: 0.0953914076089859\n",
      "Step 66/1000, Loss: 0.09294735640287399, Validation Loss: 0.09494929760694504\n",
      "Step 67/1000, Loss: 0.09167345613241196, Validation Loss: 0.09469197690486908\n",
      "Step 68/1000, Loss: 0.09050016850233078, Validation Loss: 0.09453369677066803\n",
      "Step 69/1000, Loss: 0.08941370248794556, Validation Loss: 0.09440306574106216\n",
      "Step 70/1000, Loss: 0.08839884400367737, Validation Loss: 0.0942508801817894\n",
      "Step 71/1000, Loss: 0.08744238317012787, Validation Loss: 0.09405093640089035\n",
      "Step 72/1000, Loss: 0.08653467893600464, Validation Loss: 0.09380043298006058\n",
      "Step 73/1000, Loss: 0.08566906303167343, Validation Loss: 0.09350909292697906\n",
      "Step 74/1000, Loss: 0.0848408043384552, Validation Loss: 0.09319169819355011\n",
      "Step 75/1000, Loss: 0.08404554426670074, Validation Loss: 0.09286690503358841\n",
      "Step 76/1000, Loss: 0.08327911049127579, Validation Loss: 0.09255464375019073\n",
      "Step 77/1000, Loss: 0.082537941634655, Validation Loss: 0.0922740250825882\n",
      "Step 78/1000, Loss: 0.08181916177272797, Validation Loss: 0.09204062819480896\n",
      "Step 79/1000, Loss: 0.08111971616744995, Validation Loss: 0.09186381846666336\n",
      "Step 80/1000, Loss: 0.08043573796749115, Validation Loss: 0.09174960106611252\n",
      "Step 81/1000, Loss: 0.07976268976926804, Validation Loss: 0.09170364588499069\n",
      "Step 82/1000, Loss: 0.07909556478261948, Validation Loss: 0.09173045307397842\n",
      "Step 83/1000, Loss: 0.07843005657196045, Validation Loss: 0.09183108061552048\n",
      "Step 84/1000, Loss: 0.07776355743408203, Validation Loss: 0.09200341999530792\n",
      "Step 85/1000, Loss: 0.07709626108407974, Validation Loss: 0.09224335104227066\n",
      "Step 86/1000, Loss: 0.0764307975769043, Validation Loss: 0.09254663437604904\n",
      "Step 87/1000, Loss: 0.07577118277549744, Validation Loss: 0.09290973097085953\n",
      "Step 88/1000, Loss: 0.07512130588293076, Validation Loss: 0.09332788735628128\n",
      "Step 89/1000, Loss: 0.07448367029428482, Validation Loss: 0.09379560500383377\n",
      "Step 90/1000, Loss: 0.07385864853858948, Validation Loss: 0.09430734068155289\n",
      "Step 91/1000, Loss: 0.07324465364217758, Validation Loss: 0.09485812485218048\n",
      "Step 92/1000, Loss: 0.0726391077041626, Validation Loss: 0.09544279426336288\n",
      "Step 93/1000, Loss: 0.07203934341669083, Validation Loss: 0.09605540335178375\n",
      "Step 94/1000, Loss: 0.07144313305616379, Validation Loss: 0.09668812155723572\n",
      "Step 95/1000, Loss: 0.07084878534078598, Validation Loss: 0.09733301401138306\n",
      "Step 96/1000, Loss: 0.07025489956140518, Validation Loss: 0.09798349440097809\n",
      "Step 97/1000, Loss: 0.06966040283441544, Validation Loss: 0.09863218665122986\n",
      "Step 98/1000, Loss: 0.069064661860466, Validation Loss: 0.09927184134721756\n",
      "Step 99/1000, Loss: 0.06846792250871658, Validation Loss: 0.09989750385284424\n",
      "Step 100/1000, Loss: 0.06787122786045074, Validation Loss: 0.10050677508115768\n",
      "Step 101/1000, Loss: 0.06727573275566101, Validation Loss: 0.10110015422105789\n",
      "Step 102/1000, Loss: 0.066681869328022, Validation Loss: 0.10139479488134384\n",
      "Step 103/1000, Loss: 0.06638426333665848, Validation Loss: 0.10169243812561035\n",
      "Step 104/1000, Loss: 0.06608469784259796, Validation Loss: 0.10199414193630219\n",
      "Step 105/1000, Loss: 0.06578343361616135, Validation Loss: 0.1023002490401268\n",
      "Step 106/1000, Loss: 0.06548099219799042, Validation Loss: 0.10261040180921555\n",
      "Step 107/1000, Loss: 0.0651780366897583, Validation Loss: 0.10292388498783112\n",
      "Step 108/1000, Loss: 0.06487515568733215, Validation Loss: 0.10323954373598099\n",
      "Step 109/1000, Loss: 0.06457274407148361, Validation Loss: 0.10355623811483383\n",
      "Step 110/1000, Loss: 0.06427105516195297, Validation Loss: 0.10387317091226578\n",
      "Step 111/1000, Loss: 0.06397021561861038, Validation Loss: 0.10418997704982758\n",
      "Step 112/1000, Loss: 0.06367035955190659, Validation Loss: 0.10450686514377594\n",
      "Step 113/1000, Loss: 0.06337161362171173, Validation Loss: 0.10482471436262131\n",
      "Step 114/1000, Loss: 0.06307423114776611, Validation Loss: 0.1051449179649353\n",
      "Step 115/1000, Loss: 0.06277849525213242, Validation Loss: 0.10546910762786865\n",
      "Step 116/1000, Loss: 0.06248466297984123, Validation Loss: 0.10579894483089447\n",
      "Step 117/1000, Loss: 0.0621928945183754, Validation Loss: 0.10613595694303513\n",
      "Step 118/1000, Loss: 0.06190331652760506, Validation Loss: 0.1064811572432518\n",
      "Step 119/1000, Loss: 0.061616040766239166, Validation Loss: 0.1068350151181221\n",
      "Step 120/1000, Loss: 0.06133124232292175, Validation Loss: 0.1071973592042923\n",
      "Step 121/1000, Loss: 0.06104911491274834, Validation Loss: 0.10756726562976837\n",
      "Step 122/1000, Loss: 0.06076989695429802, Validation Loss: 0.10794351994991302\n",
      "Step 123/1000, Loss: 0.060493744909763336, Validation Loss: 0.10832454264163971\n",
      "Step 124/1000, Loss: 0.06022077053785324, Validation Loss: 0.10870900750160217\n",
      "Step 125/1000, Loss: 0.0599510595202446, Validation Loss: 0.10909569263458252\n",
      "Step 126/1000, Loss: 0.05968466028571129, Validation Loss: 0.10948391258716583\n",
      "Step 127/1000, Loss: 0.059421662241220474, Validation Loss: 0.10987354069948196\n",
      "Step 128/1000, Loss: 0.05916221812367439, Validation Loss: 0.11026501655578613\n",
      "Step 129/1000, Loss: 0.058906491845846176, Validation Loss: 0.11065899580717087\n",
      "Step 130/1000, Loss: 0.05865463986992836, Validation Loss: 0.1110561266541481\n",
      "Step 131/1000, Loss: 0.05840670317411423, Validation Loss: 0.11145693063735962\n",
      "Step 132/1000, Loss: 0.05816277116537094, Validation Loss: 0.1118614599108696\n",
      "Step 133/1000, Loss: 0.05792289972305298, Validation Loss: 0.1122693121433258\n",
      "Step 134/1000, Loss: 0.057687144726514816, Validation Loss: 0.11267966032028198\n",
      "Step 135/1000, Loss: 0.05745559185743332, Validation Loss: 0.11309142410755157\n",
      "Step 136/1000, Loss: 0.05722830817103386, Validation Loss: 0.11350368708372116\n",
      "Step 137/1000, Loss: 0.05700528621673584, Validation Loss: 0.1139158084988594\n",
      "Step 138/1000, Loss: 0.05678655952215195, Validation Loss: 0.11432773619890213\n",
      "Step 139/1000, Loss: 0.056572120636701584, Validation Loss: 0.11473998427391052\n",
      "Step 140/1000, Loss: 0.05636192485690117, Validation Loss: 0.11515352129936218\n",
      "Step 141/1000, Loss: 0.056156013160943985, Validation Loss: 0.11556954681873322\n",
      "Step 142/1000, Loss: 0.055954333394765854, Validation Loss: 0.11598898470401764\n",
      "Step 143/1000, Loss: 0.055756840854883194, Validation Loss: 0.1164124608039856\n",
      "Step 144/1000, Loss: 0.05556348338723183, Validation Loss: 0.11683975160121918\n",
      "Step 145/1000, Loss: 0.055374160408973694, Validation Loss: 0.11726989597082138\n",
      "Step 146/1000, Loss: 0.05518880859017372, Validation Loss: 0.11770129203796387\n",
      "Step 147/1000, Loss: 0.055007368326187134, Validation Loss: 0.11813195049762726\n",
      "Step 148/1000, Loss: 0.054829735308885574, Validation Loss: 0.11855980008840561\n",
      "Step 149/1000, Loss: 0.05465581268072128, Validation Loss: 0.1189831867814064\n",
      "Step 150/1000, Loss: 0.0544855110347271, Validation Loss: 0.11940083652734756\n",
      "Step 151/1000, Loss: 0.05431871861219406, Validation Loss: 0.11981217563152313\n",
      "Step 152/1000, Loss: 0.054155346006155014, Validation Loss: 0.12021718919277191\n",
      "Step 153/1000, Loss: 0.05399531498551369, Validation Loss: 0.12061607837677002\n",
      "Step 154/1000, Loss: 0.05383852869272232, Validation Loss: 0.12100918591022491\n",
      "Step 155/1000, Loss: 0.053684912621974945, Validation Loss: 0.12139678001403809\n",
      "Step 156/1000, Loss: 0.053534384816884995, Validation Loss: 0.12177875638008118\n",
      "Step 157/1000, Loss: 0.05338690057396889, Validation Loss: 0.12215504050254822\n",
      "Step 158/1000, Loss: 0.05324238911271095, Validation Loss: 0.12252520769834518\n",
      "Step 159/1000, Loss: 0.05310077592730522, Validation Loss: 0.12288901209831238\n",
      "Step 160/1000, Loss: 0.05296200141310692, Validation Loss: 0.1232464611530304\n",
      "Step 161/1000, Loss: 0.05282599478960037, Validation Loss: 0.12359778583049774\n",
      "Step 162/1000, Loss: 0.05269270017743111, Validation Loss: 0.12394348531961441\n",
      "Step 163/1000, Loss: 0.05256205424666405, Validation Loss: 0.12428413331508636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 164/1000, Loss: 0.05243400111794472, Validation Loss: 0.12462031096220016\n",
      "Step 165/1000, Loss: 0.052308470010757446, Validation Loss: 0.1249522939324379\n",
      "Step 166/1000, Loss: 0.05218540132045746, Validation Loss: 0.12528014183044434\n",
      "Step 167/1000, Loss: 0.05206475406885147, Validation Loss: 0.1256035566329956\n",
      "Step 168/1000, Loss: 0.05194643512368202, Validation Loss: 0.12592217326164246\n",
      "Step 169/1000, Loss: 0.05183041840791702, Validation Loss: 0.1262354999780655\n",
      "Step 170/1000, Loss: 0.0517166368663311, Validation Loss: 0.12654338777065277\n",
      "Step 171/1000, Loss: 0.05160503461956978, Validation Loss: 0.1268458068370819\n",
      "Step 172/1000, Loss: 0.051495566964149475, Validation Loss: 0.12714308500289917\n",
      "Step 173/1000, Loss: 0.051388174295425415, Validation Loss: 0.12743566930294037\n",
      "Step 174/1000, Loss: 0.05128280445933342, Validation Loss: 0.12772409617900848\n",
      "Step 175/1000, Loss: 0.05117940530180931, Validation Loss: 0.12800872325897217\n",
      "Step 176/1000, Loss: 0.05107791721820831, Validation Loss: 0.12828969955444336\n",
      "Step 177/1000, Loss: 0.050978291779756546, Validation Loss: 0.12856698036193848\n",
      "Step 178/1000, Loss: 0.05088048055768013, Validation Loss: 0.12884050607681274\n",
      "Step 179/1000, Loss: 0.05078442394733429, Validation Loss: 0.12911009788513184\n",
      "Step 180/1000, Loss: 0.050690073519945145, Validation Loss: 0.12937580049037933\n",
      "Step 181/1000, Loss: 0.05059738829731941, Validation Loss: 0.12963788211345673\n",
      "Early stopping triggered\n",
      "Step 1/1000, Loss: 0.3456535339355469, Validation Loss: 0.3709753155708313\n",
      "Step 2/1000, Loss: 0.3236796259880066, Validation Loss: 0.35775870084762573\n",
      "Step 3/1000, Loss: 0.3043597638607025, Validation Loss: 0.34609705209732056\n",
      "Step 4/1000, Loss: 0.2876531481742859, Validation Loss: 0.3362143337726593\n",
      "Step 5/1000, Loss: 0.2734825313091278, Validation Loss: 0.32886263728141785\n",
      "Step 6/1000, Loss: 0.2617032527923584, Validation Loss: 0.3237559199333191\n",
      "Step 7/1000, Loss: 0.25208720564842224, Validation Loss: 0.31998372077941895\n",
      "Step 8/1000, Loss: 0.2444128543138504, Validation Loss: 0.3168312609195709\n",
      "Step 9/1000, Loss: 0.2384532392024994, Validation Loss: 0.3138166069984436\n",
      "Step 10/1000, Loss: 0.23395143449306488, Validation Loss: 0.310623437166214\n",
      "Step 11/1000, Loss: 0.23062337934970856, Validation Loss: 0.30709561705589294\n",
      "Step 12/1000, Loss: 0.22818522155284882, Validation Loss: 0.303244411945343\n",
      "Step 13/1000, Loss: 0.2263881266117096, Validation Loss: 0.2991866171360016\n",
      "Step 14/1000, Loss: 0.22503937780857086, Validation Loss: 0.2950552999973297\n",
      "Step 15/1000, Loss: 0.22400151193141937, Validation Loss: 0.2909499704837799\n",
      "Step 16/1000, Loss: 0.2231808602809906, Validation Loss: 0.2869311571121216\n",
      "Step 17/1000, Loss: 0.22251558303833008, Validation Loss: 0.28303325176239014\n",
      "Step 18/1000, Loss: 0.22196465730667114, Validation Loss: 0.27927717566490173\n",
      "Step 19/1000, Loss: 0.22149914503097534, Validation Loss: 0.2756769061088562\n",
      "Step 20/1000, Loss: 0.22109617292881012, Validation Loss: 0.2722412943840027\n",
      "Step 21/1000, Loss: 0.22073732316493988, Validation Loss: 0.2689746022224426\n",
      "Step 22/1000, Loss: 0.22040952742099762, Validation Loss: 0.2658778727054596\n",
      "Step 23/1000, Loss: 0.2201029360294342, Validation Loss: 0.2629512846469879\n",
      "Step 24/1000, Loss: 0.21980614960193634, Validation Loss: 0.2601948082447052\n",
      "Step 25/1000, Loss: 0.2195034623146057, Validation Loss: 0.2576068937778473\n",
      "Step 26/1000, Loss: 0.21917763352394104, Validation Loss: 0.2551822364330292\n",
      "Step 27/1000, Loss: 0.21881592273712158, Validation Loss: 0.25291118025779724\n",
      "Step 28/1000, Loss: 0.21841512620449066, Validation Loss: 0.2507796883583069\n",
      "Step 29/1000, Loss: 0.2179824858903885, Validation Loss: 0.2487701177597046\n",
      "Step 30/1000, Loss: 0.2175321877002716, Validation Loss: 0.24686361849308014\n",
      "Step 31/1000, Loss: 0.21707889437675476, Validation Loss: 0.24504302442073822\n",
      "Step 32/1000, Loss: 0.21663355827331543, Validation Loss: 0.24329130351543427\n",
      "Step 33/1000, Loss: 0.21620334684848785, Validation Loss: 0.2415853589773178\n",
      "Step 34/1000, Loss: 0.21579429507255554, Validation Loss: 0.23989200592041016\n",
      "Step 35/1000, Loss: 0.21541103720664978, Validation Loss: 0.2381708323955536\n",
      "Step 36/1000, Loss: 0.2150523066520691, Validation Loss: 0.2363821268081665\n",
      "Step 37/1000, Loss: 0.21470807492733002, Validation Loss: 0.23449653387069702\n",
      "Step 38/1000, Loss: 0.21436388790607452, Validation Loss: 0.2325027734041214\n",
      "Step 39/1000, Loss: 0.21401019394397736, Validation Loss: 0.2304156869649887\n",
      "Step 40/1000, Loss: 0.21364642679691315, Validation Loss: 0.22828324139118195\n",
      "Step 41/1000, Loss: 0.21327657997608185, Validation Loss: 0.22618314623832703\n",
      "Step 42/1000, Loss: 0.21290215849876404, Validation Loss: 0.22419609129428864\n",
      "Step 43/1000, Loss: 0.21252357959747314, Validation Loss: 0.2223712056875229\n",
      "Step 44/1000, Loss: 0.21214552223682404, Validation Loss: 0.22071412205696106\n",
      "Step 45/1000, Loss: 0.2117720991373062, Validation Loss: 0.21920429170131683\n",
      "Step 46/1000, Loss: 0.21139571070671082, Validation Loss: 0.21783462166786194\n",
      "Step 47/1000, Loss: 0.2110019028186798, Validation Loss: 0.21664448082447052\n",
      "Step 48/1000, Loss: 0.2105863094329834, Validation Loss: 0.21571069955825806\n",
      "Step 49/1000, Loss: 0.21015174686908722, Validation Loss: 0.21508340537548065\n",
      "Step 50/1000, Loss: 0.20969915390014648, Validation Loss: 0.21471384167671204\n",
      "Step 51/1000, Loss: 0.20923417806625366, Validation Loss: 0.21446821093559265\n",
      "Step 52/1000, Loss: 0.20875667035579681, Validation Loss: 0.2142431139945984\n",
      "Step 53/1000, Loss: 0.2082546055316925, Validation Loss: 0.21406421065330505\n",
      "Step 54/1000, Loss: 0.20772486925125122, Validation Loss: 0.2140318751335144\n",
      "Step 55/1000, Loss: 0.20716284215450287, Validation Loss: 0.21415477991104126\n",
      "Step 56/1000, Loss: 0.2065703421831131, Validation Loss: 0.21432267129421234\n",
      "Step 57/1000, Loss: 0.20595481991767883, Validation Loss: 0.2144930064678192\n",
      "Step 58/1000, Loss: 0.2053125947713852, Validation Loss: 0.21480494737625122\n",
      "Step 59/1000, Loss: 0.20464037358760834, Validation Loss: 0.2153467833995819\n",
      "Step 60/1000, Loss: 0.20392751693725586, Validation Loss: 0.2159457504749298\n",
      "Step 61/1000, Loss: 0.203181192278862, Validation Loss: 0.2164597064256668\n",
      "Step 62/1000, Loss: 0.2024039328098297, Validation Loss: 0.2170340120792389\n",
      "Step 63/1000, Loss: 0.20158720016479492, Validation Loss: 0.21768221259117126\n",
      "Step 64/1000, Loss: 0.20071187615394592, Validation Loss: 0.21820707619190216\n",
      "Step 65/1000, Loss: 0.1997835785150528, Validation Loss: 0.21879133582115173\n",
      "Step 66/1000, Loss: 0.19881978631019592, Validation Loss: 0.21954450011253357\n",
      "Step 67/1000, Loss: 0.19780346751213074, Validation Loss: 0.22015880048274994\n",
      "Step 68/1000, Loss: 0.19672049582004547, Validation Loss: 0.22091512382030487\n",
      "Step 69/1000, Loss: 0.19559593498706818, Validation Loss: 0.2215614765882492\n",
      "Step 70/1000, Loss: 0.1944332718849182, Validation Loss: 0.2222464680671692\n",
      "Step 71/1000, Loss: 0.19320271909236908, Validation Loss: 0.22296738624572754\n",
      "Step 72/1000, Loss: 0.1919286996126175, Validation Loss: 0.2236817479133606\n",
      "Step 73/1000, Loss: 0.1906292587518692, Validation Loss: 0.2245067059993744\n",
      "Step 74/1000, Loss: 0.18927638232707977, Validation Loss: 0.22487807273864746\n",
      "Step 75/1000, Loss: 0.18792130053043365, Validation Loss: 0.22643984854221344\n",
      "Step 76/1000, Loss: 0.1868283599615097, Validation Loss: 0.22519823908805847\n",
      "Step 77/1000, Loss: 0.18616528809070587, Validation Loss: 0.22679272294044495\n",
      "Step 78/1000, Loss: 0.18368029594421387, Validation Loss: 0.22826358675956726\n",
      "Step 79/1000, Loss: 0.18341468274593353, Validation Loss: 0.22723788022994995\n",
      "Step 80/1000, Loss: 0.18084116280078888, Validation Loss: 0.22640468180179596\n",
      "Step 81/1000, Loss: 0.18048107624053955, Validation Loss: 0.22760900855064392\n",
      "Step 82/1000, Loss: 0.17797963321208954, Validation Loss: 0.22867920994758606\n",
      "Step 83/1000, Loss: 0.17756465077400208, Validation Loss: 0.22780391573905945\n",
      "Step 84/1000, Loss: 0.1750764399766922, Validation Loss: 0.2268020361661911\n",
      "Step 85/1000, Loss: 0.17462778091430664, Validation Loss: 0.22749337553977966\n",
      "Step 86/1000, Loss: 0.1721159964799881, Validation Loss: 0.22845324873924255\n",
      "Step 87/1000, Loss: 0.17183667421340942, Validation Loss: 0.22777658700942993\n",
      "Step 88/1000, Loss: 0.1692575067281723, Validation Loss: 0.2260787934064865\n",
      "Step 89/1000, Loss: 0.16973444819450378, Validation Loss: 0.225916787981987\n",
      "Step 90/1000, Loss: 0.16806095838546753, Validation Loss: 0.22710348665714264\n",
      "Step 91/1000, Loss: 0.16485008597373962, Validation Loss: 0.22725248336791992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 92/1000, Loss: 0.1638801544904709, Validation Loss: 0.22594967484474182\n",
      "Step 93/1000, Loss: 0.16205236315727234, Validation Loss: 0.22575229406356812\n",
      "Step 94/1000, Loss: 0.1602364480495453, Validation Loss: 0.22617006301879883\n",
      "Step 95/1000, Loss: 0.15924878418445587, Validation Loss: 0.22544771432876587\n",
      "Step 96/1000, Loss: 0.15706472098827362, Validation Loss: 0.22478286921977997\n",
      "Step 97/1000, Loss: 0.1559199094772339, Validation Loss: 0.22521553933620453\n",
      "Step 98/1000, Loss: 0.1539747565984726, Validation Loss: 0.2246689349412918\n",
      "Step 99/1000, Loss: 0.1521238088607788, Validation Loss: 0.22441257536411285\n",
      "Step 100/1000, Loss: 0.15051299333572388, Validation Loss: 0.22498853504657745\n",
      "Step 101/1000, Loss: 0.14903046190738678, Validation Loss: 0.22415801882743835\n",
      "Step 102/1000, Loss: 0.14677850902080536, Validation Loss: 0.2240181714296341\n",
      "Step 103/1000, Loss: 0.14584513008594513, Validation Loss: 0.224173903465271\n",
      "Step 104/1000, Loss: 0.14487574994564056, Validation Loss: 0.22407454252243042\n",
      "Step 105/1000, Loss: 0.14387230575084686, Validation Loss: 0.22371156513690948\n",
      "Step 106/1000, Loss: 0.14287790656089783, Validation Loss: 0.2235434502363205\n",
      "Step 107/1000, Loss: 0.14181825518608093, Validation Loss: 0.22349773347377777\n",
      "Step 108/1000, Loss: 0.14080975949764252, Validation Loss: 0.22321443259716034\n",
      "Step 109/1000, Loss: 0.13970912992954254, Validation Loss: 0.22288021445274353\n",
      "Step 110/1000, Loss: 0.13869528472423553, Validation Loss: 0.22275321185588837\n",
      "Step 111/1000, Loss: 0.13757812976837158, Validation Loss: 0.2225668877363205\n",
      "Step 112/1000, Loss: 0.13654720783233643, Validation Loss: 0.2222154438495636\n",
      "Step 113/1000, Loss: 0.13545723259449005, Validation Loss: 0.22200651466846466\n",
      "Step 114/1000, Loss: 0.13439665734767914, Validation Loss: 0.22189222276210785\n",
      "Step 115/1000, Loss: 0.13335725665092468, Validation Loss: 0.22162814438343048\n",
      "Step 116/1000, Loss: 0.13228508830070496, Validation Loss: 0.2214052379131317\n",
      "Step 117/1000, Loss: 0.13127584755420685, Validation Loss: 0.22133392095565796\n",
      "Step 118/1000, Loss: 0.1302325278520584, Validation Loss: 0.22120389342308044\n",
      "Step 119/1000, Loss: 0.12920653820037842, Validation Loss: 0.22106407582759857\n",
      "Step 120/1000, Loss: 0.12820130586624146, Validation Loss: 0.22104637324810028\n",
      "Step 121/1000, Loss: 0.12715916335582733, Validation Loss: 0.22104594111442566\n",
      "Step 122/1000, Loss: 0.12613394856452942, Validation Loss: 0.22105970978736877\n",
      "Step 123/1000, Loss: 0.12510952353477478, Validation Loss: 0.2211466133594513\n",
      "Step 124/1000, Loss: 0.12405192852020264, Validation Loss: 0.22126328945159912\n",
      "Step 125/1000, Loss: 0.123007632791996, Validation Loss: 0.2214101254940033\n",
      "Step 126/1000, Loss: 0.12197010964155197, Validation Loss: 0.22158800065517426\n",
      "Step 127/1000, Loss: 0.12091457843780518, Validation Loss: 0.22179259359836578\n",
      "Step 128/1000, Loss: 0.11987157166004181, Validation Loss: 0.2220124900341034\n",
      "Step 129/1000, Loss: 0.11885375529527664, Validation Loss: 0.22222721576690674\n",
      "Step 130/1000, Loss: 0.1178421899676323, Validation Loss: 0.22246067225933075\n",
      "Step 131/1000, Loss: 0.11683718860149384, Validation Loss: 0.22267816960811615\n",
      "Step 132/1000, Loss: 0.11585201323032379, Validation Loss: 0.22287020087242126\n",
      "Step 133/1000, Loss: 0.11488732695579529, Validation Loss: 0.22305060923099518\n",
      "Step 134/1000, Loss: 0.1139315515756607, Validation Loss: 0.22320951521396637\n",
      "Step 135/1000, Loss: 0.11297543346881866, Validation Loss: 0.22332975268363953\n",
      "Step 136/1000, Loss: 0.11202137172222137, Validation Loss: 0.22342263162136078\n",
      "Step 137/1000, Loss: 0.11107314378023148, Validation Loss: 0.22348929941654205\n",
      "Step 138/1000, Loss: 0.11013314872980118, Validation Loss: 0.22351735830307007\n",
      "Step 139/1000, Loss: 0.10920204222202301, Validation Loss: 0.22356364130973816\n",
      "Step 140/1000, Loss: 0.10828018933534622, Validation Loss: 0.2235463708639145\n",
      "Step 141/1000, Loss: 0.1073695570230484, Validation Loss: 0.22364339232444763\n",
      "Step 142/1000, Loss: 0.10647337138652802, Validation Loss: 0.2236335724592209\n",
      "Step 143/1000, Loss: 0.10559754818677902, Validation Loss: 0.22390547394752502\n",
      "Step 144/1000, Loss: 0.10475177317857742, Validation Loss: 0.22389066219329834\n",
      "Step 145/1000, Loss: 0.1039586141705513, Validation Loss: 0.22442620992660522\n",
      "Step 146/1000, Loss: 0.10305120795965195, Validation Loss: 0.22458946704864502\n",
      "Step 147/1000, Loss: 0.1021939143538475, Validation Loss: 0.2252712994813919\n",
      "Step 148/1000, Loss: 0.10136514157056808, Validation Loss: 0.2256292998790741\n",
      "Step 149/1000, Loss: 0.10056319832801819, Validation Loss: 0.226588636636734\n",
      "Step 150/1000, Loss: 0.09979026019573212, Validation Loss: 0.22696220874786377\n",
      "Step 151/1000, Loss: 0.09906305372714996, Validation Loss: 0.22810553014278412\n",
      "Step 152/1000, Loss: 0.09818218648433685, Validation Loss: 0.22879953682422638\n",
      "Step 153/1000, Loss: 0.09738358855247498, Validation Loss: 0.22971653938293457\n",
      "Step 154/1000, Loss: 0.09662943333387375, Validation Loss: 0.23079295456409454\n",
      "Early stopping triggered\n",
      "fixing (0,0,0) with 1/sqrt(x), r2=0.9404147863388062\n",
      "fixing (0,0,1) with erfc, r2=0.42655855417251587\n",
      "fixing (0,1,0) with erfc, r2=0.9342131018638611\n",
      "fixing (0,1,1) with erfc, r2=0.978168249130249\n",
      "fixing (1,0,0) with erfc, r2=0.9841532707214355\n",
      "fixing (1,0,1) with erfc, r2=0.9777220487594604\n",
      "fixing (1,1,0) with erfc, r2=0.7514686584472656\n",
      "fixing (1,1,1) with erfc, r2=0.6792137622833252\n",
      "fixing (2,0,0) with erfc, r2=0.8034475445747375\n",
      "fixing (2,1,0) with erfc, r2=0.9590514898300171\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sympy import symbols, sympify\n",
    "from scipy.special import erfc\n",
    "from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# Parameters\n",
    "alpha = 2.5e-6\n",
    "T0 = 20\n",
    "T1 = 40\n",
    "L = 4\n",
    "dx = 0.1\n",
    "dt = 1800\n",
    "tMax = 86400\n",
    "W = 30\n",
    "\n",
    "# Time and space grid\n",
    "x = np.arange(0, L + dx, dx)\n",
    "t = np.arange(dt, tMax + dt, dt)\n",
    "\n",
    "# Initialize temperature data storage\n",
    "TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# Compute temperature distribution and store results\n",
    "for k in range(len(t)):\n",
    "    TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# Prepare the data\n",
    "x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Normalize the data\n",
    "x_mean = np.mean(x_data, axis=0)\n",
    "x_std = np.std(x_data, axis=0)\n",
    "y_mean = np.mean(y_data)\n",
    "y_std = np.std(y_data)\n",
    "\n",
    "x_data_normalized = (x_data - x_mean) / x_std\n",
    "y_data_normalized = (y_data - y_mean) / y_std\n",
    "# x_data_normalized = x_data\n",
    "# y_data_normalized = y_data\n",
    "\n",
    "# Convert to tensors\n",
    "x_tensor = torch.tensor(x_data_normalized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_data_normalized, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "train_size = int(0.8 * len(x_tensor))\n",
    "test_size = len(x_tensor) - train_size\n",
    "train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# Create the dataset dictionary as expected by KAN\n",
    "dataset = {\n",
    "    'train_input': train_input,\n",
    "    'train_label': train_label,\n",
    "    'test_input': test_input,\n",
    "    'test_label': test_label\n",
    "}\n",
    "\n",
    "# Add erfc to the symbolic library\n",
    "add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# Function to train KAN with early stopping and learning rate scheduling\n",
    "def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "    current_lr = initial_lr\n",
    "\n",
    "    for step in range(steps):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(dataset['train_input'])\n",
    "            loss = criterion(outputs, dataset['train_label'])\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(closure).item()\n",
    "\n",
    "        # Validation step\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(dataset['test_input'])\n",
    "            val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "        print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        if step % patience == 0 and step > 0:\n",
    "            current_lr = max(min_lr, current_lr * lr_decay)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "\n",
    "        # Check for NaN values\n",
    "        if np.isnan(loss) or np.isnan(val_loss):\n",
    "            print(\"NaN detected, stopping training\")\n",
    "            break\n",
    "\n",
    "    # Load the best model\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "# Initial training with a coarse grid\n",
    "initial_grid = 3\n",
    "model = KAN(width=[2, 2, 2, 1], grid=initial_grid, k=3, seed=0)\n",
    "train_with_early_stopping(model, dataset, steps=1000, patience=100, initial_lr=0.002)\n",
    "\n",
    "# Iteratively refine the grid and retrain the model\n",
    "grids = [5, 10, 20]\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for grid in grids:\n",
    "    new_model = KAN(width=[2, 2, 2, 1], grid=grid, k=3).initialize_from_another_model(model, dataset['train_input'])\n",
    "    train_with_early_stopping(new_model, dataset, steps=1000, patience=100, initial_lr=0.002)\n",
    "    model = new_model  # Update the model to the new refined grid model\n",
    "\n",
    "    # Collect training and test losses\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(dataset['train_input'])\n",
    "        train_loss = torch.nn.functional.mse_loss(train_outputs, dataset['train_label']).item()\n",
    "        test_outputs = model(dataset['test_input'])\n",
    "        test_loss = torch.nn.functional.mse_loss(test_outputs, dataset['test_label']).item()\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "# Automatically set activation functions to be symbolic\n",
    "lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "model.auto_symbolic(lib=lib)\n",
    "\n",
    "# Prune the model\n",
    "model = model.prune()\n",
    "\n",
    "# Obtain the symbolic formula and denormalize it\n",
    "symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# symbolic_formula_denormalized = sympify(symbolic_formula.replace('x_1', '(x_1*{:.6f}+{:.6f})'.format(x_std[0], x_mean[0]))\n",
    "#                                         .replace('x_2', '(x_2*{:.6f}+{:.6f})'.format(x_std[1], x_mean[1])))\n",
    "# symbolic_formula_denormalized = (symbolic_formula_denormalized * y_std + y_mean).simplify()\n",
    "\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula_denormalized)\n",
    "\n",
    "# Create output directory for plots\n",
    "outputDir = 'TemperaturePlots'\n",
    "if not os.path.exists(outputDir):\n",
    "    os.makedirs(outputDir)\n",
    "\n",
    "# Plot predicted temperature distribution\n",
    "x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "x_test_normalized = (x_test - x_mean) / x_std\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "predicted_temperature = model(x_test_tensor).detach().numpy().flatten() * y_std + y_mean\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, predicted_temperature, label='Predicted')\n",
    "plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "plt.xlabel('Position along the wall thickness (m)')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and test losses over different grid refinements\n",
    "plt.figure()\n",
    "plt.plot(grids, train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(grids, test_losses, marker='o', label='Test Loss')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Grid Size')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Training and Test Losses over Grid Refinements')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14422989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered Symbolic Formula (Normalized):\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 1.81 \\operatorname{erfc}{\\left(0.1 \\operatorname{erfc}{\\left(- 2.85 \\operatorname{erfc}{\\left(9.88 - 8.02 x_{1} \\right)} + 84.78 \\operatorname{erfc}{\\left(1.46 x_{2} - 0.2 \\right)} - 0.92 \\right)} - 0.25 \\operatorname{erfc}{\\left(0.12 \\operatorname{erfc}{\\left(9.0 x_{2} - 10.0 \\right)} - 2.58 + \\frac{2.28}{\\sqrt{0.38 x_{1} + 1}} \\right)} - 0.58 \\right)} - 1.22 \\operatorname{erfc}{\\left(- 0.69 \\operatorname{erfc}{\\left(2.85 \\operatorname{erfc}{\\left(9.88 - 8.02 x_{1} \\right)} - 84.78 \\operatorname{erfc}{\\left(1.46 x_{2} - 0.2 \\right)} + 1.12 \\right)} - 2.04 \\operatorname{erfc}{\\left(0.14 \\operatorname{erfc}{\\left(9.0 x_{2} - 10.0 \\right)} - 2.97 + \\frac{2.66}{\\sqrt{0.38 x_{1} + 1}} \\right)} + 1.12 \\right)} + 5.19$"
      ],
      "text/plain": [
       "-1.81*erfc(0.1*erfc(-2.85*erfc(9.88 - 8.02*x_1) + 84.78*erfc(1.46*x_2 - 0.2) - 0.92) - 0.25*erfc(0.12*erfc(9.0*x_2 - 10.0) - 2.58 + 2.28/sqrt(0.38*x_1 + 1)) - 0.58) - 1.22*erfc(-0.69*erfc(2.85*erfc(9.88 - 8.02*x_1) - 84.78*erfc(1.46*x_2 - 0.2) + 1.12) - 2.04*erfc(0.14*erfc(9.0*x_2 - 10.0) - 2.97 + 2.66/sqrt(0.38*x_1 + 1)) + 1.12) + 5.19"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Discovered Symbolic Formula (Normalized):\")\n",
    "model.symbolic_formula()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd4dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Discovered Symbolic Formula (Original):\")\n",
    "# print(original_formula)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
