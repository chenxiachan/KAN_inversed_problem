{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55f8a172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 固定壁面温度的一维稳态计算\n",
    "\n",
    "# % 参数设置\n",
    "# L = 0.24;          % 墙体的厚度，单位为米 (240mm)\n",
    "# W = 1.0;           % 墙体的长度，单位为米 (1000mm)\n",
    "# alpha = 2.5*10^(-6);      % 热扩散率，单位为平方米每秒\n",
    "# T0 = 20;           % x=0处的温度，单位为摄氏度\n",
    "# TL = 40;           % x=L处的温度，单位为摄氏度\n",
    "# Ti = 25;           % 初始平均温度，单位为摄氏度\n",
    "\n",
    "# % 计算温度分布\n",
    "# x = linspace(0, L, 100); % 在墙的厚度方向生成100个点以计算温度分布\n",
    "# T = T0 + (TL - T0) * (x / L); % 线性温度分布计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "553725ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6  # Thermal diffusivity of the soil in m^2/s\n",
    "# T0 = 20        # Initial surface temperature in Celsius\n",
    "# T1 = 40        # Surface temperature after change in Celsius\n",
    "# L = 4          # Calculation thickness in meters\n",
    "# dx = 0.1       # Spatial step in meters\n",
    "# dt = 1800      # Time step in seconds\n",
    "# tMax = 86400   # Simulation duration in seconds (1 day)\n",
    "# W = 30         # Image width for plotting in meters\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)  # Space grid\n",
    "# t = np.arange(dt, tMax + dt, dt)  # Time grid, start from dt to avoid divide by zero\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Normalize the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# x_mean = np.mean(x_data, axis=0)\n",
    "# x_std = np.std(x_data, axis=0) + 1e-8  # Add small constant to avoid division by zero\n",
    "# x_data_norm = (x_data - x_mean) / x_std\n",
    "\n",
    "# y_mean = np.mean(y_data)\n",
    "# y_std = np.std(y_data) + 1e-8  # Add small constant to avoid division by zero\n",
    "# y_data_norm = (y_data - y_mean) / y_std\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data_norm, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data_norm, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Initialize KAN with adjusted parameters\n",
    "# model = KAN(width=[2, 10, 1], grid=20, k=3, seed=0)  # Input layer now has 2 neurons\n",
    "\n",
    "# # Plot KAN at initialization\n",
    "# model(dataset['train_input'])\n",
    "# model.plot(beta=100)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Train the model with early stopping and learning rate scheduling\n",
    "# train_with_early_stopping(model, dataset, steps=500, patience=100, initial_lr=0.0025)\n",
    "\n",
    "# # Plot trained KAN\n",
    "# model.plot()\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "# # # Continue training to almost machine precision with conservative settings\n",
    "# # train_with_early_stopping(model, dataset, steps=200, patience=50, initial_lr=0.00001)\n",
    "\n",
    "# # Obtain the symbolic formula in terms of normalized data\n",
    "# symbolic_formula_normalized = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula (Normalized):\")\n",
    "# print(symbolic_formula_normalized)\n",
    "\n",
    "# # Reverse normalization for symbolic formula using sympy\n",
    "# x_sym, t_sym = symbols('x t')\n",
    "# normalized_formula = sympify(symbolic_formula_normalized)\n",
    "\n",
    "# # Replace normalized variables with original scale variables\n",
    "# original_formula = normalized_formula * (y_std / x_std[0]) + (y_mean - y_std / x_std[0] * x_mean[0])\n",
    "\n",
    "# print(\"Discovered Symbolic Formula (Original):\")\n",
    "# print(original_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_norm = (x_test - x_mean) / x_std\n",
    "# x_test_tensor = torch.tensor(x_test_norm, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "# predicted_temperature = predicted_temperature * y_std + y_mean  # Denormalize the prediction\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4c7b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6  # Thermal diffusivity of the soil in m^2/s\n",
    "# T0 = 20        # Initial surface temperature in Celsius\n",
    "# T1 = 40        # Surface temperature after change in Celsius\n",
    "# L = 4          # Calculation thickness in meters\n",
    "# dx = 0.1       # Spatial step in meters\n",
    "# dt = 1800      # Time step in seconds\n",
    "# tMax = 86400   # Simulation duration in seconds (1 day)\n",
    "# W = 30         # Image width for plotting in meters\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)  # Space grid\n",
    "# t = np.arange(dt, tMax + dt, dt)  # Time grid, start from dt to avoid divide by zero\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Prepare the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Initialize KAN with adjusted parameters\n",
    "# model = KAN(width=[2, 20, 1], grid=20, k=3, seed=0)  # Input layer now has 2 neurons\n",
    "\n",
    "# # Plot KAN at initialization\n",
    "# model(dataset['train_input'])\n",
    "# model.plot(beta=100)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Train the model with early stopping and learning rate scheduling\n",
    "# train_with_early_stopping(model, dataset, steps=500, patience=100, initial_lr=0.0025)\n",
    "\n",
    "# # Plot trained KAN\n",
    "# model.plot()\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "# # # Continue training to almost machine precision with conservative settings\n",
    "# # train_with_early_stopping(model, dataset, steps=200, patience=50, initial_lr=0.00001)\n",
    "\n",
    "# # Obtain the symbolic formula\n",
    "# symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82dd1b83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# from sympy import symbols, sympify\n",
    "# from scipy.special import erfc\n",
    "# from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# # Parameters\n",
    "# alpha = 2.5e-6\n",
    "# T0 = 20\n",
    "# T1 = 40\n",
    "# L = 4\n",
    "# dx = 0.1\n",
    "# dt = 1800\n",
    "# tMax = 86400\n",
    "# W = 30\n",
    "\n",
    "# # Time and space grid\n",
    "# x = np.arange(0, L + dx, dx)\n",
    "# t = np.arange(dt, tMax + dt, dt)\n",
    "\n",
    "# # Initialize temperature data storage\n",
    "# TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# # Compute temperature distribution and store results\n",
    "# for k in range(len(t)):\n",
    "#     TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# # Prepare the data\n",
    "# x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "# y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Convert to tensors\n",
    "# x_tensor = torch.tensor(x_data, dtype=torch.float32)\n",
    "# y_tensor = torch.tensor(y_data, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# # Split dataset into training and test sets\n",
    "# train_size = int(0.8 * len(x_tensor))\n",
    "# test_size = len(x_tensor) - train_size\n",
    "# train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "# train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# # Create the dataset dictionary as expected by KAN\n",
    "# dataset = {\n",
    "#     'train_input': train_input,\n",
    "#     'train_label': train_label,\n",
    "#     'test_input': test_input,\n",
    "#     'test_label': test_label\n",
    "# }\n",
    "\n",
    "# # Add erfc to the symbolic library\n",
    "# add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# # Function to train KAN with early stopping and learning rate scheduling\n",
    "# def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "#     criterion = torch.nn.MSELoss()\n",
    "\n",
    "#     best_loss = float('inf')\n",
    "#     best_model = None\n",
    "#     patience_counter = 0\n",
    "#     current_lr = initial_lr\n",
    "\n",
    "#     for step in range(steps):\n",
    "#         def closure():\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(dataset['train_input'])\n",
    "#             loss = criterion(outputs, dataset['train_label'])\n",
    "#             loss.backward()\n",
    "\n",
    "#             # Gradient clipping\n",
    "#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "#             return loss\n",
    "\n",
    "#         loss = optimizer.step(closure).item()\n",
    "\n",
    "#         # Validation step\n",
    "#         with torch.no_grad():\n",
    "#             val_outputs = model(dataset['test_input'])\n",
    "#             val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "#         print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "#         # Early stopping logic\n",
    "#         if val_loss < best_loss:\n",
    "#             best_loss = val_loss\n",
    "#             best_model = model.state_dict()\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "\n",
    "#         if patience_counter >= patience:\n",
    "#             print(\"Early stopping triggered\")\n",
    "#             break\n",
    "\n",
    "#         # Learning rate scheduling\n",
    "#         if step % patience == 0 and step > 0:\n",
    "#             current_lr = max(min_lr, current_lr * lr_decay)\n",
    "#             for param_group in optimizer.param_groups:\n",
    "#                 param_group['lr'] = current_lr\n",
    "\n",
    "#         # Check for NaN values\n",
    "#         if np.isnan(loss) or np.isnan(val_loss):\n",
    "#             print(\"NaN detected, stopping training\")\n",
    "#             break\n",
    "\n",
    "#     # Load the best model\n",
    "#     if best_model is not None:\n",
    "#         model.load_state_dict(best_model)\n",
    "\n",
    "# # Initial training with a coarse grid\n",
    "# initial_grid = 3\n",
    "# model = KAN(width=[2, 2, 2, 1], grid=initial_grid, k=3, seed=0)\n",
    "# train_with_early_stopping(model, dataset, steps=1000, patience=100, initial_lr=0.002)\n",
    "\n",
    "# # Iteratively refine the grid and retrain the model\n",
    "# grids = [5, 10, 20, 50, 100]\n",
    "# train_losses = []\n",
    "# test_losses = []\n",
    "\n",
    "# for grid in grids:\n",
    "#     new_model = KAN(width=[2, 2, 2, 1], grid=grid, k=3).initialize_from_another_model(model, dataset['train_input'])\n",
    "#     train_with_early_stopping(new_model, dataset, steps=500, patience=100, initial_lr=0.00005)\n",
    "#     model = new_model  # Update the model to the new refined grid model\n",
    "\n",
    "#     # Collect training and test losses\n",
    "#     with torch.no_grad():\n",
    "#         train_outputs = model(dataset['train_input'])\n",
    "#         train_loss = torch.nn.functional.mse_loss(train_outputs, dataset['train_label']).item()\n",
    "#         test_outputs = model(dataset['test_input'])\n",
    "#         test_loss = torch.nn.functional.mse_loss(test_outputs, dataset['test_label']).item()\n",
    "#         train_losses.append(train_loss)\n",
    "#         test_losses.append(test_loss)\n",
    "\n",
    "# # Automatically set activation functions to be symbolic\n",
    "# lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "# model.auto_symbolic(lib=lib)\n",
    "\n",
    "# # Prune the model\n",
    "# model = model.prune()\n",
    "\n",
    "# # Obtain the symbolic formula\n",
    "# symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula)\n",
    "\n",
    "# # Create output directory for plots\n",
    "# outputDir = 'TemperaturePlots'\n",
    "# if not os.path.exists(outputDir):\n",
    "#     os.makedirs(outputDir)\n",
    "\n",
    "# # Plot predicted temperature distribution\n",
    "# x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(x, predicted_temperature, label='Predicted')\n",
    "# plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "# plt.xlabel('Position along the wall thickness (m)')\n",
    "# plt.ylabel('Temperature (°C)')\n",
    "# plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "# plt.show()\n",
    "\n",
    "# # Plot training and test losses over different grid refinements\n",
    "# plt.figure()\n",
    "# plt.plot(grids, train_losses, marker='o', label='Train Loss')\n",
    "# plt.plot(grids, test_losses, marker='o', label='Test Loss')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel('Grid Size')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.title('Training and Test Losses over Grid Refinements')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c6289c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1/1000, Loss: 1.006316065788269, Validation Loss: 1.371787428855896\n",
      "Step 2/1000, Loss: 1.0023128986358643, Validation Loss: 1.359310269355774\n",
      "Step 3/1000, Loss: 0.9985527396202087, Validation Loss: 1.3476669788360596\n",
      "Step 4/1000, Loss: 0.9950290322303772, Validation Loss: 1.3368865251541138\n",
      "Step 5/1000, Loss: 0.9917342066764832, Validation Loss: 1.3269546031951904\n",
      "Step 6/1000, Loss: 0.9886600375175476, Validation Loss: 1.3178017139434814\n",
      "Step 7/1000, Loss: 0.9857967495918274, Validation Loss: 1.3093459606170654\n",
      "Step 8/1000, Loss: 0.9831345081329346, Validation Loss: 1.301518440246582\n",
      "Step 9/1000, Loss: 0.9806627631187439, Validation Loss: 1.294265866279602\n",
      "Step 10/1000, Loss: 0.9783706068992615, Validation Loss: 1.2875436544418335\n",
      "Step 11/1000, Loss: 0.97624671459198, Validation Loss: 1.2813127040863037\n",
      "Step 12/1000, Loss: 0.9742792844772339, Validation Loss: 1.2755358219146729\n",
      "Step 13/1000, Loss: 0.9724573493003845, Validation Loss: 1.2701786756515503\n",
      "Step 14/1000, Loss: 0.9707695245742798, Validation Loss: 1.2652071714401245\n",
      "Step 15/1000, Loss: 0.969205379486084, Validation Loss: 1.2605899572372437\n",
      "Step 16/1000, Loss: 0.9677556157112122, Validation Loss: 1.2562960386276245\n",
      "Step 17/1000, Loss: 0.9664110541343689, Validation Loss: 1.2522974014282227\n",
      "Step 18/1000, Loss: 0.9651638865470886, Validation Loss: 1.2485681772232056\n",
      "Step 19/1000, Loss: 0.964007556438446, Validation Loss: 1.2450841665267944\n",
      "Step 20/1000, Loss: 0.9629359245300293, Validation Loss: 1.241824746131897\n",
      "Step 21/1000, Loss: 0.9619439840316772, Validation Loss: 1.2387703657150269\n",
      "Step 22/1000, Loss: 0.9610271453857422, Validation Loss: 1.2359050512313843\n",
      "Step 23/1000, Loss: 0.9601820707321167, Validation Loss: 1.233214259147644\n",
      "Step 24/1000, Loss: 0.9594050049781799, Validation Loss: 1.2306855916976929\n",
      "Step 25/1000, Loss: 0.9586933255195618, Validation Loss: 1.2283083200454712\n",
      "Step 26/1000, Loss: 0.9580438733100891, Validation Loss: 1.2260736227035522\n",
      "Step 27/1000, Loss: 0.9574540853500366, Validation Loss: 1.2239737510681152\n",
      "Step 28/1000, Loss: 0.9569212198257446, Validation Loss: 1.2220022678375244\n",
      "Step 29/1000, Loss: 0.9564422369003296, Validation Loss: 1.2201539278030396\n",
      "Step 30/1000, Loss: 0.9560143351554871, Validation Loss: 1.2184239625930786\n",
      "Step 31/1000, Loss: 0.9556341171264648, Validation Loss: 1.216808557510376\n",
      "Step 32/1000, Loss: 0.9552984833717346, Validation Loss: 1.2153042554855347\n",
      "Step 33/1000, Loss: 0.9550034999847412, Validation Loss: 1.213908314704895\n",
      "Step 34/1000, Loss: 0.9547455906867981, Validation Loss: 1.2126178741455078\n",
      "Step 35/1000, Loss: 0.9545210003852844, Validation Loss: 1.211430311203003\n",
      "Step 36/1000, Loss: 0.9543253779411316, Validation Loss: 1.2103431224822998\n",
      "Step 37/1000, Loss: 0.9541550874710083, Validation Loss: 1.2093533277511597\n",
      "Step 38/1000, Loss: 0.9540061950683594, Validation Loss: 1.2084585428237915\n",
      "Step 39/1000, Loss: 0.9538751244544983, Validation Loss: 1.2076553106307983\n",
      "Step 40/1000, Loss: 0.9537583589553833, Validation Loss: 1.2069404125213623\n",
      "Step 41/1000, Loss: 0.9536526203155518, Validation Loss: 1.2063097953796387\n",
      "Step 42/1000, Loss: 0.9535552859306335, Validation Loss: 1.2057592868804932\n",
      "Step 43/1000, Loss: 0.9534639120101929, Validation Loss: 1.2052843570709229\n",
      "Step 44/1000, Loss: 0.953376054763794, Validation Loss: 1.204879879951477\n",
      "Step 45/1000, Loss: 0.9532900452613831, Validation Loss: 1.2045402526855469\n",
      "Step 46/1000, Loss: 0.9532045722007751, Validation Loss: 1.2042597532272339\n",
      "Step 47/1000, Loss: 0.9531185030937195, Validation Loss: 1.204032063484192\n",
      "Step 48/1000, Loss: 0.9530307054519653, Validation Loss: 1.203850507736206\n",
      "Step 49/1000, Loss: 0.9529401659965515, Validation Loss: 1.203708529472351\n",
      "Step 50/1000, Loss: 0.9528465867042542, Validation Loss: 1.2035988569259644\n",
      "Step 51/1000, Loss: 0.9527490139007568, Validation Loss: 1.2035146951675415\n",
      "Step 52/1000, Loss: 0.9526470303535461, Validation Loss: 1.2034488916397095\n",
      "Step 53/1000, Loss: 0.95253986120224, Validation Loss: 1.2033944129943848\n",
      "Step 54/1000, Loss: 0.9524268507957458, Validation Loss: 1.2033445835113525\n",
      "Step 55/1000, Loss: 0.9523075222969055, Validation Loss: 1.2032928466796875\n",
      "Step 56/1000, Loss: 0.9521806836128235, Validation Loss: 1.203233003616333\n",
      "Step 57/1000, Loss: 0.9520459175109863, Validation Loss: 1.2031595706939697\n",
      "Step 58/1000, Loss: 0.9519021511077881, Validation Loss: 1.2030670642852783\n",
      "Step 59/1000, Loss: 0.9517484307289124, Validation Loss: 1.2029513120651245\n",
      "Step 60/1000, Loss: 0.951583981513977, Validation Loss: 1.202807903289795\n",
      "Step 61/1000, Loss: 0.951407790184021, Validation Loss: 1.2026338577270508\n",
      "Step 62/1000, Loss: 0.9512190222740173, Validation Loss: 1.2024258375167847\n",
      "Step 63/1000, Loss: 0.9510166049003601, Validation Loss: 1.202182650566101\n",
      "Step 64/1000, Loss: 0.9507996439933777, Validation Loss: 1.2019020318984985\n",
      "Step 65/1000, Loss: 0.9505671858787537, Validation Loss: 1.2015831470489502\n",
      "Step 66/1000, Loss: 0.9503183364868164, Validation Loss: 1.2012255191802979\n",
      "Step 67/1000, Loss: 0.9500519037246704, Validation Loss: 1.2008287906646729\n",
      "Step 68/1000, Loss: 0.9497670531272888, Validation Loss: 1.2003929615020752\n",
      "Step 69/1000, Loss: 0.9494625926017761, Validation Loss: 1.1999183893203735\n",
      "Step 70/1000, Loss: 0.9491373300552368, Validation Loss: 1.1994054317474365\n",
      "Step 71/1000, Loss: 0.9487900137901306, Validation Loss: 1.1988542079925537\n",
      "Step 72/1000, Loss: 0.9484193325042725, Validation Loss: 1.1982648372650146\n",
      "Step 73/1000, Loss: 0.948023796081543, Validation Loss: 1.197637677192688\n",
      "Step 74/1000, Loss: 0.9476017355918884, Validation Loss: 1.1969720125198364\n",
      "Step 75/1000, Loss: 0.9471518397331238, Validation Loss: 1.1962674856185913\n",
      "Step 76/1000, Loss: 0.9466719627380371, Validation Loss: 1.1955229043960571\n",
      "Step 77/1000, Loss: 0.9461601376533508, Validation Loss: 1.194736361503601\n",
      "Step 78/1000, Loss: 0.9456146359443665, Validation Loss: 1.1939059495925903\n",
      "Step 79/1000, Loss: 0.9450330138206482, Validation Loss: 1.1930289268493652\n",
      "Step 80/1000, Loss: 0.9444131851196289, Validation Loss: 1.1921019554138184\n",
      "Step 81/1000, Loss: 0.9437525868415833, Validation Loss: 1.1911208629608154\n",
      "Step 82/1000, Loss: 0.9430487155914307, Validation Loss: 1.1900814771652222\n",
      "Step 83/1000, Loss: 0.9422988295555115, Validation Loss: 1.1889783143997192\n",
      "Step 84/1000, Loss: 0.9415000081062317, Validation Loss: 1.1878058910369873\n",
      "Step 85/1000, Loss: 0.9406492710113525, Validation Loss: 1.1865577697753906\n",
      "Step 86/1000, Loss: 0.9397430419921875, Validation Loss: 1.185227394104004\n",
      "Step 87/1000, Loss: 0.9387778043746948, Validation Loss: 1.1838072538375854\n",
      "Step 88/1000, Loss: 0.9377497434616089, Validation Loss: 1.182289958000183\n",
      "Step 89/1000, Loss: 0.9366546869277954, Validation Loss: 1.180667519569397\n",
      "Step 90/1000, Loss: 0.9354879260063171, Validation Loss: 1.178931713104248\n",
      "Step 91/1000, Loss: 0.9342446327209473, Validation Loss: 1.1770737171173096\n",
      "Step 92/1000, Loss: 0.9329196214675903, Validation Loss: 1.1750850677490234\n",
      "Step 93/1000, Loss: 0.931506872177124, Validation Loss: 1.172956943511963\n",
      "Step 94/1000, Loss: 0.9300000667572021, Validation Loss: 1.170680284500122\n",
      "Step 95/1000, Loss: 0.9283928275108337, Validation Loss: 1.1682463884353638\n",
      "Step 96/1000, Loss: 0.9266780614852905, Validation Loss: 1.165645956993103\n",
      "Step 97/1000, Loss: 0.9248483777046204, Validation Loss: 1.1628704071044922\n",
      "Step 98/1000, Loss: 0.9228960275650024, Validation Loss: 1.1599104404449463\n",
      "Step 99/1000, Loss: 0.9208124279975891, Validation Loss: 1.1567572355270386\n",
      "Step 100/1000, Loss: 0.9185892343521118, Validation Loss: 1.1534019708633423\n",
      "Step 101/1000, Loss: 0.9162172675132751, Validation Loss: 1.149835467338562\n",
      "Step 102/1000, Loss: 0.9136872291564941, Validation Loss: 1.1479629278182983\n",
      "Step 103/1000, Loss: 0.9123519659042358, Validation Loss: 1.1460212469100952\n",
      "Step 104/1000, Loss: 0.9109598398208618, Validation Loss: 1.1440114974975586\n",
      "Step 105/1000, Loss: 0.9095107913017273, Validation Loss: 1.1419342756271362\n",
      "Step 106/1000, Loss: 0.9080047607421875, Validation Loss: 1.1397897005081177\n",
      "Step 107/1000, Loss: 0.9064410924911499, Validation Loss: 1.1375783681869507\n",
      "Step 108/1000, Loss: 0.904819905757904, Validation Loss: 1.1353000402450562\n",
      "Step 109/1000, Loss: 0.9031403660774231, Validation Loss: 1.1329554319381714\n",
      "Step 110/1000, Loss: 0.9014021158218384, Validation Loss: 1.130543828010559\n",
      "Step 111/1000, Loss: 0.8996041417121887, Validation Loss: 1.128065586090088\n",
      "Step 112/1000, Loss: 0.8977463245391846, Validation Loss: 1.1255199909210205\n",
      "Step 113/1000, Loss: 0.8958274722099304, Validation Loss: 1.1229074001312256\n",
      "Step 114/1000, Loss: 0.8938472867012024, Validation Loss: 1.1202269792556763\n",
      "Step 115/1000, Loss: 0.8918049931526184, Validation Loss: 1.1174790859222412\n",
      "Step 116/1000, Loss: 0.8897001147270203, Validation Loss: 1.114662528038025\n",
      "Step 117/1000, Loss: 0.8875318765640259, Validation Loss: 1.1117775440216064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 118/1000, Loss: 0.8852999806404114, Validation Loss: 1.1088236570358276\n",
      "Step 119/1000, Loss: 0.8830040097236633, Validation Loss: 1.1058006286621094\n",
      "Step 120/1000, Loss: 0.8806434869766235, Validation Loss: 1.102708101272583\n",
      "Step 121/1000, Loss: 0.878217875957489, Validation Loss: 1.0995458364486694\n",
      "Step 122/1000, Loss: 0.8757271766662598, Validation Loss: 1.0963135957717896\n",
      "Step 123/1000, Loss: 0.8731709718704224, Validation Loss: 1.0930109024047852\n",
      "Step 124/1000, Loss: 0.8705492615699768, Validation Loss: 1.0896377563476562\n",
      "Step 125/1000, Loss: 0.8678617477416992, Validation Loss: 1.086193561553955\n",
      "Step 126/1000, Loss: 0.8651086091995239, Validation Loss: 1.0826783180236816\n",
      "Step 127/1000, Loss: 0.8622896671295166, Validation Loss: 1.079092025756836\n",
      "Step 128/1000, Loss: 0.8594049215316772, Validation Loss: 1.0754340887069702\n",
      "Step 129/1000, Loss: 0.856454610824585, Validation Loss: 1.0717042684555054\n",
      "Step 130/1000, Loss: 0.8534388542175293, Validation Loss: 1.0679025650024414\n",
      "Step 131/1000, Loss: 0.850357711315155, Validation Loss: 1.0640286207199097\n",
      "Step 132/1000, Loss: 0.847211480140686, Validation Loss: 1.0600817203521729\n",
      "Step 133/1000, Loss: 0.8440003395080566, Validation Loss: 1.0560617446899414\n",
      "Step 134/1000, Loss: 0.840724527835846, Validation Loss: 1.0519680976867676\n",
      "Step 135/1000, Loss: 0.8373843431472778, Validation Loss: 1.047800064086914\n",
      "Step 136/1000, Loss: 0.8339800834655762, Validation Loss: 1.0435569286346436\n",
      "Step 137/1000, Loss: 0.8305119872093201, Validation Loss: 1.0392374992370605\n",
      "Step 138/1000, Loss: 0.8269802927970886, Validation Loss: 1.0348412990570068\n",
      "Step 139/1000, Loss: 0.8233853578567505, Validation Loss: 1.030367136001587\n",
      "Step 140/1000, Loss: 0.8197274208068848, Validation Loss: 1.0258136987686157\n",
      "Step 141/1000, Loss: 0.8160068988800049, Validation Loss: 1.021180272102356\n",
      "Step 142/1000, Loss: 0.8122239112854004, Validation Loss: 1.0164657831192017\n",
      "Step 143/1000, Loss: 0.8083789944648743, Validation Loss: 1.011669397354126\n",
      "Step 144/1000, Loss: 0.8044723868370056, Validation Loss: 1.0067905187606812\n",
      "Step 145/1000, Loss: 0.8005044460296631, Validation Loss: 1.0018283128738403\n",
      "Step 146/1000, Loss: 0.7964754700660706, Validation Loss: 0.9967820048332214\n",
      "Step 147/1000, Loss: 0.7923859357833862, Validation Loss: 0.9916514158248901\n",
      "Step 148/1000, Loss: 0.7882363200187683, Validation Loss: 0.9864358901977539\n",
      "Step 149/1000, Loss: 0.7840270400047302, Validation Loss: 0.9811349511146545\n",
      "Step 150/1000, Loss: 0.7797586917877197, Validation Loss: 0.9757480025291443\n",
      "Step 151/1000, Loss: 0.775431752204895, Validation Loss: 0.970274806022644\n",
      "Step 152/1000, Loss: 0.7710465788841248, Validation Loss: 0.9647147059440613\n",
      "Step 153/1000, Loss: 0.7666038870811462, Validation Loss: 0.9590675830841064\n",
      "Step 154/1000, Loss: 0.762104332447052, Validation Loss: 0.9533331394195557\n",
      "Step 155/1000, Loss: 0.7575485110282898, Validation Loss: 0.9475111365318298\n",
      "Step 156/1000, Loss: 0.7529371380805969, Validation Loss: 0.9416015148162842\n",
      "Step 157/1000, Loss: 0.7482709288597107, Validation Loss: 0.9356046319007874\n",
      "Step 158/1000, Loss: 0.7435507774353027, Validation Loss: 0.9295211434364319\n",
      "Step 159/1000, Loss: 0.7387774586677551, Validation Loss: 0.9233517646789551\n",
      "Step 160/1000, Loss: 0.7339519262313843, Validation Loss: 0.9170975089073181\n",
      "Step 161/1000, Loss: 0.7290752530097961, Validation Loss: 0.9107598662376404\n",
      "Step 162/1000, Loss: 0.724148690700531, Validation Loss: 0.9043408632278442\n",
      "Step 163/1000, Loss: 0.7191734313964844, Validation Loss: 0.8978426456451416\n",
      "Step 164/1000, Loss: 0.7141505479812622, Validation Loss: 0.8912674784660339\n",
      "Step 165/1000, Loss: 0.709081768989563, Validation Loss: 0.8846184611320496\n",
      "Step 166/1000, Loss: 0.703968346118927, Validation Loss: 0.8778984546661377\n",
      "Step 167/1000, Loss: 0.6988120079040527, Validation Loss: 0.8711106181144714\n",
      "Step 168/1000, Loss: 0.6936143040657043, Validation Loss: 0.8642584085464478\n",
      "Step 169/1000, Loss: 0.6883769631385803, Validation Loss: 0.8573452234268188\n",
      "Step 170/1000, Loss: 0.6831017732620239, Validation Loss: 0.8503748178482056\n",
      "Step 171/1000, Loss: 0.6777908205986023, Validation Loss: 0.8433505892753601\n",
      "Step 172/1000, Loss: 0.6724456548690796, Validation Loss: 0.8362762331962585\n",
      "Step 173/1000, Loss: 0.6670684814453125, Validation Loss: 0.829155683517456\n",
      "Step 174/1000, Loss: 0.6616612672805786, Validation Loss: 0.8219925761222839\n",
      "Step 175/1000, Loss: 0.6562260389328003, Validation Loss: 0.8147906064987183\n",
      "Step 176/1000, Loss: 0.6507648825645447, Validation Loss: 0.807553768157959\n",
      "Step 177/1000, Loss: 0.6452797651290894, Validation Loss: 0.800286054611206\n",
      "Step 178/1000, Loss: 0.639772891998291, Validation Loss: 0.7929915189743042\n",
      "Step 179/1000, Loss: 0.6342464089393616, Validation Loss: 0.7856744527816772\n",
      "Step 180/1000, Loss: 0.6287024617195129, Validation Loss: 0.7783389091491699\n",
      "Step 181/1000, Loss: 0.6231430172920227, Validation Loss: 0.7709898352622986\n",
      "Step 182/1000, Loss: 0.6175702810287476, Validation Loss: 0.7636314630508423\n",
      "Step 183/1000, Loss: 0.6119864583015442, Validation Loss: 0.7562687397003174\n",
      "Step 184/1000, Loss: 0.6063938140869141, Validation Loss: 0.7489067316055298\n",
      "Step 185/1000, Loss: 0.6007943153381348, Validation Loss: 0.7415499091148376\n",
      "Step 186/1000, Loss: 0.5951902866363525, Validation Loss: 0.7342034578323364\n",
      "Step 187/1000, Loss: 0.5895838737487793, Validation Loss: 0.7268717885017395\n",
      "Step 188/1000, Loss: 0.583977222442627, Validation Loss: 0.7195595502853394\n",
      "Step 189/1000, Loss: 0.5783725380897522, Validation Loss: 0.7122711539268494\n",
      "Step 190/1000, Loss: 0.5727719664573669, Validation Loss: 0.7050105333328247\n",
      "Step 191/1000, Loss: 0.5671777725219727, Validation Loss: 0.6977812647819519\n",
      "Step 192/1000, Loss: 0.5615921020507812, Validation Loss: 0.6905864477157593\n",
      "Step 193/1000, Loss: 0.5560171008110046, Validation Loss: 0.6834291815757751\n",
      "Step 194/1000, Loss: 0.5504549741744995, Validation Loss: 0.6763112545013428\n",
      "Step 195/1000, Loss: 0.5449076294898987, Validation Loss: 0.6692349314689636\n",
      "Step 196/1000, Loss: 0.5393774509429932, Validation Loss: 0.6622014045715332\n",
      "Step 197/1000, Loss: 0.533866286277771, Validation Loss: 0.6552114486694336\n",
      "Step 198/1000, Loss: 0.5283761620521545, Validation Loss: 0.6482660174369812\n",
      "Step 199/1000, Loss: 0.5229093432426453, Validation Loss: 0.6413649916648865\n",
      "Step 200/1000, Loss: 0.5174674391746521, Validation Loss: 0.6345082521438599\n",
      "Step 201/1000, Loss: 0.5120524764060974, Validation Loss: 0.6276958584785461\n",
      "Step 202/1000, Loss: 0.5066663026809692, Validation Loss: 0.6243146061897278\n",
      "Step 203/1000, Loss: 0.5039883255958557, Validation Loss: 0.6209606528282166\n",
      "Step 204/1000, Loss: 0.5013258457183838, Validation Loss: 0.6176331639289856\n",
      "Step 205/1000, Loss: 0.4986792504787445, Validation Loss: 0.6143308281898499\n",
      "Step 206/1000, Loss: 0.49604859948158264, Validation Loss: 0.6110527515411377\n",
      "Step 207/1000, Loss: 0.49343425035476685, Validation Loss: 0.6077976226806641\n",
      "Step 208/1000, Loss: 0.49083614349365234, Validation Loss: 0.6045642495155334\n",
      "Step 209/1000, Loss: 0.48825448751449585, Validation Loss: 0.6013515591621399\n",
      "Step 210/1000, Loss: 0.4856894016265869, Validation Loss: 0.5981585383415222\n",
      "Step 211/1000, Loss: 0.4831409156322479, Validation Loss: 0.5949839949607849\n",
      "Step 212/1000, Loss: 0.4806089997291565, Validation Loss: 0.5918270945549011\n",
      "Step 213/1000, Loss: 0.4780937433242798, Validation Loss: 0.5886868834495544\n",
      "Step 214/1000, Loss: 0.4755951762199402, Validation Loss: 0.5855624079704285\n",
      "Step 215/1000, Loss: 0.47311314940452576, Validation Loss: 0.5824530124664307\n",
      "Step 216/1000, Loss: 0.47064775228500366, Validation Loss: 0.5793582797050476\n",
      "Step 217/1000, Loss: 0.4681989252567291, Validation Loss: 0.5762771964073181\n",
      "Step 218/1000, Loss: 0.465766578912735, Validation Loss: 0.5732096433639526\n",
      "Step 219/1000, Loss: 0.46335071325302124, Validation Loss: 0.5701552033424377\n",
      "Step 220/1000, Loss: 0.4609512686729431, Validation Loss: 0.5671135187149048\n",
      "Step 221/1000, Loss: 0.45856815576553345, Validation Loss: 0.564084529876709\n",
      "Step 222/1000, Loss: 0.4562011957168579, Validation Loss: 0.561068058013916\n",
      "Step 223/1000, Loss: 0.45385050773620605, Validation Loss: 0.5580640435218811\n",
      "Step 224/1000, Loss: 0.45151591300964355, Validation Loss: 0.5550726652145386\n",
      "Step 225/1000, Loss: 0.4491972327232361, Validation Loss: 0.5520938634872437\n",
      "Step 226/1000, Loss: 0.4468945562839508, Validation Loss: 0.5491279363632202\n",
      "Step 227/1000, Loss: 0.444607675075531, Validation Loss: 0.5461750626564026\n",
      "Step 228/1000, Loss: 0.4423365592956543, Validation Loss: 0.5432354807853699\n",
      "Step 229/1000, Loss: 0.44008100032806396, Validation Loss: 0.5403094291687012\n",
      "Step 230/1000, Loss: 0.4378410875797272, Validation Loss: 0.5373971462249756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 231/1000, Loss: 0.43561652302742004, Validation Loss: 0.5344990491867065\n",
      "Step 232/1000, Loss: 0.43340733647346497, Validation Loss: 0.5316153764724731\n",
      "Step 233/1000, Loss: 0.4312133491039276, Validation Loss: 0.528746485710144\n",
      "Step 234/1000, Loss: 0.4290345311164856, Validation Loss: 0.5258926153182983\n",
      "Step 235/1000, Loss: 0.42687076330184937, Validation Loss: 0.5230541229248047\n",
      "Step 236/1000, Loss: 0.4247218668460846, Validation Loss: 0.5202312469482422\n",
      "Step 237/1000, Loss: 0.4225878119468689, Validation Loss: 0.5174242258071899\n",
      "Step 238/1000, Loss: 0.4204684793949127, Validation Loss: 0.514633297920227\n",
      "Step 239/1000, Loss: 0.41836366057395935, Validation Loss: 0.5118587613105774\n",
      "Step 240/1000, Loss: 0.4162735044956207, Validation Loss: 0.5091007947921753\n",
      "Step 241/1000, Loss: 0.4141975939273834, Validation Loss: 0.5063595175743103\n",
      "Step 242/1000, Loss: 0.412136048078537, Validation Loss: 0.5036352276802063\n",
      "Step 243/1000, Loss: 0.4100886285305023, Validation Loss: 0.5009279847145081\n",
      "Step 244/1000, Loss: 0.4080553352832794, Validation Loss: 0.4982379674911499\n",
      "Step 245/1000, Loss: 0.4060359597206116, Validation Loss: 0.4955652356147766\n",
      "Step 246/1000, Loss: 0.4040304720401764, Validation Loss: 0.49290984869003296\n",
      "Step 247/1000, Loss: 0.40203872323036194, Validation Loss: 0.4902718961238861\n",
      "Step 248/1000, Loss: 0.4000606834888458, Validation Loss: 0.487651526927948\n",
      "Step 249/1000, Loss: 0.3980961740016937, Validation Loss: 0.4850486218929291\n",
      "Step 250/1000, Loss: 0.3961451053619385, Validation Loss: 0.48246335983276367\n",
      "Step 251/1000, Loss: 0.3942073881626129, Validation Loss: 0.47989559173583984\n",
      "Step 252/1000, Loss: 0.39228296279907227, Validation Loss: 0.47734543681144714\n",
      "Step 253/1000, Loss: 0.3903716206550598, Validation Loss: 0.47481289505958557\n",
      "Step 254/1000, Loss: 0.3884733021259308, Validation Loss: 0.4722978174686432\n",
      "Step 255/1000, Loss: 0.3865880072116852, Validation Loss: 0.46980026364326477\n",
      "Step 256/1000, Loss: 0.3847154378890991, Validation Loss: 0.46732017397880554\n",
      "Step 257/1000, Loss: 0.38285574316978455, Validation Loss: 0.46485745906829834\n",
      "Step 258/1000, Loss: 0.3810085952281952, Validation Loss: 0.4624120891094208\n",
      "Step 259/1000, Loss: 0.37917405366897583, Validation Loss: 0.45998409390449524\n",
      "Step 260/1000, Loss: 0.37735193967819214, Validation Loss: 0.45757320523262024\n",
      "Step 261/1000, Loss: 0.37554219365119934, Validation Loss: 0.4551796019077301\n",
      "Step 262/1000, Loss: 0.3737446367740631, Validation Loss: 0.45280301570892334\n",
      "Step 263/1000, Loss: 0.3719593584537506, Validation Loss: 0.45044344663619995\n",
      "Step 264/1000, Loss: 0.37018609046936035, Validation Loss: 0.4481008052825928\n",
      "Step 265/1000, Loss: 0.36842480301856995, Validation Loss: 0.4457750618457794\n",
      "Step 266/1000, Loss: 0.36667540669441223, Validation Loss: 0.44346609711647034\n",
      "Step 267/1000, Loss: 0.36493781208992004, Validation Loss: 0.4411737620830536\n",
      "Step 268/1000, Loss: 0.36321187019348145, Validation Loss: 0.4388982057571411\n",
      "Step 269/1000, Loss: 0.36149758100509644, Validation Loss: 0.4366391599178314\n",
      "Step 270/1000, Loss: 0.3597947955131531, Validation Loss: 0.4343966543674469\n",
      "Step 271/1000, Loss: 0.3581034243106842, Validation Loss: 0.43217065930366516\n",
      "Step 272/1000, Loss: 0.35642337799072266, Validation Loss: 0.4299609959125519\n",
      "Step 273/1000, Loss: 0.35475456714630127, Validation Loss: 0.42776769399642944\n",
      "Step 274/1000, Loss: 0.35309696197509766, Validation Loss: 0.42559075355529785\n",
      "Step 275/1000, Loss: 0.3514503836631775, Validation Loss: 0.42343005537986755\n",
      "Step 276/1000, Loss: 0.349814772605896, Validation Loss: 0.42128556966781616\n",
      "Step 277/1000, Loss: 0.3481900990009308, Validation Loss: 0.4191572368144989\n",
      "Step 278/1000, Loss: 0.34657615423202515, Validation Loss: 0.4170450270175934\n",
      "Step 279/1000, Loss: 0.3449729084968567, Validation Loss: 0.414948970079422\n",
      "Step 280/1000, Loss: 0.3433803617954254, Validation Loss: 0.4128689765930176\n",
      "Step 281/1000, Loss: 0.341798335313797, Validation Loss: 0.41080501675605774\n",
      "Step 282/1000, Loss: 0.3402267396450043, Validation Loss: 0.4087570905685425\n",
      "Step 283/1000, Loss: 0.33866554498672485, Validation Loss: 0.406725138425827\n",
      "Step 284/1000, Loss: 0.3371146321296692, Validation Loss: 0.4047090411186218\n",
      "Step 285/1000, Loss: 0.33557388186454773, Validation Loss: 0.40270882844924927\n",
      "Step 286/1000, Loss: 0.3340432345867157, Validation Loss: 0.40072450041770935\n",
      "Step 287/1000, Loss: 0.3325227200984955, Validation Loss: 0.39875587821006775\n",
      "Step 288/1000, Loss: 0.331012099981308, Validation Loss: 0.39680296182632446\n",
      "Step 289/1000, Loss: 0.3295113146305084, Validation Loss: 0.39486563205718994\n",
      "Step 290/1000, Loss: 0.3280203342437744, Validation Loss: 0.3929439187049866\n",
      "Step 291/1000, Loss: 0.3265390992164612, Validation Loss: 0.3910376727581024\n",
      "Step 292/1000, Loss: 0.3250674307346344, Validation Loss: 0.3891468346118927\n",
      "Step 293/1000, Loss: 0.32360532879829407, Validation Loss: 0.3872712552547455\n",
      "Step 294/1000, Loss: 0.32215267419815063, Validation Loss: 0.385410875082016\n",
      "Step 295/1000, Loss: 0.3207094371318817, Validation Loss: 0.38356560468673706\n",
      "Step 296/1000, Loss: 0.319275438785553, Validation Loss: 0.381735235452652\n",
      "Step 297/1000, Loss: 0.3178507387638092, Validation Loss: 0.37991979718208313\n",
      "Step 298/1000, Loss: 0.31643515825271606, Validation Loss: 0.37811917066574097\n",
      "Step 299/1000, Loss: 0.31502866744995117, Validation Loss: 0.3763331174850464\n",
      "Step 300/1000, Loss: 0.3136311173439026, Validation Loss: 0.37456151843070984\n",
      "Step 301/1000, Loss: 0.3122424781322479, Validation Loss: 0.37280431389808655\n",
      "Step 302/1000, Loss: 0.3108627200126648, Validation Loss: 0.3719324469566345\n",
      "Step 303/1000, Loss: 0.3101768493652344, Validation Loss: 0.3710665702819824\n",
      "Step 304/1000, Loss: 0.30949434638023376, Validation Loss: 0.37020644545555115\n",
      "Step 305/1000, Loss: 0.3088151216506958, Validation Loss: 0.3693520426750183\n",
      "Step 306/1000, Loss: 0.3081391751766205, Validation Loss: 0.36850300431251526\n",
      "Step 307/1000, Loss: 0.30746641755104065, Validation Loss: 0.367659330368042\n",
      "Step 308/1000, Loss: 0.306796669960022, Validation Loss: 0.3668206036090851\n",
      "Step 309/1000, Loss: 0.30612999200820923, Validation Loss: 0.3659868538379669\n",
      "Step 310/1000, Loss: 0.3054662048816681, Validation Loss: 0.3651578426361084\n",
      "Step 311/1000, Loss: 0.30480527877807617, Validation Loss: 0.36433354020118713\n",
      "Step 312/1000, Loss: 0.30414721369743347, Validation Loss: 0.36351364850997925\n",
      "Step 313/1000, Loss: 0.30349189043045044, Validation Loss: 0.36269816756248474\n",
      "Step 314/1000, Loss: 0.3028392791748047, Validation Loss: 0.3618869185447693\n",
      "Step 315/1000, Loss: 0.30218935012817383, Validation Loss: 0.3610798120498657\n",
      "Step 316/1000, Loss: 0.3015420138835907, Validation Loss: 0.3602766692638397\n",
      "Step 317/1000, Loss: 0.3008972704410553, Validation Loss: 0.35947751998901367\n",
      "Step 318/1000, Loss: 0.30025508999824524, Validation Loss: 0.35868215560913086\n",
      "Step 319/1000, Loss: 0.29961544275283813, Validation Loss: 0.35789060592651367\n",
      "Step 320/1000, Loss: 0.2989782691001892, Validation Loss: 0.357102632522583\n",
      "Step 321/1000, Loss: 0.29834359884262085, Validation Loss: 0.35631829500198364\n",
      "Step 322/1000, Loss: 0.29771125316619873, Validation Loss: 0.35553744435310364\n",
      "Step 323/1000, Loss: 0.2970813810825348, Validation Loss: 0.3547600209712982\n",
      "Step 324/1000, Loss: 0.2964538037776947, Validation Loss: 0.3539859354496002\n",
      "Step 325/1000, Loss: 0.29582861065864563, Validation Loss: 0.35321518778800964\n",
      "Step 326/1000, Loss: 0.2952057123184204, Validation Loss: 0.3524476885795593\n",
      "Step 327/1000, Loss: 0.29458513855934143, Validation Loss: 0.3516834080219269\n",
      "Step 328/1000, Loss: 0.29396679997444153, Validation Loss: 0.35092225670814514\n",
      "Step 329/1000, Loss: 0.2933506667613983, Validation Loss: 0.3501642942428589\n",
      "Step 330/1000, Loss: 0.29273682832717896, Validation Loss: 0.3494093716144562\n",
      "Step 331/1000, Loss: 0.2921251654624939, Validation Loss: 0.3486575484275818\n",
      "Step 332/1000, Loss: 0.29151567816734314, Validation Loss: 0.34790879487991333\n",
      "Step 333/1000, Loss: 0.2909083962440491, Validation Loss: 0.34716302156448364\n",
      "Step 334/1000, Loss: 0.29030323028564453, Validation Loss: 0.3464202880859375\n",
      "Step 335/1000, Loss: 0.2897002398967743, Validation Loss: 0.3456803858280182\n",
      "Step 336/1000, Loss: 0.2890992760658264, Validation Loss: 0.3449435532093048\n",
      "Step 337/1000, Loss: 0.28850045800209045, Validation Loss: 0.34420961141586304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 338/1000, Loss: 0.28790369629859924, Validation Loss: 0.3434787094593048\n",
      "Step 339/1000, Loss: 0.28730908036231995, Validation Loss: 0.3427506387233734\n",
      "Step 340/1000, Loss: 0.28671643137931824, Validation Loss: 0.3420255184173584\n",
      "Step 341/1000, Loss: 0.2861258387565613, Validation Loss: 0.3413033187389374\n",
      "Step 342/1000, Loss: 0.2855372726917267, Validation Loss: 0.3405839800834656\n",
      "Step 343/1000, Loss: 0.28495073318481445, Validation Loss: 0.33986756205558777\n",
      "Step 344/1000, Loss: 0.2843661606311798, Validation Loss: 0.3391540050506592\n",
      "Step 345/1000, Loss: 0.28378358483314514, Validation Loss: 0.3384433090686798\n",
      "Step 346/1000, Loss: 0.2832029461860657, Validation Loss: 0.33773547410964966\n",
      "Step 347/1000, Loss: 0.2826242744922638, Validation Loss: 0.33703047037124634\n",
      "Step 348/1000, Loss: 0.2820475697517395, Validation Loss: 0.336328387260437\n",
      "Step 349/1000, Loss: 0.28147274255752563, Validation Loss: 0.33562907576560974\n",
      "Step 350/1000, Loss: 0.28089988231658936, Validation Loss: 0.3349325954914093\n",
      "Step 351/1000, Loss: 0.2803289294242859, Validation Loss: 0.33423885703086853\n",
      "Step 352/1000, Loss: 0.27975982427597046, Validation Loss: 0.33354803919792175\n",
      "Step 353/1000, Loss: 0.27919262647628784, Validation Loss: 0.33285990357398987\n",
      "Step 354/1000, Loss: 0.27862727642059326, Validation Loss: 0.3321745693683624\n",
      "Step 355/1000, Loss: 0.27806374430656433, Validation Loss: 0.33149203658103943\n",
      "Step 356/1000, Loss: 0.2775021493434906, Validation Loss: 0.3308122456073761\n",
      "Step 357/1000, Loss: 0.27694234251976013, Validation Loss: 0.3301351070404053\n",
      "Step 358/1000, Loss: 0.27638429403305054, Validation Loss: 0.3294607102870941\n",
      "Step 359/1000, Loss: 0.27582812309265137, Validation Loss: 0.32878902554512024\n",
      "Step 360/1000, Loss: 0.27527371048927307, Validation Loss: 0.32812005281448364\n",
      "Step 361/1000, Loss: 0.2747211456298828, Validation Loss: 0.32745373249053955\n",
      "Step 362/1000, Loss: 0.2741703391075134, Validation Loss: 0.32679009437561035\n",
      "Step 363/1000, Loss: 0.2736212909221649, Validation Loss: 0.32612910866737366\n",
      "Step 364/1000, Loss: 0.2730740010738373, Validation Loss: 0.3254706561565399\n",
      "Step 365/1000, Loss: 0.27252843976020813, Validation Loss: 0.32481497526168823\n",
      "Step 366/1000, Loss: 0.27198466658592224, Validation Loss: 0.3241617679595947\n",
      "Step 367/1000, Loss: 0.27144259214401245, Validation Loss: 0.3235113024711609\n",
      "Step 368/1000, Loss: 0.27090224623680115, Validation Loss: 0.3228633403778076\n",
      "Step 369/1000, Loss: 0.27036359906196594, Validation Loss: 0.3222179412841797\n",
      "Step 370/1000, Loss: 0.2698266804218292, Validation Loss: 0.3215751349925995\n",
      "Step 371/1000, Loss: 0.26929140090942383, Validation Loss: 0.32093486189842224\n",
      "Step 372/1000, Loss: 0.2687578797340393, Validation Loss: 0.32029709219932556\n",
      "Step 373/1000, Loss: 0.2682259678840637, Validation Loss: 0.3196618854999542\n",
      "Step 374/1000, Loss: 0.26769569516181946, Validation Loss: 0.31902918219566345\n",
      "Step 375/1000, Loss: 0.2671671211719513, Validation Loss: 0.31839895248413086\n",
      "Step 376/1000, Loss: 0.26664021611213684, Validation Loss: 0.3177712559700012\n",
      "Step 377/1000, Loss: 0.2661149203777313, Validation Loss: 0.31714606285095215\n",
      "Step 378/1000, Loss: 0.26559123396873474, Validation Loss: 0.3165232539176941\n",
      "Step 379/1000, Loss: 0.2650691568851471, Validation Loss: 0.3159030079841614\n",
      "Step 380/1000, Loss: 0.2645486891269684, Validation Loss: 0.3152851462364197\n",
      "Step 381/1000, Loss: 0.264029860496521, Validation Loss: 0.3146698474884033\n",
      "Step 382/1000, Loss: 0.26351261138916016, Validation Loss: 0.3140569031238556\n",
      "Step 383/1000, Loss: 0.26299697160720825, Validation Loss: 0.3134463131427765\n",
      "Step 384/1000, Loss: 0.2624828517436981, Validation Loss: 0.31283819675445557\n",
      "Step 385/1000, Loss: 0.2619703412055969, Validation Loss: 0.31223249435424805\n",
      "Step 386/1000, Loss: 0.2614593803882599, Validation Loss: 0.31162911653518677\n",
      "Step 387/1000, Loss: 0.26094990968704224, Validation Loss: 0.31102821230888367\n",
      "Step 388/1000, Loss: 0.2604420781135559, Validation Loss: 0.31042957305908203\n",
      "Step 389/1000, Loss: 0.25993573665618896, Validation Loss: 0.3098333775997162\n",
      "Step 390/1000, Loss: 0.2594309151172638, Validation Loss: 0.30923953652381897\n",
      "Step 391/1000, Loss: 0.25892767310142517, Validation Loss: 0.308648020029068\n",
      "Step 392/1000, Loss: 0.25842586159706116, Validation Loss: 0.30805879831314087\n",
      "Step 393/1000, Loss: 0.2579255998134613, Validation Loss: 0.30747199058532715\n",
      "Step 394/1000, Loss: 0.25742682814598083, Validation Loss: 0.3068874478340149\n",
      "Step 395/1000, Loss: 0.25692957639694214, Validation Loss: 0.3063051998615265\n",
      "Step 396/1000, Loss: 0.25643375515937805, Validation Loss: 0.3057252764701843\n",
      "Step 397/1000, Loss: 0.25593942403793335, Validation Loss: 0.30514758825302124\n",
      "Step 398/1000, Loss: 0.25544658303260803, Validation Loss: 0.3045722544193268\n",
      "Step 399/1000, Loss: 0.2549552023410797, Validation Loss: 0.303999125957489\n",
      "Step 400/1000, Loss: 0.2544652819633484, Validation Loss: 0.3034282922744751\n",
      "Step 401/1000, Loss: 0.25397682189941406, Validation Loss: 0.30285966396331787\n",
      "Step 402/1000, Loss: 0.25348979234695435, Validation Loss: 0.30257636308670044\n",
      "Step 403/1000, Loss: 0.25324687361717224, Validation Loss: 0.30229395627975464\n",
      "Step 404/1000, Loss: 0.2530045807361603, Validation Loss: 0.30201229453086853\n",
      "Step 405/1000, Loss: 0.2527627646923065, Validation Loss: 0.30173152685165405\n",
      "Step 406/1000, Loss: 0.25252145528793335, Validation Loss: 0.30145150423049927\n",
      "Step 407/1000, Loss: 0.25228068232536316, Validation Loss: 0.30117231607437134\n",
      "Step 408/1000, Loss: 0.25204038619995117, Validation Loss: 0.3008938431739807\n",
      "Step 409/1000, Loss: 0.2518005967140198, Validation Loss: 0.30061614513397217\n",
      "Step 410/1000, Loss: 0.2515611946582794, Validation Loss: 0.3003391623497009\n",
      "Step 411/1000, Loss: 0.25132235884666443, Validation Loss: 0.3000629246234894\n",
      "Step 412/1000, Loss: 0.2510839104652405, Validation Loss: 0.29978734254837036\n",
      "Step 413/1000, Loss: 0.25084593892097473, Validation Loss: 0.29951247572898865\n",
      "Step 414/1000, Loss: 0.2506083548069, Validation Loss: 0.29923829436302185\n",
      "Step 415/1000, Loss: 0.2503712773323059, Validation Loss: 0.2989647686481476\n",
      "Step 416/1000, Loss: 0.25013458728790283, Validation Loss: 0.29869192838668823\n",
      "Step 417/1000, Loss: 0.24989831447601318, Validation Loss: 0.298419713973999\n",
      "Step 418/1000, Loss: 0.24966248869895935, Validation Loss: 0.29814809560775757\n",
      "Step 419/1000, Loss: 0.24942699074745178, Validation Loss: 0.2978772222995758\n",
      "Step 420/1000, Loss: 0.2491919845342636, Validation Loss: 0.2976069152355194\n",
      "Step 421/1000, Loss: 0.2489573210477829, Validation Loss: 0.29733720421791077\n",
      "Step 422/1000, Loss: 0.2487230747938156, Validation Loss: 0.29706814885139465\n",
      "Step 423/1000, Loss: 0.24848923087120056, Validation Loss: 0.2967996597290039\n",
      "Step 424/1000, Loss: 0.24825577437877655, Validation Loss: 0.2965317666530609\n",
      "Step 425/1000, Loss: 0.24802272021770477, Validation Loss: 0.29626449942588806\n",
      "Step 426/1000, Loss: 0.24779000878334045, Validation Loss: 0.2959977984428406\n",
      "Step 427/1000, Loss: 0.24755774438381195, Validation Loss: 0.29573163390159607\n",
      "Step 428/1000, Loss: 0.2473258078098297, Validation Loss: 0.29546600580215454\n",
      "Step 429/1000, Loss: 0.2470942586660385, Validation Loss: 0.29520100355148315\n",
      "Step 430/1000, Loss: 0.24686306715011597, Validation Loss: 0.29493656754493713\n",
      "Step 431/1000, Loss: 0.24663229286670685, Validation Loss: 0.2946726083755493\n",
      "Step 432/1000, Loss: 0.246401846408844, Validation Loss: 0.2944093346595764\n",
      "Step 433/1000, Loss: 0.24617180228233337, Validation Loss: 0.29414650797843933\n",
      "Step 434/1000, Loss: 0.24594208598136902, Validation Loss: 0.2938842475414276\n",
      "Step 435/1000, Loss: 0.2457127720117569, Validation Loss: 0.29362252354621887\n",
      "Step 436/1000, Loss: 0.24548380076885223, Validation Loss: 0.2933613061904907\n",
      "Step 437/1000, Loss: 0.24525520205497742, Validation Loss: 0.29310059547424316\n",
      "Step 438/1000, Loss: 0.24502694606781006, Validation Loss: 0.2928404211997986\n",
      "Step 439/1000, Loss: 0.24479906260967255, Validation Loss: 0.292580783367157\n",
      "Step 440/1000, Loss: 0.2445715218782425, Validation Loss: 0.29232171177864075\n",
      "Step 441/1000, Loss: 0.2443443238735199, Validation Loss: 0.2920631468296051\n",
      "Step 442/1000, Loss: 0.24411749839782715, Validation Loss: 0.2918050289154053\n",
      "Step 443/1000, Loss: 0.24389101564884186, Validation Loss: 0.2915474474430084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 444/1000, Loss: 0.24366487562656403, Validation Loss: 0.29129037261009216\n",
      "Step 445/1000, Loss: 0.24343906342983246, Validation Loss: 0.29103389382362366\n",
      "Step 446/1000, Loss: 0.24321365356445312, Validation Loss: 0.29077786207199097\n",
      "Step 447/1000, Loss: 0.24298857152462006, Validation Loss: 0.29052236676216125\n",
      "Step 448/1000, Loss: 0.24276380240917206, Validation Loss: 0.29026737809181213\n",
      "Step 449/1000, Loss: 0.2425394058227539, Validation Loss: 0.29001280665397644\n",
      "Step 450/1000, Loss: 0.2423153519630432, Validation Loss: 0.2897588610649109\n",
      "Step 451/1000, Loss: 0.24209162592887878, Validation Loss: 0.28950536251068115\n",
      "Step 452/1000, Loss: 0.24186822772026062, Validation Loss: 0.289252370595932\n",
      "Step 453/1000, Loss: 0.2416451871395111, Validation Loss: 0.28899988532066345\n",
      "Step 454/1000, Loss: 0.24142244458198547, Validation Loss: 0.2887479066848755\n",
      "Step 455/1000, Loss: 0.24120007455348969, Validation Loss: 0.2884964048862457\n",
      "Step 456/1000, Loss: 0.24097801744937897, Validation Loss: 0.28824540972709656\n",
      "Step 457/1000, Loss: 0.2407563328742981, Validation Loss: 0.2879948914051056\n",
      "Step 458/1000, Loss: 0.2405349165201187, Validation Loss: 0.2877449691295624\n",
      "Step 459/1000, Loss: 0.24031390249729156, Validation Loss: 0.287495493888855\n",
      "Step 460/1000, Loss: 0.2400931864976883, Validation Loss: 0.2872465252876282\n",
      "Step 461/1000, Loss: 0.23987281322479248, Validation Loss: 0.2869980037212372\n",
      "Step 462/1000, Loss: 0.23965273797512054, Validation Loss: 0.286749929189682\n",
      "Step 463/1000, Loss: 0.23943297564983368, Validation Loss: 0.286502480506897\n",
      "Step 464/1000, Loss: 0.23921363055706024, Validation Loss: 0.28625547885894775\n",
      "Step 465/1000, Loss: 0.2389945387840271, Validation Loss: 0.2860089838504791\n",
      "Step 466/1000, Loss: 0.23877577483654022, Validation Loss: 0.2857629358768463\n",
      "Step 467/1000, Loss: 0.23855732381343842, Validation Loss: 0.2855173945426941\n",
      "Step 468/1000, Loss: 0.23833921551704407, Validation Loss: 0.2852723300457001\n",
      "Step 469/1000, Loss: 0.23812143504619598, Validation Loss: 0.28502777218818665\n",
      "Step 470/1000, Loss: 0.23790395259857178, Validation Loss: 0.28478366136550903\n",
      "Step 471/1000, Loss: 0.23768678307533264, Validation Loss: 0.2845400869846344\n",
      "Step 472/1000, Loss: 0.23746994137763977, Validation Loss: 0.2842969596385956\n",
      "Step 473/1000, Loss: 0.23725344240665436, Validation Loss: 0.28405433893203735\n",
      "Step 474/1000, Loss: 0.23703722655773163, Validation Loss: 0.28381216526031494\n",
      "Step 475/1000, Loss: 0.23682135343551636, Validation Loss: 0.2835704982280731\n",
      "Step 476/1000, Loss: 0.23660574853420258, Validation Loss: 0.2833292484283447\n",
      "Step 477/1000, Loss: 0.23639045655727386, Validation Loss: 0.2830885350704193\n",
      "Step 478/1000, Loss: 0.2361755222082138, Validation Loss: 0.2828482687473297\n",
      "Step 479/1000, Loss: 0.23596088588237762, Validation Loss: 0.2826085686683655\n",
      "Step 480/1000, Loss: 0.2357465773820877, Validation Loss: 0.2823692262172699\n",
      "Step 481/1000, Loss: 0.23553256690502167, Validation Loss: 0.2821303904056549\n",
      "Step 482/1000, Loss: 0.2353188395500183, Validation Loss: 0.2818920314311981\n",
      "Step 483/1000, Loss: 0.23510541021823883, Validation Loss: 0.2816541790962219\n",
      "Step 484/1000, Loss: 0.2348923236131668, Validation Loss: 0.28141671419143677\n",
      "Step 485/1000, Loss: 0.23467952013015747, Validation Loss: 0.281179815530777\n",
      "Step 486/1000, Loss: 0.2344670593738556, Validation Loss: 0.2809433043003082\n",
      "Step 487/1000, Loss: 0.234254851937294, Validation Loss: 0.2807072699069977\n",
      "Step 488/1000, Loss: 0.23404298722743988, Validation Loss: 0.28047171235084534\n",
      "Step 489/1000, Loss: 0.23383143544197083, Validation Loss: 0.2802366316318512\n",
      "Step 490/1000, Loss: 0.23362012207508087, Validation Loss: 0.28000202775001526\n",
      "Step 491/1000, Loss: 0.23340918123722076, Validation Loss: 0.27976787090301514\n",
      "Step 492/1000, Loss: 0.23319852352142334, Validation Loss: 0.27953416109085083\n",
      "Step 493/1000, Loss: 0.2329881340265274, Validation Loss: 0.2793009281158447\n",
      "Step 494/1000, Loss: 0.23277810215950012, Validation Loss: 0.27906814217567444\n",
      "Step 495/1000, Loss: 0.23256827890872955, Validation Loss: 0.2788357436656952\n",
      "Step 496/1000, Loss: 0.23235882818698883, Validation Loss: 0.2786038815975189\n",
      "Step 497/1000, Loss: 0.2321496605873108, Validation Loss: 0.2783724069595337\n",
      "Step 498/1000, Loss: 0.23194073140621185, Validation Loss: 0.27814143896102905\n",
      "Step 499/1000, Loss: 0.23173214495182037, Validation Loss: 0.27791088819503784\n",
      "Step 500/1000, Loss: 0.23152385652065277, Validation Loss: 0.27768081426620483\n",
      "Step 501/1000, Loss: 0.23131583631038666, Validation Loss: 0.27745121717453003\n",
      "Step 502/1000, Loss: 0.2311081439256668, Validation Loss: 0.27733656764030457\n",
      "Step 503/1000, Loss: 0.23100441694259644, Validation Loss: 0.2772221267223358\n",
      "Step 504/1000, Loss: 0.23090077936649323, Validation Loss: 0.2771078050136566\n",
      "Step 505/1000, Loss: 0.23079730570316315, Validation Loss: 0.27699360251426697\n",
      "Step 506/1000, Loss: 0.23069387674331665, Validation Loss: 0.27687960863113403\n",
      "Step 507/1000, Loss: 0.2305905520915985, Validation Loss: 0.27676576375961304\n",
      "Step 508/1000, Loss: 0.23048731684684753, Validation Loss: 0.2766520380973816\n",
      "Step 509/1000, Loss: 0.2303842157125473, Validation Loss: 0.2765384316444397\n",
      "Step 510/1000, Loss: 0.23028120398521423, Validation Loss: 0.27642500400543213\n",
      "Step 511/1000, Loss: 0.23017825186252594, Validation Loss: 0.2763116955757141\n",
      "Step 512/1000, Loss: 0.2300753891468048, Validation Loss: 0.27619850635528564\n",
      "Step 513/1000, Loss: 0.22997263073921204, Validation Loss: 0.2760855555534363\n",
      "Step 514/1000, Loss: 0.2298699915409088, Validation Loss: 0.2759726941585541\n",
      "Step 515/1000, Loss: 0.22976742684841156, Validation Loss: 0.2758599519729614\n",
      "Step 516/1000, Loss: 0.2296649068593979, Validation Loss: 0.27574729919433594\n",
      "Step 517/1000, Loss: 0.22956249117851257, Validation Loss: 0.2756347954273224\n",
      "Step 518/1000, Loss: 0.22946013510227203, Validation Loss: 0.27552247047424316\n",
      "Step 519/1000, Loss: 0.22935788333415985, Validation Loss: 0.2754102945327759\n",
      "Step 520/1000, Loss: 0.22925572097301483, Validation Loss: 0.275298148393631\n",
      "Step 521/1000, Loss: 0.2291536182165146, Validation Loss: 0.2751862406730652\n",
      "Step 522/1000, Loss: 0.2290516197681427, Validation Loss: 0.2750743627548218\n",
      "Step 523/1000, Loss: 0.22894969582557678, Validation Loss: 0.2749626636505127\n",
      "Step 524/1000, Loss: 0.22884783148765564, Validation Loss: 0.2748510539531708\n",
      "Step 525/1000, Loss: 0.22874604165554047, Validation Loss: 0.2747395932674408\n",
      "Step 526/1000, Loss: 0.22864437103271484, Validation Loss: 0.27462825179100037\n",
      "Step 527/1000, Loss: 0.2285427302122116, Validation Loss: 0.2745169997215271\n",
      "Step 528/1000, Loss: 0.22844119369983673, Validation Loss: 0.2744058668613434\n",
      "Step 529/1000, Loss: 0.22833971679210663, Validation Loss: 0.2742948532104492\n",
      "Step 530/1000, Loss: 0.22823837399482727, Validation Loss: 0.2741839587688446\n",
      "Step 531/1000, Loss: 0.2281370311975479, Validation Loss: 0.27407318353652954\n",
      "Step 532/1000, Loss: 0.22803577780723572, Validation Loss: 0.27396252751350403\n",
      "Step 533/1000, Loss: 0.2279345840215683, Validation Loss: 0.27385199069976807\n",
      "Step 534/1000, Loss: 0.22783350944519043, Validation Loss: 0.27374154329299927\n",
      "Step 535/1000, Loss: 0.22773247957229614, Validation Loss: 0.27363118529319763\n",
      "Step 536/1000, Loss: 0.22763150930404663, Validation Loss: 0.27352094650268555\n",
      "Step 537/1000, Loss: 0.22753068804740906, Validation Loss: 0.2734109163284302\n",
      "Step 538/1000, Loss: 0.22742986679077148, Validation Loss: 0.2733009159564972\n",
      "Step 539/1000, Loss: 0.2273291051387787, Validation Loss: 0.27319106459617615\n",
      "Step 540/1000, Loss: 0.22722849249839783, Validation Loss: 0.27308133244514465\n",
      "Step 541/1000, Loss: 0.22712790966033936, Validation Loss: 0.2729716897010803\n",
      "Step 542/1000, Loss: 0.22702738642692566, Validation Loss: 0.2728620767593384\n",
      "Step 543/1000, Loss: 0.22692695260047913, Validation Loss: 0.272752583026886\n",
      "Step 544/1000, Loss: 0.22682657837867737, Validation Loss: 0.27264323830604553\n",
      "Step 545/1000, Loss: 0.22672627866268158, Validation Loss: 0.27253398299217224\n",
      "Step 546/1000, Loss: 0.22662605345249176, Validation Loss: 0.2724248766899109\n",
      "Step 547/1000, Loss: 0.22652588784694672, Validation Loss: 0.2723158299922943\n",
      "Step 548/1000, Loss: 0.22642585635185242, Validation Loss: 0.27220696210861206\n",
      "Step 549/1000, Loss: 0.22632580995559692, Validation Loss: 0.2720981538295746\n",
      "Step 550/1000, Loss: 0.22622588276863098, Validation Loss: 0.2719894349575043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 551/1000, Loss: 0.22612600028514862, Validation Loss: 0.2718808054924011\n",
      "Step 552/1000, Loss: 0.22602619230747223, Validation Loss: 0.27177220582962036\n",
      "Step 553/1000, Loss: 0.225926473736763, Validation Loss: 0.2716638147830963\n",
      "Step 554/1000, Loss: 0.22582677006721497, Validation Loss: 0.27155551314353943\n",
      "Step 555/1000, Loss: 0.22572720050811768, Validation Loss: 0.2714473009109497\n",
      "Step 556/1000, Loss: 0.22562766075134277, Validation Loss: 0.2713392376899719\n",
      "Step 557/1000, Loss: 0.22552824020385742, Validation Loss: 0.27123120427131653\n",
      "Step 558/1000, Loss: 0.22542884945869446, Validation Loss: 0.27112334966659546\n",
      "Step 559/1000, Loss: 0.22532951831817627, Validation Loss: 0.2710154950618744\n",
      "Step 560/1000, Loss: 0.22523029148578644, Validation Loss: 0.27090781927108765\n",
      "Step 561/1000, Loss: 0.22513112425804138, Validation Loss: 0.27080029249191284\n",
      "Step 562/1000, Loss: 0.2250320315361023, Validation Loss: 0.27069273591041565\n",
      "Step 563/1000, Loss: 0.2249329835176468, Validation Loss: 0.2705853581428528\n",
      "Step 564/1000, Loss: 0.22483401000499725, Validation Loss: 0.27047809958457947\n",
      "Step 565/1000, Loss: 0.22473512589931488, Validation Loss: 0.27037087082862854\n",
      "Step 566/1000, Loss: 0.2246362864971161, Validation Loss: 0.27026376128196716\n",
      "Step 567/1000, Loss: 0.22453752160072327, Validation Loss: 0.27015677094459534\n",
      "Step 568/1000, Loss: 0.2244388312101364, Validation Loss: 0.27004989981651306\n",
      "Step 569/1000, Loss: 0.22434023022651672, Validation Loss: 0.26994314789772034\n",
      "Step 570/1000, Loss: 0.22424162924289703, Validation Loss: 0.2698364555835724\n",
      "Step 571/1000, Loss: 0.2241431623697281, Validation Loss: 0.2697299122810364\n",
      "Step 572/1000, Loss: 0.22404474020004272, Validation Loss: 0.26962336897850037\n",
      "Step 573/1000, Loss: 0.22394637763500214, Validation Loss: 0.2695170044898987\n",
      "Step 574/1000, Loss: 0.22384805977344513, Validation Loss: 0.2694106996059418\n",
      "Step 575/1000, Loss: 0.22374987602233887, Validation Loss: 0.2693045437335968\n",
      "Step 576/1000, Loss: 0.2236517369747162, Validation Loss: 0.2691984176635742\n",
      "Step 577/1000, Loss: 0.2235536277294159, Validation Loss: 0.2690924108028412\n",
      "Step 578/1000, Loss: 0.22345562279224396, Validation Loss: 0.26898661255836487\n",
      "Step 579/1000, Loss: 0.2233576625585556, Validation Loss: 0.26888078451156616\n",
      "Step 580/1000, Loss: 0.22325973212718964, Validation Loss: 0.2687750458717346\n",
      "Step 581/1000, Loss: 0.2231619656085968, Validation Loss: 0.2686694860458374\n",
      "Step 582/1000, Loss: 0.22306418418884277, Validation Loss: 0.26856398582458496\n",
      "Step 583/1000, Loss: 0.2229665219783783, Validation Loss: 0.2684585452079773\n",
      "Step 584/1000, Loss: 0.2228688895702362, Validation Loss: 0.26835325360298157\n",
      "Step 585/1000, Loss: 0.2227713167667389, Validation Loss: 0.2682480812072754\n",
      "Step 586/1000, Loss: 0.22267387807369232, Validation Loss: 0.268142968416214\n",
      "Step 587/1000, Loss: 0.22257640957832336, Validation Loss: 0.2680380344390869\n",
      "Step 588/1000, Loss: 0.22247906029224396, Validation Loss: 0.26793310046195984\n",
      "Step 589/1000, Loss: 0.22238177061080933, Validation Loss: 0.2678282558917999\n",
      "Step 590/1000, Loss: 0.22228452563285828, Validation Loss: 0.26772356033325195\n",
      "Step 591/1000, Loss: 0.22218739986419678, Validation Loss: 0.26761892437934875\n",
      "Step 592/1000, Loss: 0.22209030389785767, Validation Loss: 0.2675143778324127\n",
      "Step 593/1000, Loss: 0.22199323773384094, Validation Loss: 0.26740995049476624\n",
      "Step 594/1000, Loss: 0.22189629077911377, Validation Loss: 0.2673056423664093\n",
      "Step 595/1000, Loss: 0.22179938852787018, Validation Loss: 0.26720142364501953\n",
      "Step 596/1000, Loss: 0.22170253098011017, Validation Loss: 0.2670973241329193\n",
      "Step 597/1000, Loss: 0.22160577774047852, Validation Loss: 0.2669932544231415\n",
      "Step 598/1000, Loss: 0.22150908410549164, Validation Loss: 0.2668893039226532\n",
      "Step 599/1000, Loss: 0.22141247987747192, Validation Loss: 0.2667854428291321\n",
      "Step 600/1000, Loss: 0.2213158756494522, Validation Loss: 0.2666817009449005\n",
      "Step 601/1000, Loss: 0.22121936082839966, Validation Loss: 0.2665780484676361\n",
      "Step 602/1000, Loss: 0.22112290561199188, Validation Loss: 0.2665262818336487\n",
      "Step 603/1000, Loss: 0.22107470035552979, Validation Loss: 0.26647451519966125\n",
      "Step 604/1000, Loss: 0.22102653980255127, Validation Loss: 0.26642274856567383\n",
      "Step 605/1000, Loss: 0.22097840905189514, Validation Loss: 0.26637110114097595\n",
      "Step 606/1000, Loss: 0.220930278301239, Validation Loss: 0.2663194239139557\n",
      "Step 607/1000, Loss: 0.22088216245174408, Validation Loss: 0.2662677764892578\n",
      "Step 608/1000, Loss: 0.22083407640457153, Validation Loss: 0.2662162482738495\n",
      "Step 609/1000, Loss: 0.22078603506088257, Validation Loss: 0.2661646604537964\n",
      "Step 610/1000, Loss: 0.22073794901371002, Validation Loss: 0.26611313223838806\n",
      "Step 611/1000, Loss: 0.22068993747234344, Validation Loss: 0.2660616338253021\n",
      "Step 612/1000, Loss: 0.22064195573329926, Validation Loss: 0.2660101354122162\n",
      "Step 613/1000, Loss: 0.22059392929077148, Validation Loss: 0.2659587562084198\n",
      "Step 614/1000, Loss: 0.22054600715637207, Validation Loss: 0.26590731739997864\n",
      "Step 615/1000, Loss: 0.22049804031848907, Validation Loss: 0.26585590839385986\n",
      "Step 616/1000, Loss: 0.22045008838176727, Validation Loss: 0.2658045291900635\n",
      "Step 617/1000, Loss: 0.22040215134620667, Validation Loss: 0.26575326919555664\n",
      "Step 618/1000, Loss: 0.22035427391529083, Validation Loss: 0.26570194959640503\n",
      "Step 619/1000, Loss: 0.220306396484375, Validation Loss: 0.2656506597995758\n",
      "Step 620/1000, Loss: 0.22025850415229797, Validation Loss: 0.2655993103981018\n",
      "Step 621/1000, Loss: 0.22021065652370453, Validation Loss: 0.26554811000823975\n",
      "Step 622/1000, Loss: 0.22016280889511108, Validation Loss: 0.2654969096183777\n",
      "Step 623/1000, Loss: 0.22011499106884003, Validation Loss: 0.2654457688331604\n",
      "Step 624/1000, Loss: 0.22006720304489136, Validation Loss: 0.2653946280479431\n",
      "Step 625/1000, Loss: 0.22001942992210388, Validation Loss: 0.26534348726272583\n",
      "Step 626/1000, Loss: 0.21997162699699402, Validation Loss: 0.26529234647750854\n",
      "Step 627/1000, Loss: 0.21992391347885132, Validation Loss: 0.26524123549461365\n",
      "Step 628/1000, Loss: 0.21987617015838623, Validation Loss: 0.2651902139186859\n",
      "Step 629/1000, Loss: 0.21982845664024353, Validation Loss: 0.2651391923427582\n",
      "Step 630/1000, Loss: 0.21978075802326202, Validation Loss: 0.26508820056915283\n",
      "Step 631/1000, Loss: 0.21973305940628052, Validation Loss: 0.2650371789932251\n",
      "Step 632/1000, Loss: 0.2196853756904602, Validation Loss: 0.2649862468242645\n",
      "Step 633/1000, Loss: 0.21963773667812347, Validation Loss: 0.26493531465530396\n",
      "Step 634/1000, Loss: 0.21959011256694794, Validation Loss: 0.26488444209098816\n",
      "Step 635/1000, Loss: 0.2195424884557724, Validation Loss: 0.26483359932899475\n",
      "Step 636/1000, Loss: 0.21949484944343567, Validation Loss: 0.26478275656700134\n",
      "Step 637/1000, Loss: 0.2194472700357437, Validation Loss: 0.26473191380500793\n",
      "Step 638/1000, Loss: 0.21939970552921295, Validation Loss: 0.2646810710430145\n",
      "Step 639/1000, Loss: 0.21935215592384338, Validation Loss: 0.2646302878856659\n",
      "Step 640/1000, Loss: 0.21930459141731262, Validation Loss: 0.26457953453063965\n",
      "Step 641/1000, Loss: 0.21925702691078186, Validation Loss: 0.2645287811756134\n",
      "Step 642/1000, Loss: 0.21920952200889587, Validation Loss: 0.26447805762290955\n",
      "Step 643/1000, Loss: 0.21916204690933228, Validation Loss: 0.2644273340702057\n",
      "Step 644/1000, Loss: 0.21911455690860748, Validation Loss: 0.264376699924469\n",
      "Step 645/1000, Loss: 0.21906708180904388, Validation Loss: 0.2643260359764099\n",
      "Step 646/1000, Loss: 0.2190195918083191, Validation Loss: 0.264275461435318\n",
      "Step 647/1000, Loss: 0.21897216141223907, Validation Loss: 0.2642247676849365\n",
      "Step 648/1000, Loss: 0.21892468631267548, Validation Loss: 0.2641741931438446\n",
      "Step 649/1000, Loss: 0.21887728571891785, Validation Loss: 0.2641236484050751\n",
      "Step 650/1000, Loss: 0.21882988512516022, Validation Loss: 0.26407313346862793\n",
      "Step 651/1000, Loss: 0.21878249943256378, Validation Loss: 0.2640226483345032\n",
      "Step 652/1000, Loss: 0.21873514354228973, Validation Loss: 0.2639721632003784\n",
      "Step 653/1000, Loss: 0.21868778765201569, Validation Loss: 0.2639216184616089\n",
      "Step 654/1000, Loss: 0.21864041686058044, Validation Loss: 0.2638711929321289\n",
      "Step 655/1000, Loss: 0.21859309077262878, Validation Loss: 0.2638207674026489\n",
      "Step 656/1000, Loss: 0.2185457944869995, Validation Loss: 0.26377037167549133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 657/1000, Loss: 0.21849851310253143, Validation Loss: 0.26371994614601135\n",
      "Step 658/1000, Loss: 0.21845124661922455, Validation Loss: 0.26366958022117615\n",
      "Step 659/1000, Loss: 0.21840399503707886, Validation Loss: 0.2636192739009857\n",
      "Step 660/1000, Loss: 0.21835672855377197, Validation Loss: 0.2635689675807953\n",
      "Step 661/1000, Loss: 0.21830949187278748, Validation Loss: 0.26351866126060486\n",
      "Step 662/1000, Loss: 0.21826228499412537, Validation Loss: 0.2634683847427368\n",
      "Step 663/1000, Loss: 0.21821506321430206, Validation Loss: 0.26341813802719116\n",
      "Step 664/1000, Loss: 0.21816790103912354, Validation Loss: 0.2633679509162903\n",
      "Step 665/1000, Loss: 0.2181207686662674, Validation Loss: 0.263317734003067\n",
      "Step 666/1000, Loss: 0.21807359158992767, Validation Loss: 0.2632676362991333\n",
      "Step 667/1000, Loss: 0.21802644431591034, Validation Loss: 0.2632174789905548\n",
      "Step 668/1000, Loss: 0.2179793119430542, Validation Loss: 0.2631673514842987\n",
      "Step 669/1000, Loss: 0.21793219447135925, Validation Loss: 0.2631171643733978\n",
      "Step 670/1000, Loss: 0.2178850620985031, Validation Loss: 0.2630670666694641\n",
      "Step 671/1000, Loss: 0.21783795952796936, Validation Loss: 0.2630169689655304\n",
      "Step 672/1000, Loss: 0.2177909016609192, Validation Loss: 0.2629668712615967\n",
      "Step 673/1000, Loss: 0.21774382889270782, Validation Loss: 0.26291680335998535\n",
      "Step 674/1000, Loss: 0.21769681572914124, Validation Loss: 0.262866735458374\n",
      "Step 675/1000, Loss: 0.21764975786209106, Validation Loss: 0.26281675696372986\n",
      "Step 676/1000, Loss: 0.21760275959968567, Validation Loss: 0.2627667784690857\n",
      "Step 677/1000, Loss: 0.21755576133728027, Validation Loss: 0.26271679997444153\n",
      "Step 678/1000, Loss: 0.21750874817371368, Validation Loss: 0.26266688108444214\n",
      "Step 679/1000, Loss: 0.21746176481246948, Validation Loss: 0.26261696219444275\n",
      "Step 680/1000, Loss: 0.21741478145122528, Validation Loss: 0.26256707310676575\n",
      "Step 681/1000, Loss: 0.21736784279346466, Validation Loss: 0.26251718401908875\n",
      "Step 682/1000, Loss: 0.21732090413570404, Validation Loss: 0.2624673545360565\n",
      "Step 683/1000, Loss: 0.2172739952802658, Validation Loss: 0.2624174654483795\n",
      "Step 684/1000, Loss: 0.21722707152366638, Validation Loss: 0.2623676657676697\n",
      "Step 685/1000, Loss: 0.21718016266822815, Validation Loss: 0.26231783628463745\n",
      "Step 686/1000, Loss: 0.2171332687139511, Validation Loss: 0.26226806640625\n",
      "Step 687/1000, Loss: 0.21708638966083527, Validation Loss: 0.26221829652786255\n",
      "Step 688/1000, Loss: 0.2170395404100418, Validation Loss: 0.2621685266494751\n",
      "Step 689/1000, Loss: 0.21699273586273193, Validation Loss: 0.2621188461780548\n",
      "Step 690/1000, Loss: 0.21694590151309967, Validation Loss: 0.2620691657066345\n",
      "Step 691/1000, Loss: 0.21689915657043457, Validation Loss: 0.2620195150375366\n",
      "Step 692/1000, Loss: 0.2168523520231247, Validation Loss: 0.2619698643684387\n",
      "Step 693/1000, Loss: 0.216805562376976, Validation Loss: 0.2619202733039856\n",
      "Step 694/1000, Loss: 0.2167588174343109, Validation Loss: 0.2618706226348877\n",
      "Step 695/1000, Loss: 0.21671204268932343, Validation Loss: 0.26182109117507935\n",
      "Step 696/1000, Loss: 0.2166653424501419, Validation Loss: 0.2617715895175934\n",
      "Step 697/1000, Loss: 0.2166185826063156, Validation Loss: 0.26172202825546265\n",
      "Step 698/1000, Loss: 0.21657192707061768, Validation Loss: 0.26167258620262146\n",
      "Step 699/1000, Loss: 0.21652524173259735, Validation Loss: 0.2616230845451355\n",
      "Step 700/1000, Loss: 0.21647855639457703, Validation Loss: 0.2615736126899719\n",
      "Step 701/1000, Loss: 0.21643194556236267, Validation Loss: 0.26152411103248596\n",
      "Step 702/1000, Loss: 0.21638526022434235, Validation Loss: 0.26149943470954895\n",
      "Step 703/1000, Loss: 0.21636195480823517, Validation Loss: 0.26147472858428955\n",
      "Step 704/1000, Loss: 0.2163386195898056, Validation Loss: 0.26145005226135254\n",
      "Step 705/1000, Loss: 0.21631531417369843, Validation Loss: 0.2614253759384155\n",
      "Step 706/1000, Loss: 0.21629203855991364, Validation Loss: 0.26140066981315613\n",
      "Step 707/1000, Loss: 0.21626873314380646, Validation Loss: 0.26137596368789673\n",
      "Step 708/1000, Loss: 0.21624542772769928, Validation Loss: 0.26135125756263733\n",
      "Step 709/1000, Loss: 0.2162221223115921, Validation Loss: 0.2613266110420227\n",
      "Step 710/1000, Loss: 0.2161988615989685, Validation Loss: 0.2613019347190857\n",
      "Step 711/1000, Loss: 0.21617552638053894, Validation Loss: 0.2612772583961487\n",
      "Step 712/1000, Loss: 0.21615226566791534, Validation Loss: 0.26125261187553406\n",
      "Step 713/1000, Loss: 0.21612900495529175, Validation Loss: 0.26122793555259705\n",
      "Step 714/1000, Loss: 0.21610571444034576, Validation Loss: 0.2612032890319824\n",
      "Step 715/1000, Loss: 0.21608248353004456, Validation Loss: 0.2611786425113678\n",
      "Step 716/1000, Loss: 0.21605919301509857, Validation Loss: 0.26115402579307556\n",
      "Step 717/1000, Loss: 0.21603594720363617, Validation Loss: 0.2611294090747833\n",
      "Step 718/1000, Loss: 0.21601268649101257, Validation Loss: 0.2611047923564911\n",
      "Step 719/1000, Loss: 0.21598942577838898, Validation Loss: 0.26108020544052124\n",
      "Step 720/1000, Loss: 0.21596617996692657, Validation Loss: 0.2610556483268738\n",
      "Step 721/1000, Loss: 0.21594294905662537, Validation Loss: 0.26103103160858154\n",
      "Step 722/1000, Loss: 0.21591970324516296, Validation Loss: 0.2610063850879669\n",
      "Step 723/1000, Loss: 0.21589645743370056, Validation Loss: 0.26098179817199707\n",
      "Step 724/1000, Loss: 0.21587324142456055, Validation Loss: 0.2609572112560272\n",
      "Step 725/1000, Loss: 0.21585004031658173, Validation Loss: 0.2609326243400574\n",
      "Step 726/1000, Loss: 0.21582680940628052, Validation Loss: 0.26090800762176514\n",
      "Step 727/1000, Loss: 0.2158035784959793, Validation Loss: 0.26088348031044006\n",
      "Step 728/1000, Loss: 0.21578039228916168, Validation Loss: 0.2608588635921478\n",
      "Step 729/1000, Loss: 0.21575716137886047, Validation Loss: 0.260834276676178\n",
      "Step 730/1000, Loss: 0.21573393046855927, Validation Loss: 0.2608097493648529\n",
      "Step 731/1000, Loss: 0.21571074426174164, Validation Loss: 0.26078516244888306\n",
      "Step 732/1000, Loss: 0.215687558054924, Validation Loss: 0.260760635137558\n",
      "Step 733/1000, Loss: 0.2156643271446228, Validation Loss: 0.26073604822158813\n",
      "Step 734/1000, Loss: 0.2156411111354828, Validation Loss: 0.26071152091026306\n",
      "Step 735/1000, Loss: 0.21561791002750397, Validation Loss: 0.2606870234012604\n",
      "Step 736/1000, Loss: 0.21559470891952515, Validation Loss: 0.2606625258922577\n",
      "Step 737/1000, Loss: 0.21557149291038513, Validation Loss: 0.2606379985809326\n",
      "Step 738/1000, Loss: 0.2155483067035675, Validation Loss: 0.26061350107192993\n",
      "Step 739/1000, Loss: 0.21552512049674988, Validation Loss: 0.26058894395828247\n",
      "Step 740/1000, Loss: 0.21550188958644867, Validation Loss: 0.2605644166469574\n",
      "Step 741/1000, Loss: 0.21547871828079224, Validation Loss: 0.2605398893356323\n",
      "Step 742/1000, Loss: 0.215455561876297, Validation Loss: 0.26051539182662964\n",
      "Step 743/1000, Loss: 0.21543234586715698, Validation Loss: 0.2604908347129822\n",
      "Step 744/1000, Loss: 0.21540920436382294, Validation Loss: 0.2604663670063019\n",
      "Step 745/1000, Loss: 0.21538598835468292, Validation Loss: 0.2604418992996216\n",
      "Step 746/1000, Loss: 0.2153628170490265, Validation Loss: 0.2604173719882965\n",
      "Step 747/1000, Loss: 0.21533964574337006, Validation Loss: 0.2603929340839386\n",
      "Step 748/1000, Loss: 0.2153165191411972, Validation Loss: 0.2603684365749359\n",
      "Step 749/1000, Loss: 0.21529333293437958, Validation Loss: 0.260343998670578\n",
      "Step 750/1000, Loss: 0.21527016162872314, Validation Loss: 0.2603195607662201\n",
      "Step 751/1000, Loss: 0.2152469903230667, Validation Loss: 0.2602950632572174\n",
      "Step 752/1000, Loss: 0.21522381901741028, Validation Loss: 0.2602706253528595\n",
      "Step 753/1000, Loss: 0.21520067751407623, Validation Loss: 0.2602461278438568\n",
      "Step 754/1000, Loss: 0.2151775360107422, Validation Loss: 0.2602216601371765\n",
      "Step 755/1000, Loss: 0.21515439450740814, Validation Loss: 0.2601972222328186\n",
      "Step 756/1000, Loss: 0.2151312381029129, Validation Loss: 0.2601727843284607\n",
      "Step 757/1000, Loss: 0.21510812640190125, Validation Loss: 0.2601483166217804\n",
      "Step 758/1000, Loss: 0.2150849848985672, Validation Loss: 0.2601238787174225\n",
      "Step 759/1000, Loss: 0.21506182849407196, Validation Loss: 0.2600994110107422\n",
      "Step 760/1000, Loss: 0.21503868699073792, Validation Loss: 0.2600749731063843\n",
      "Step 761/1000, Loss: 0.21501557528972626, Validation Loss: 0.26005056500434875\n",
      "Step 762/1000, Loss: 0.21499241888523102, Validation Loss: 0.26002615690231323\n",
      "Step 763/1000, Loss: 0.21496933698654175, Validation Loss: 0.2600017189979553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 764/1000, Loss: 0.2149461805820465, Validation Loss: 0.2599772810935974\n",
      "Step 765/1000, Loss: 0.21492305397987366, Validation Loss: 0.2599529027938843\n",
      "Step 766/1000, Loss: 0.214899942278862, Validation Loss: 0.25992852449417114\n",
      "Step 767/1000, Loss: 0.21487683057785034, Validation Loss: 0.25990408658981323\n",
      "Step 768/1000, Loss: 0.21485371887683868, Validation Loss: 0.2598797082901001\n",
      "Step 769/1000, Loss: 0.21483056247234344, Validation Loss: 0.2598553001880646\n",
      "Step 770/1000, Loss: 0.21480749547481537, Validation Loss: 0.25983095169067383\n",
      "Step 771/1000, Loss: 0.2147843837738037, Validation Loss: 0.2598065733909607\n",
      "Step 772/1000, Loss: 0.21476125717163086, Validation Loss: 0.25978219509124756\n",
      "Step 773/1000, Loss: 0.2147381752729416, Validation Loss: 0.2597578167915344\n",
      "Step 774/1000, Loss: 0.21471509337425232, Validation Loss: 0.2597334384918213\n",
      "Step 775/1000, Loss: 0.21469199657440186, Validation Loss: 0.25970908999443054\n",
      "Step 776/1000, Loss: 0.2146688997745514, Validation Loss: 0.2596847116947174\n",
      "Step 777/1000, Loss: 0.21464583277702332, Validation Loss: 0.2596603333950043\n",
      "Step 778/1000, Loss: 0.21462272107601166, Validation Loss: 0.2596360146999359\n",
      "Step 779/1000, Loss: 0.21459966897964478, Validation Loss: 0.2596116364002228\n",
      "Step 780/1000, Loss: 0.21457655727863312, Validation Loss: 0.2595873177051544\n",
      "Step 781/1000, Loss: 0.21455349028110504, Validation Loss: 0.2595629096031189\n",
      "Step 782/1000, Loss: 0.21453039348125458, Validation Loss: 0.2595386505126953\n",
      "Step 783/1000, Loss: 0.2145073413848877, Validation Loss: 0.25951430201530457\n",
      "Step 784/1000, Loss: 0.2144842892885208, Validation Loss: 0.25948992371559143\n",
      "Step 785/1000, Loss: 0.21446119248867035, Validation Loss: 0.2594655752182007\n",
      "Step 786/1000, Loss: 0.21443809568881989, Validation Loss: 0.25944119691848755\n",
      "Step 787/1000, Loss: 0.214415043592453, Validation Loss: 0.2594168782234192\n",
      "Step 788/1000, Loss: 0.21439197659492493, Validation Loss: 0.25939247012138367\n",
      "Step 789/1000, Loss: 0.21436892449855804, Validation Loss: 0.2593681812286377\n",
      "Step 790/1000, Loss: 0.21434587240219116, Validation Loss: 0.25934386253356934\n",
      "Step 791/1000, Loss: 0.21432280540466309, Validation Loss: 0.2593194842338562\n",
      "Step 792/1000, Loss: 0.2142997682094574, Validation Loss: 0.2592952251434326\n",
      "Step 793/1000, Loss: 0.21427671611309052, Validation Loss: 0.25927096605300903\n",
      "Step 794/1000, Loss: 0.21425367891788483, Validation Loss: 0.2592466473579407\n",
      "Step 795/1000, Loss: 0.21423064172267914, Validation Loss: 0.2592223286628723\n",
      "Step 796/1000, Loss: 0.21420758962631226, Validation Loss: 0.25919806957244873\n",
      "Step 797/1000, Loss: 0.21418455243110657, Validation Loss: 0.25917378067970276\n",
      "Step 798/1000, Loss: 0.21416153013706207, Validation Loss: 0.2591494917869568\n",
      "Step 799/1000, Loss: 0.21413850784301758, Validation Loss: 0.2591252326965332\n",
      "Step 800/1000, Loss: 0.2141154706478119, Validation Loss: 0.2591009736061096\n",
      "Step 801/1000, Loss: 0.2140924632549286, Validation Loss: 0.25907671451568604\n",
      "Step 802/1000, Loss: 0.2140694260597229, Validation Loss: 0.25906458497047424\n",
      "Step 803/1000, Loss: 0.21405793726444244, Validation Loss: 0.25905242562294006\n",
      "Step 804/1000, Loss: 0.214046448469162, Validation Loss: 0.25904029607772827\n",
      "Step 805/1000, Loss: 0.21403492987155914, Validation Loss: 0.2590281665325165\n",
      "Step 806/1000, Loss: 0.21402345597743988, Validation Loss: 0.25901609659194946\n",
      "Step 807/1000, Loss: 0.21401193737983704, Validation Loss: 0.2590039372444153\n",
      "Step 808/1000, Loss: 0.2140004187822342, Validation Loss: 0.2589917778968811\n",
      "Step 809/1000, Loss: 0.21398892998695374, Validation Loss: 0.2589796781539917\n",
      "Step 810/1000, Loss: 0.2139774113893509, Validation Loss: 0.2589675784111023\n",
      "Step 811/1000, Loss: 0.21396590769290924, Validation Loss: 0.2589554488658905\n",
      "Step 812/1000, Loss: 0.2139544039964676, Validation Loss: 0.2589433491230011\n",
      "Step 813/1000, Loss: 0.21394288539886475, Validation Loss: 0.2589312195777893\n",
      "Step 814/1000, Loss: 0.2139313966035843, Validation Loss: 0.2589191496372223\n",
      "Step 815/1000, Loss: 0.21391989290714264, Validation Loss: 0.2589070498943329\n",
      "Step 816/1000, Loss: 0.2139083743095398, Validation Loss: 0.2588949501514435\n",
      "Step 817/1000, Loss: 0.21389688551425934, Validation Loss: 0.2588828206062317\n",
      "Step 818/1000, Loss: 0.21388541162014008, Validation Loss: 0.2588707208633423\n",
      "Step 819/1000, Loss: 0.21387390792369843, Validation Loss: 0.2588585615158081\n",
      "Step 820/1000, Loss: 0.21386241912841797, Validation Loss: 0.2588464617729187\n",
      "Step 821/1000, Loss: 0.2138509452342987, Validation Loss: 0.2588343620300293\n",
      "Step 822/1000, Loss: 0.21383944153785706, Validation Loss: 0.2588222920894623\n",
      "Step 823/1000, Loss: 0.2138279229402542, Validation Loss: 0.2588101625442505\n",
      "Step 824/1000, Loss: 0.21381643414497375, Validation Loss: 0.25879812240600586\n",
      "Step 825/1000, Loss: 0.2138049453496933, Validation Loss: 0.25878605246543884\n",
      "Step 826/1000, Loss: 0.21379345655441284, Validation Loss: 0.2587739825248718\n",
      "Step 827/1000, Loss: 0.2137819528579712, Validation Loss: 0.25876182317733765\n",
      "Step 828/1000, Loss: 0.21377043426036835, Validation Loss: 0.258749783039093\n",
      "Step 829/1000, Loss: 0.21375896036624908, Validation Loss: 0.258737713098526\n",
      "Step 830/1000, Loss: 0.21374748647212982, Validation Loss: 0.2587256133556366\n",
      "Step 831/1000, Loss: 0.21373598277568817, Validation Loss: 0.2587135434150696\n",
      "Step 832/1000, Loss: 0.21372446417808533, Validation Loss: 0.2587013840675354\n",
      "Step 833/1000, Loss: 0.21371299028396606, Validation Loss: 0.2586892545223236\n",
      "Step 834/1000, Loss: 0.21370148658752441, Validation Loss: 0.2586771249771118\n",
      "Step 835/1000, Loss: 0.21368998289108276, Validation Loss: 0.25866496562957764\n",
      "Step 836/1000, Loss: 0.2136785238981247, Validation Loss: 0.25865286588668823\n",
      "Step 837/1000, Loss: 0.21366702020168304, Validation Loss: 0.25864076614379883\n",
      "Step 838/1000, Loss: 0.2136555016040802, Validation Loss: 0.25862863659858704\n",
      "Step 839/1000, Loss: 0.21364401280879974, Validation Loss: 0.25861653685569763\n",
      "Step 840/1000, Loss: 0.21363255381584167, Validation Loss: 0.25860440731048584\n",
      "Step 841/1000, Loss: 0.21362106502056122, Validation Loss: 0.25859227776527405\n",
      "Step 842/1000, Loss: 0.21360957622528076, Validation Loss: 0.25858017802238464\n",
      "Step 843/1000, Loss: 0.2135980725288391, Validation Loss: 0.25856804847717285\n",
      "Step 844/1000, Loss: 0.21358659863471985, Validation Loss: 0.25855597853660583\n",
      "Step 845/1000, Loss: 0.2135750949382782, Validation Loss: 0.25854387879371643\n",
      "Step 846/1000, Loss: 0.21356360614299774, Validation Loss: 0.258531779050827\n",
      "Step 847/1000, Loss: 0.2135521024465561, Validation Loss: 0.25851961970329285\n",
      "Step 848/1000, Loss: 0.21354062855243683, Validation Loss: 0.25850751996040344\n",
      "Step 849/1000, Loss: 0.21352912485599518, Validation Loss: 0.25849542021751404\n",
      "Step 850/1000, Loss: 0.2135176658630371, Validation Loss: 0.25848332047462463\n",
      "Step 851/1000, Loss: 0.21350617706775665, Validation Loss: 0.25847122073173523\n",
      "Step 852/1000, Loss: 0.2134946882724762, Validation Loss: 0.25845909118652344\n",
      "Step 853/1000, Loss: 0.21348321437835693, Validation Loss: 0.2584470510482788\n",
      "Step 854/1000, Loss: 0.21347174048423767, Validation Loss: 0.25843489170074463\n",
      "Step 855/1000, Loss: 0.2134602665901184, Validation Loss: 0.2584227919578552\n",
      "Step 856/1000, Loss: 0.21344877779483795, Validation Loss: 0.2584106922149658\n",
      "Step 857/1000, Loss: 0.21343731880187988, Validation Loss: 0.25839856266975403\n",
      "Step 858/1000, Loss: 0.21342584490776062, Validation Loss: 0.258386492729187\n",
      "Step 859/1000, Loss: 0.21341434121131897, Validation Loss: 0.2583743929862976\n",
      "Step 860/1000, Loss: 0.2134028971195221, Validation Loss: 0.2583622634410858\n",
      "Step 861/1000, Loss: 0.21339139342308044, Validation Loss: 0.2583501935005188\n",
      "Step 862/1000, Loss: 0.2133798897266388, Validation Loss: 0.2583381235599518\n",
      "Step 863/1000, Loss: 0.21336843073368073, Validation Loss: 0.25832599401474\n",
      "Step 864/1000, Loss: 0.21335694193840027, Validation Loss: 0.258313924074173\n",
      "Step 865/1000, Loss: 0.213345468044281, Validation Loss: 0.2583017945289612\n",
      "Step 866/1000, Loss: 0.21333397924900055, Validation Loss: 0.2582896947860718\n",
      "Step 867/1000, Loss: 0.21332252025604248, Validation Loss: 0.25827765464782715\n",
      "Step 868/1000, Loss: 0.21331103146076202, Validation Loss: 0.25826552510261536\n",
      "Step 869/1000, Loss: 0.21329952776432037, Validation Loss: 0.25825345516204834\n",
      "Step 870/1000, Loss: 0.2132880538702011, Validation Loss: 0.2582413852214813\n",
      "Step 871/1000, Loss: 0.21327659487724304, Validation Loss: 0.25822925567626953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 872/1000, Loss: 0.2132650911808014, Validation Loss: 0.2582172155380249\n",
      "Step 873/1000, Loss: 0.21325361728668213, Validation Loss: 0.2582050859928131\n",
      "Step 874/1000, Loss: 0.21324215829372406, Validation Loss: 0.2581930458545685\n",
      "Step 875/1000, Loss: 0.2132306545972824, Validation Loss: 0.2581809461116791\n",
      "Step 876/1000, Loss: 0.21321919560432434, Validation Loss: 0.25816890597343445\n",
      "Step 877/1000, Loss: 0.21320775151252747, Validation Loss: 0.25815683603286743\n",
      "Step 878/1000, Loss: 0.21319624781608582, Validation Loss: 0.2581447958946228\n",
      "Step 879/1000, Loss: 0.21318477392196655, Validation Loss: 0.2581326961517334\n",
      "Step 880/1000, Loss: 0.21317331492900848, Validation Loss: 0.258120596408844\n",
      "Step 881/1000, Loss: 0.21316181123256683, Validation Loss: 0.258108526468277\n",
      "Step 882/1000, Loss: 0.21315032243728638, Validation Loss: 0.25809645652770996\n",
      "Step 883/1000, Loss: 0.21313883364200592, Validation Loss: 0.25808435678482056\n",
      "Step 884/1000, Loss: 0.21312737464904785, Validation Loss: 0.2580723464488983\n",
      "Step 885/1000, Loss: 0.21311591565608978, Validation Loss: 0.2580602467060089\n",
      "Step 886/1000, Loss: 0.21310444176197052, Validation Loss: 0.2580481469631195\n",
      "Step 887/1000, Loss: 0.21309296786785126, Validation Loss: 0.2580361068248749\n",
      "Step 888/1000, Loss: 0.2130814492702484, Validation Loss: 0.2580240070819855\n",
      "Step 889/1000, Loss: 0.21306999027729034, Validation Loss: 0.25801193714141846\n",
      "Step 890/1000, Loss: 0.21305853128433228, Validation Loss: 0.25799989700317383\n",
      "Step 891/1000, Loss: 0.21304701268672943, Validation Loss: 0.2579878568649292\n",
      "Step 892/1000, Loss: 0.21303556859493256, Validation Loss: 0.2579757869243622\n",
      "Step 893/1000, Loss: 0.2130240947008133, Validation Loss: 0.25796371698379517\n",
      "Step 894/1000, Loss: 0.21301259100437164, Validation Loss: 0.25795167684555054\n",
      "Step 895/1000, Loss: 0.21300114691257477, Validation Loss: 0.25793957710266113\n",
      "Step 896/1000, Loss: 0.2129896879196167, Validation Loss: 0.2579275667667389\n",
      "Step 897/1000, Loss: 0.21297819912433624, Validation Loss: 0.2579154074192047\n",
      "Step 898/1000, Loss: 0.21296672523021698, Validation Loss: 0.2579033076763153\n",
      "Step 899/1000, Loss: 0.21295525133609772, Validation Loss: 0.2578912079334259\n",
      "Step 900/1000, Loss: 0.21294376254081726, Validation Loss: 0.2578791379928589\n",
      "Step 901/1000, Loss: 0.2129323035478592, Validation Loss: 0.2578669786453247\n",
      "Step 902/1000, Loss: 0.21292082965373993, Validation Loss: 0.2578609585762024\n",
      "Step 903/1000, Loss: 0.2129150927066803, Validation Loss: 0.2578548789024353\n",
      "Step 904/1000, Loss: 0.21290934085845947, Validation Loss: 0.2578487992286682\n",
      "Step 905/1000, Loss: 0.21290357410907745, Validation Loss: 0.2578427195549011\n",
      "Step 906/1000, Loss: 0.21289782226085663, Validation Loss: 0.2578366696834564\n",
      "Step 907/1000, Loss: 0.21289211511611938, Validation Loss: 0.25783059000968933\n",
      "Step 908/1000, Loss: 0.21288636326789856, Validation Loss: 0.25782451033592224\n",
      "Step 909/1000, Loss: 0.21288062632083893, Validation Loss: 0.25781843066215515\n",
      "Step 910/1000, Loss: 0.2128748595714569, Validation Loss: 0.25781235098838806\n",
      "Step 911/1000, Loss: 0.21286910772323608, Validation Loss: 0.2578062415122986\n",
      "Step 912/1000, Loss: 0.21286337077617645, Validation Loss: 0.2578001618385315\n",
      "Step 913/1000, Loss: 0.21285761892795563, Validation Loss: 0.2577941119670868\n",
      "Step 914/1000, Loss: 0.2128518670797348, Validation Loss: 0.2577880322933197\n",
      "Step 915/1000, Loss: 0.21284613013267517, Validation Loss: 0.2577819526195526\n",
      "Step 916/1000, Loss: 0.21284036338329315, Validation Loss: 0.2577758729457855\n",
      "Step 917/1000, Loss: 0.21283462643623352, Validation Loss: 0.2577698230743408\n",
      "Step 918/1000, Loss: 0.2128288894891739, Validation Loss: 0.25776374340057373\n",
      "Step 919/1000, Loss: 0.21282313764095306, Validation Loss: 0.25775763392448425\n",
      "Step 920/1000, Loss: 0.21281737089157104, Validation Loss: 0.2577515244483948\n",
      "Step 921/1000, Loss: 0.21281161904335022, Validation Loss: 0.2577454745769501\n",
      "Step 922/1000, Loss: 0.2128058820962906, Validation Loss: 0.25773942470550537\n",
      "Step 923/1000, Loss: 0.21280016005039215, Validation Loss: 0.2577333450317383\n",
      "Step 924/1000, Loss: 0.21279436349868774, Validation Loss: 0.2577272355556488\n",
      "Step 925/1000, Loss: 0.2127886265516281, Validation Loss: 0.2577211856842041\n",
      "Step 926/1000, Loss: 0.21278288960456848, Validation Loss: 0.257715106010437\n",
      "Step 927/1000, Loss: 0.21277712285518646, Validation Loss: 0.2577090263366699\n",
      "Step 928/1000, Loss: 0.21277137100696564, Validation Loss: 0.25770291686058044\n",
      "Step 929/1000, Loss: 0.21276560425758362, Validation Loss: 0.25769689679145813\n",
      "Step 930/1000, Loss: 0.21275991201400757, Validation Loss: 0.25769081711769104\n",
      "Step 931/1000, Loss: 0.21275413036346436, Validation Loss: 0.25768473744392395\n",
      "Step 932/1000, Loss: 0.21274837851524353, Validation Loss: 0.25767868757247925\n",
      "Step 933/1000, Loss: 0.2127426117658615, Validation Loss: 0.25767263770103455\n",
      "Step 934/1000, Loss: 0.21273687481880188, Validation Loss: 0.25766652822494507\n",
      "Step 935/1000, Loss: 0.21273115277290344, Validation Loss: 0.257660448551178\n",
      "Step 936/1000, Loss: 0.21272535622119904, Validation Loss: 0.2576543986797333\n",
      "Step 937/1000, Loss: 0.2127196192741394, Validation Loss: 0.2576483190059662\n",
      "Step 938/1000, Loss: 0.21271388232707977, Validation Loss: 0.2576422393321991\n",
      "Step 939/1000, Loss: 0.21270811557769775, Validation Loss: 0.257636159658432\n",
      "Step 940/1000, Loss: 0.21270236372947693, Validation Loss: 0.2576300799846649\n",
      "Step 941/1000, Loss: 0.2126966267824173, Validation Loss: 0.2576240301132202\n",
      "Step 942/1000, Loss: 0.21269090473651886, Validation Loss: 0.2576179802417755\n",
      "Step 943/1000, Loss: 0.21268513798713684, Validation Loss: 0.2576119005680084\n",
      "Step 944/1000, Loss: 0.21267937123775482, Validation Loss: 0.25760582089424133\n",
      "Step 945/1000, Loss: 0.2126736342906952, Validation Loss: 0.257599800825119\n",
      "Step 946/1000, Loss: 0.21266788244247437, Validation Loss: 0.25759369134902954\n",
      "Step 947/1000, Loss: 0.21266214549541473, Validation Loss: 0.25758764147758484\n",
      "Step 948/1000, Loss: 0.2126563936471939, Validation Loss: 0.25758153200149536\n",
      "Step 949/1000, Loss: 0.2126506268978119, Validation Loss: 0.25757551193237305\n",
      "Step 950/1000, Loss: 0.21264491975307465, Validation Loss: 0.25756946206092834\n",
      "Step 951/1000, Loss: 0.21263915300369263, Validation Loss: 0.25756341218948364\n",
      "Step 952/1000, Loss: 0.2126334011554718, Validation Loss: 0.25755733251571655\n",
      "Step 953/1000, Loss: 0.2126276195049286, Validation Loss: 0.25755131244659424\n",
      "Step 954/1000, Loss: 0.21262191236019135, Validation Loss: 0.25754523277282715\n",
      "Step 955/1000, Loss: 0.21261614561080933, Validation Loss: 0.25753915309906006\n",
      "Step 956/1000, Loss: 0.2126103639602661, Validation Loss: 0.25753307342529297\n",
      "Step 957/1000, Loss: 0.2126046121120453, Validation Loss: 0.25752705335617065\n",
      "Step 958/1000, Loss: 0.21259889006614685, Validation Loss: 0.25752100348472595\n",
      "Step 959/1000, Loss: 0.21259312331676483, Validation Loss: 0.25751495361328125\n",
      "Step 960/1000, Loss: 0.212587371468544, Validation Loss: 0.2575088441371918\n",
      "Step 961/1000, Loss: 0.212581604719162, Validation Loss: 0.25750282406806946\n",
      "Step 962/1000, Loss: 0.21257586777210236, Validation Loss: 0.25749680399894714\n",
      "Step 963/1000, Loss: 0.21257011592388153, Validation Loss: 0.25749072432518005\n",
      "Step 964/1000, Loss: 0.2125643491744995, Validation Loss: 0.25748464465141296\n",
      "Step 965/1000, Loss: 0.2125585824251175, Validation Loss: 0.25747862458229065\n",
      "Step 966/1000, Loss: 0.21255283057689667, Validation Loss: 0.25747254490852356\n",
      "Step 967/1000, Loss: 0.21254709362983704, Validation Loss: 0.25746649503707886\n",
      "Step 968/1000, Loss: 0.2125413417816162, Validation Loss: 0.25746041536331177\n",
      "Step 969/1000, Loss: 0.2125355452299118, Validation Loss: 0.25745439529418945\n",
      "Step 970/1000, Loss: 0.21252980828285217, Validation Loss: 0.25744831562042236\n",
      "Step 971/1000, Loss: 0.21252407133579254, Validation Loss: 0.25744229555130005\n",
      "Step 972/1000, Loss: 0.21251827478408813, Validation Loss: 0.25743618607521057\n",
      "Step 973/1000, Loss: 0.21251250803470612, Validation Loss: 0.25743016600608826\n",
      "Step 974/1000, Loss: 0.21250677108764648, Validation Loss: 0.25742408633232117\n",
      "Step 975/1000, Loss: 0.21250101923942566, Validation Loss: 0.25741806626319885\n",
      "Step 976/1000, Loss: 0.21249523758888245, Validation Loss: 0.25741198658943176\n",
      "Step 977/1000, Loss: 0.21248950064182281, Validation Loss: 0.25740596652030945\n",
      "Step 978/1000, Loss: 0.2124837338924408, Validation Loss: 0.25739988684654236\n",
      "Step 979/1000, Loss: 0.21247798204421997, Validation Loss: 0.25739380717277527\n",
      "Step 980/1000, Loss: 0.21247220039367676, Validation Loss: 0.25738775730133057\n",
      "Step 981/1000, Loss: 0.21246644854545593, Validation Loss: 0.2573816776275635\n",
      "Step 982/1000, Loss: 0.2124606817960739, Validation Loss: 0.25737565755844116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 983/1000, Loss: 0.2124549299478531, Validation Loss: 0.2573695778846741\n",
      "Step 984/1000, Loss: 0.21244916319847107, Validation Loss: 0.257363498210907\n",
      "Step 985/1000, Loss: 0.21244339644908905, Validation Loss: 0.25735747814178467\n",
      "Step 986/1000, Loss: 0.21243764460086823, Validation Loss: 0.2573513984680176\n",
      "Step 987/1000, Loss: 0.2124318778514862, Validation Loss: 0.25734537839889526\n",
      "Step 988/1000, Loss: 0.2124261111021042, Validation Loss: 0.2573392987251282\n",
      "Step 989/1000, Loss: 0.21242035925388336, Validation Loss: 0.25733324885368347\n",
      "Step 990/1000, Loss: 0.21241459250450134, Validation Loss: 0.2573271691799164\n",
      "Step 991/1000, Loss: 0.21240882575511932, Validation Loss: 0.25732114911079407\n",
      "Step 992/1000, Loss: 0.2124030739068985, Validation Loss: 0.25731509923934937\n",
      "Step 993/1000, Loss: 0.21239730715751648, Validation Loss: 0.25730904936790466\n",
      "Step 994/1000, Loss: 0.21239157021045685, Validation Loss: 0.25730299949645996\n",
      "Step 995/1000, Loss: 0.21238580346107483, Validation Loss: 0.25729694962501526\n",
      "Step 996/1000, Loss: 0.2123800814151764, Validation Loss: 0.25729089975357056\n",
      "Step 997/1000, Loss: 0.21237431466579437, Validation Loss: 0.25728487968444824\n",
      "Step 998/1000, Loss: 0.21236859261989594, Validation Loss: 0.2572788596153259\n",
      "Step 999/1000, Loss: 0.2123628556728363, Validation Loss: 0.25727272033691406\n",
      "Step 1000/1000, Loss: 0.21235708892345428, Validation Loss: 0.25726667046546936\n",
      "Step 1/1000, Loss: 0.3244418799877167, Validation Loss: 0.6409236192703247\n",
      "Step 2/1000, Loss: 0.3236101567745209, Validation Loss: 0.641655683517456\n",
      "Step 3/1000, Loss: 0.3227902352809906, Validation Loss: 0.6419258713722229\n",
      "Step 4/1000, Loss: 0.32198211550712585, Validation Loss: 0.6418284177780151\n",
      "Step 5/1000, Loss: 0.3211854100227356, Validation Loss: 0.6415178179740906\n",
      "Step 6/1000, Loss: 0.32039979100227356, Validation Loss: 0.6410838961601257\n",
      "Step 7/1000, Loss: 0.31962525844573975, Validation Loss: 0.6405792236328125\n",
      "Step 8/1000, Loss: 0.31886184215545654, Validation Loss: 0.6400423049926758\n",
      "Step 9/1000, Loss: 0.3181094825267792, Validation Loss: 0.6395136117935181\n",
      "Step 10/1000, Loss: 0.3173682391643524, Validation Loss: 0.6390392184257507\n",
      "Step 11/1000, Loss: 0.31663787364959717, Validation Loss: 0.6386640667915344\n",
      "Step 12/1000, Loss: 0.31591835618019104, Validation Loss: 0.6384146213531494\n",
      "Step 13/1000, Loss: 0.3152094781398773, Validation Loss: 0.638290286064148\n",
      "Step 14/1000, Loss: 0.3145109713077545, Validation Loss: 0.6382731795310974\n",
      "Step 15/1000, Loss: 0.3138226866722107, Validation Loss: 0.6383394002914429\n",
      "Step 16/1000, Loss: 0.3131442964076996, Validation Loss: 0.6384665966033936\n",
      "Step 17/1000, Loss: 0.31247562170028687, Validation Loss: 0.6386364102363586\n",
      "Step 18/1000, Loss: 0.3118163049221039, Validation Loss: 0.6388339996337891\n",
      "Step 19/1000, Loss: 0.3111661672592163, Validation Loss: 0.6390484571456909\n",
      "Step 20/1000, Loss: 0.31052494049072266, Validation Loss: 0.6392709016799927\n",
      "Step 21/1000, Loss: 0.30989235639572144, Validation Loss: 0.6394951343536377\n",
      "Step 22/1000, Loss: 0.30926817655563354, Validation Loss: 0.639716386795044\n",
      "Step 23/1000, Loss: 0.3086521327495575, Validation Loss: 0.6399313807487488\n",
      "Step 24/1000, Loss: 0.30804404616355896, Validation Loss: 0.6401379704475403\n",
      "Step 25/1000, Loss: 0.3074435889720917, Validation Loss: 0.640334963798523\n",
      "Step 26/1000, Loss: 0.3068506717681885, Validation Loss: 0.6405213475227356\n",
      "Step 27/1000, Loss: 0.3062649965286255, Validation Loss: 0.6406980156898499\n",
      "Step 28/1000, Loss: 0.30568641424179077, Validation Loss: 0.6408647894859314\n",
      "Step 29/1000, Loss: 0.30511465668678284, Validation Loss: 0.6410224437713623\n",
      "Step 30/1000, Loss: 0.3045494854450226, Validation Loss: 0.6411719918251038\n",
      "Step 31/1000, Loss: 0.30399084091186523, Validation Loss: 0.6413150429725647\n",
      "Step 32/1000, Loss: 0.30343836545944214, Validation Loss: 0.641452431678772\n",
      "Step 33/1000, Loss: 0.3028920292854309, Validation Loss: 0.6415861248970032\n",
      "Step 34/1000, Loss: 0.30235153436660767, Validation Loss: 0.6417173743247986\n",
      "Step 35/1000, Loss: 0.30181682109832764, Validation Loss: 0.6418477296829224\n",
      "Step 36/1000, Loss: 0.3012876808643341, Validation Loss: 0.641978919506073\n",
      "Step 37/1000, Loss: 0.30076390504837036, Validation Loss: 0.642112135887146\n",
      "Step 38/1000, Loss: 0.30024540424346924, Validation Loss: 0.6422489881515503\n",
      "Step 39/1000, Loss: 0.2997320890426636, Validation Loss: 0.6423913836479187\n",
      "Step 40/1000, Loss: 0.2992236614227295, Validation Loss: 0.6425403356552124\n",
      "Step 41/1000, Loss: 0.29872018098831177, Validation Loss: 0.6426976919174194\n",
      "Step 42/1000, Loss: 0.2982213497161865, Validation Loss: 0.6428644061088562\n",
      "Step 43/1000, Loss: 0.29772719740867615, Validation Loss: 0.6430416703224182\n",
      "Step 44/1000, Loss: 0.29723745584487915, Validation Loss: 0.6432307362556458\n",
      "Step 45/1000, Loss: 0.29675212502479553, Validation Loss: 0.6434324979782104\n",
      "Step 46/1000, Loss: 0.29627111554145813, Validation Loss: 0.6436477303504944\n",
      "Step 47/1000, Loss: 0.29579421877861023, Validation Loss: 0.643877387046814\n",
      "Step 48/1000, Loss: 0.29532134532928467, Validation Loss: 0.6441217660903931\n",
      "Step 49/1000, Loss: 0.29485249519348145, Validation Loss: 0.6443816423416138\n",
      "Step 50/1000, Loss: 0.2943875193595886, Validation Loss: 0.6446570158004761\n",
      "Step 51/1000, Loss: 0.2939262092113495, Validation Loss: 0.6449480056762695\n",
      "Step 52/1000, Loss: 0.2934686541557312, Validation Loss: 0.645254909992218\n",
      "Step 53/1000, Loss: 0.29301461577415466, Validation Loss: 0.6455773115158081\n",
      "Step 54/1000, Loss: 0.2925640940666199, Validation Loss: 0.6459150910377502\n",
      "Step 55/1000, Loss: 0.2921169698238373, Validation Loss: 0.6462674736976624\n",
      "Step 56/1000, Loss: 0.2916731536388397, Validation Loss: 0.6466342806816101\n",
      "Step 57/1000, Loss: 0.29123255610466003, Validation Loss: 0.6470143795013428\n",
      "Step 58/1000, Loss: 0.29079514741897583, Validation Loss: 0.6474073529243469\n",
      "Step 59/1000, Loss: 0.29036077857017517, Validation Loss: 0.6478120684623718\n",
      "Step 60/1000, Loss: 0.2899293899536133, Validation Loss: 0.6482271552085876\n",
      "Step 61/1000, Loss: 0.2895009517669678, Validation Loss: 0.6486519575119019\n",
      "Step 62/1000, Loss: 0.2890753149986267, Validation Loss: 0.6490849256515503\n",
      "Step 63/1000, Loss: 0.2886524796485901, Validation Loss: 0.6495249271392822\n",
      "Step 64/1000, Loss: 0.28823238611221313, Validation Loss: 0.6499704122543335\n",
      "Step 65/1000, Loss: 0.2878148555755615, Validation Loss: 0.650420069694519\n",
      "Step 66/1000, Loss: 0.28739994764328003, Validation Loss: 0.6508723497390747\n",
      "Step 67/1000, Loss: 0.2869875431060791, Validation Loss: 0.6513255834579468\n",
      "Step 68/1000, Loss: 0.2865775525569916, Validation Loss: 0.6517786383628845\n",
      "Step 69/1000, Loss: 0.28616994619369507, Validation Loss: 0.6522299647331238\n",
      "Step 70/1000, Loss: 0.2857646942138672, Validation Loss: 0.6526780724525452\n",
      "Step 71/1000, Loss: 0.2853616774082184, Validation Loss: 0.6531217098236084\n",
      "Step 72/1000, Loss: 0.28496089577674866, Validation Loss: 0.6535595059394836\n",
      "Step 73/1000, Loss: 0.28456228971481323, Validation Loss: 0.6539903283119202\n",
      "Step 74/1000, Loss: 0.28416574001312256, Validation Loss: 0.6544127464294434\n",
      "Step 75/1000, Loss: 0.283771276473999, Validation Loss: 0.6548258066177368\n",
      "Step 76/1000, Loss: 0.28337880969047546, Validation Loss: 0.6552287936210632\n",
      "Step 77/1000, Loss: 0.2829882502555847, Validation Loss: 0.6556207537651062\n",
      "Step 78/1000, Loss: 0.28259962797164917, Validation Loss: 0.6560009717941284\n",
      "Step 79/1000, Loss: 0.28221288323402405, Validation Loss: 0.6563687920570374\n",
      "Step 80/1000, Loss: 0.28182798624038696, Validation Loss: 0.6567240953445435\n",
      "Step 81/1000, Loss: 0.281444787979126, Validation Loss: 0.6570662260055542\n",
      "Step 82/1000, Loss: 0.28106340765953064, Validation Loss: 0.6573954820632935\n",
      "Step 83/1000, Loss: 0.28068363666534424, Validation Loss: 0.6577116847038269\n",
      "Step 84/1000, Loss: 0.28030553460121155, Validation Loss: 0.6580148935317993\n",
      "Step 85/1000, Loss: 0.2799290716648102, Validation Loss: 0.6583055257797241\n",
      "Step 86/1000, Loss: 0.27955418825149536, Validation Loss: 0.6585835814476013\n",
      "Step 87/1000, Loss: 0.27918076515197754, Validation Loss: 0.6588497757911682\n",
      "Step 88/1000, Loss: 0.27880895137786865, Validation Loss: 0.6591044664382935\n",
      "Step 89/1000, Loss: 0.2784384787082672, Validation Loss: 0.6593487858772278\n",
      "Step 90/1000, Loss: 0.27806952595710754, Validation Loss: 0.6595830917358398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 91/1000, Loss: 0.2777019739151001, Validation Loss: 0.6598083972930908\n",
      "Step 92/1000, Loss: 0.2773357331752777, Validation Loss: 0.6600252985954285\n",
      "Step 93/1000, Loss: 0.27697086334228516, Validation Loss: 0.6602344512939453\n",
      "Step 94/1000, Loss: 0.27660733461380005, Validation Loss: 0.6604369282722473\n",
      "Step 95/1000, Loss: 0.27624502778053284, Validation Loss: 0.6606336236000061\n",
      "Step 96/1000, Loss: 0.2758840024471283, Validation Loss: 0.6608253121376038\n",
      "Step 97/1000, Loss: 0.27552419900894165, Validation Loss: 0.6610128283500671\n",
      "Step 98/1000, Loss: 0.2751655578613281, Validation Loss: 0.6611967086791992\n",
      "Step 99/1000, Loss: 0.2748080790042877, Validation Loss: 0.661378026008606\n",
      "Step 100/1000, Loss: 0.2744518220424652, Validation Loss: 0.6615572571754456\n",
      "Step 101/1000, Loss: 0.27409660816192627, Validation Loss: 0.6617351174354553\n",
      "Step 102/1000, Loss: 0.27374252676963806, Validation Loss: 0.6618231534957886\n",
      "Step 103/1000, Loss: 0.27356594800949097, Validation Loss: 0.6619102358818054\n",
      "Step 104/1000, Loss: 0.27338963747024536, Validation Loss: 0.6619965434074402\n",
      "Step 105/1000, Loss: 0.2732137441635132, Validation Loss: 0.6620824933052063\n",
      "Step 106/1000, Loss: 0.2730380892753601, Validation Loss: 0.6621678471565247\n",
      "Step 107/1000, Loss: 0.2728627324104309, Validation Loss: 0.6622534394264221\n",
      "Step 108/1000, Loss: 0.2726876735687256, Validation Loss: 0.6623389720916748\n",
      "Step 109/1000, Loss: 0.27251285314559937, Validation Loss: 0.6624243259429932\n",
      "Step 110/1000, Loss: 0.27233824133872986, Validation Loss: 0.6625102758407593\n",
      "Step 111/1000, Loss: 0.27216389775276184, Validation Loss: 0.662596583366394\n",
      "Step 112/1000, Loss: 0.27198976278305054, Validation Loss: 0.6626830697059631\n",
      "Step 113/1000, Loss: 0.2718159258365631, Validation Loss: 0.6627703309059143\n",
      "Step 114/1000, Loss: 0.27164217829704285, Validation Loss: 0.6628580093383789\n",
      "Early stopping triggered\n",
      "Step 1/1000, Loss: 1.034821629524231, Validation Loss: 1.190604567527771\n",
      "Step 2/1000, Loss: 1.033720850944519, Validation Loss: 1.1896613836288452\n",
      "Step 3/1000, Loss: 1.0326217412948608, Validation Loss: 1.1887199878692627\n",
      "Step 4/1000, Loss: 1.0315238237380981, Validation Loss: 1.1877801418304443\n",
      "Step 5/1000, Loss: 1.0304269790649414, Validation Loss: 1.1868417263031006\n",
      "Step 6/1000, Loss: 1.0293309688568115, Validation Loss: 1.1859043836593628\n",
      "Step 7/1000, Loss: 1.028235673904419, Validation Loss: 1.184968113899231\n",
      "Step 8/1000, Loss: 1.0271406173706055, Validation Loss: 1.1840324401855469\n",
      "Step 9/1000, Loss: 1.0260459184646606, Validation Loss: 1.183097243309021\n",
      "Step 10/1000, Loss: 1.0249511003494263, Validation Loss: 1.1821622848510742\n",
      "Step 11/1000, Loss: 1.0238559246063232, Validation Loss: 1.181227445602417\n",
      "Step 12/1000, Loss: 1.022760272026062, Validation Loss: 1.1802926063537598\n",
      "Step 13/1000, Loss: 1.021664023399353, Validation Loss: 1.179357647895813\n",
      "Step 14/1000, Loss: 1.0205665826797485, Validation Loss: 1.178422212600708\n",
      "Step 15/1000, Loss: 1.0194677114486694, Validation Loss: 1.1774866580963135\n",
      "Step 16/1000, Loss: 1.0183675289154053, Validation Loss: 1.1765505075454712\n",
      "Step 17/1000, Loss: 1.0172655582427979, Validation Loss: 1.1756138801574707\n",
      "Step 18/1000, Loss: 1.0161614418029785, Validation Loss: 1.174676775932312\n",
      "Step 19/1000, Loss: 1.0150551795959473, Validation Loss: 1.1737383604049683\n",
      "Step 20/1000, Loss: 1.0139460563659668, Validation Loss: 1.1727992296218872\n",
      "Step 21/1000, Loss: 1.0128343105316162, Validation Loss: 1.1718590259552002\n",
      "Step 22/1000, Loss: 1.0117194652557373, Validation Loss: 1.1709175109863281\n",
      "Step 23/1000, Loss: 1.0106010437011719, Validation Loss: 1.169974446296692\n",
      "Step 24/1000, Loss: 1.0094789266586304, Validation Loss: 1.169029712677002\n",
      "Step 25/1000, Loss: 1.0083528757095337, Validation Loss: 1.1680833101272583\n",
      "Step 26/1000, Loss: 1.0072225332260132, Validation Loss: 1.1671351194381714\n",
      "Step 27/1000, Loss: 1.0060876607894897, Validation Loss: 1.166184902191162\n",
      "Step 28/1000, Loss: 1.0049481391906738, Validation Loss: 1.165232539176941\n",
      "Step 29/1000, Loss: 1.0038037300109863, Validation Loss: 1.1642777919769287\n",
      "Step 30/1000, Loss: 1.002653956413269, Validation Loss: 1.1633208990097046\n",
      "Step 31/1000, Loss: 1.0014989376068115, Validation Loss: 1.1623613834381104\n",
      "Step 32/1000, Loss: 1.0003383159637451, Validation Loss: 1.161399245262146\n",
      "Step 33/1000, Loss: 0.999172031879425, Validation Loss: 1.160434365272522\n",
      "Step 34/1000, Loss: 0.9979996681213379, Validation Loss: 1.1594663858413696\n",
      "Step 35/1000, Loss: 0.9968212246894836, Validation Loss: 1.1584954261779785\n",
      "Step 36/1000, Loss: 0.9956362843513489, Validation Loss: 1.1575212478637695\n",
      "Step 37/1000, Loss: 0.9944448471069336, Validation Loss: 1.156543493270874\n",
      "Step 38/1000, Loss: 0.9932467937469482, Validation Loss: 1.155562400817871\n",
      "Step 39/1000, Loss: 0.9920419454574585, Validation Loss: 1.154577374458313\n",
      "Step 40/1000, Loss: 0.9908298850059509, Validation Loss: 1.1535882949829102\n",
      "Step 41/1000, Loss: 0.9896107912063599, Validation Loss: 1.152595043182373\n",
      "Step 42/1000, Loss: 0.9883843064308167, Validation Loss: 1.151597499847412\n",
      "Step 43/1000, Loss: 0.9871501326560974, Validation Loss: 1.1505955457687378\n",
      "Step 44/1000, Loss: 0.9859083890914917, Validation Loss: 1.1495888233184814\n",
      "Step 45/1000, Loss: 0.9846588969230652, Validation Loss: 1.1485769748687744\n",
      "Step 46/1000, Loss: 0.9834015369415283, Validation Loss: 1.1475600004196167\n",
      "Step 47/1000, Loss: 0.9821360111236572, Validation Loss: 1.1465377807617188\n",
      "Step 48/1000, Loss: 0.9808622598648071, Validation Loss: 1.145509958267212\n",
      "Step 49/1000, Loss: 0.9795799851417542, Validation Loss: 1.1444765329360962\n",
      "Step 50/1000, Loss: 0.9782893061637878, Validation Loss: 1.1434370279312134\n",
      "Step 51/1000, Loss: 0.9769899845123291, Validation Loss: 1.1423914432525635\n",
      "Step 52/1000, Loss: 0.9756819605827332, Validation Loss: 1.141339659690857\n",
      "Step 53/1000, Loss: 0.974364697933197, Validation Loss: 1.1402814388275146\n",
      "Step 54/1000, Loss: 0.9730386137962341, Validation Loss: 1.139216423034668\n",
      "Step 55/1000, Loss: 0.9717031717300415, Validation Loss: 1.138144612312317\n",
      "Step 56/1000, Loss: 0.9703587293624878, Validation Loss: 1.1370657682418823\n",
      "Step 57/1000, Loss: 0.9690045118331909, Validation Loss: 1.1359796524047852\n",
      "Step 58/1000, Loss: 0.9676409959793091, Validation Loss: 1.1348860263824463\n",
      "Step 59/1000, Loss: 0.9662678837776184, Validation Loss: 1.133784532546997\n",
      "Step 60/1000, Loss: 0.9648849964141846, Validation Loss: 1.1326755285263062\n",
      "Step 61/1000, Loss: 0.963492214679718, Validation Loss: 1.1315581798553467\n",
      "Step 62/1000, Loss: 0.9620896577835083, Validation Loss: 1.1304326057434082\n",
      "Step 63/1000, Loss: 0.9606770873069763, Validation Loss: 1.129298448562622\n",
      "Step 64/1000, Loss: 0.9592544436454773, Validation Loss: 1.1281557083129883\n",
      "Step 65/1000, Loss: 0.957821786403656, Validation Loss: 1.1270040273666382\n",
      "Step 66/1000, Loss: 0.9563788771629333, Validation Loss: 1.1258431673049927\n",
      "Step 67/1000, Loss: 0.9549257159233093, Validation Loss: 1.124672770500183\n",
      "Step 68/1000, Loss: 0.9534621834754944, Validation Loss: 1.1234930753707886\n",
      "Step 69/1000, Loss: 0.9519883394241333, Validation Loss: 1.1223033666610718\n",
      "Step 70/1000, Loss: 0.9505041837692261, Validation Loss: 1.1211036443710327\n",
      "Step 71/1000, Loss: 0.9490095973014832, Validation Loss: 1.1198937892913818\n",
      "Step 72/1000, Loss: 0.9475043416023254, Validation Loss: 1.1186732053756714\n",
      "Step 73/1000, Loss: 0.9459885358810425, Validation Loss: 1.1174421310424805\n",
      "Step 74/1000, Loss: 0.9444622993469238, Validation Loss: 1.1162002086639404\n",
      "Step 75/1000, Loss: 0.9429252743721008, Validation Loss: 1.114946961402893\n",
      "Step 76/1000, Loss: 0.9413776397705078, Validation Loss: 1.113682508468628\n",
      "Step 77/1000, Loss: 0.9398192763328552, Validation Loss: 1.1124064922332764\n",
      "Step 78/1000, Loss: 0.9382502436637878, Validation Loss: 1.1111186742782593\n",
      "Step 79/1000, Loss: 0.9366704821586609, Validation Loss: 1.109818935394287\n",
      "Step 80/1000, Loss: 0.9350798726081848, Validation Loss: 1.1085071563720703\n",
      "Step 81/1000, Loss: 0.9334783554077148, Validation Loss: 1.1071829795837402\n",
      "Step 82/1000, Loss: 0.9318659901618958, Validation Loss: 1.1058461666107178\n",
      "Step 83/1000, Loss: 0.9302428364753723, Validation Loss: 1.1044968366622925\n",
      "Step 84/1000, Loss: 0.9286087155342102, Validation Loss: 1.1031345129013062\n",
      "Step 85/1000, Loss: 0.9269636273384094, Validation Loss: 1.1017593145370483\n",
      "Step 86/1000, Loss: 0.9253076910972595, Validation Loss: 1.1003708839416504\n",
      "Step 87/1000, Loss: 0.9236408472061157, Validation Loss: 1.0989692211151123\n",
      "Step 88/1000, Loss: 0.921963095664978, Validation Loss: 1.0975542068481445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 89/1000, Loss: 0.9202742576599121, Validation Loss: 1.0961253643035889\n",
      "Step 90/1000, Loss: 0.9185745120048523, Validation Loss: 1.094683051109314\n",
      "Step 91/1000, Loss: 0.9168638586997986, Validation Loss: 1.0932271480560303\n",
      "Step 92/1000, Loss: 0.9151421189308167, Validation Loss: 1.0917571783065796\n",
      "Step 93/1000, Loss: 0.913409411907196, Validation Loss: 1.0902734994888306\n",
      "Step 94/1000, Loss: 0.9116658568382263, Validation Loss: 1.088775634765625\n",
      "Step 95/1000, Loss: 0.9099112749099731, Validation Loss: 1.087263822555542\n",
      "Step 96/1000, Loss: 0.9081454873085022, Validation Loss: 1.085737943649292\n",
      "Step 97/1000, Loss: 0.9063689708709717, Validation Loss: 1.0841976404190063\n",
      "Step 98/1000, Loss: 0.9045811891555786, Validation Loss: 1.0826433897018433\n",
      "Step 99/1000, Loss: 0.9027824997901917, Validation Loss: 1.0810747146606445\n",
      "Step 100/1000, Loss: 0.9009726643562317, Validation Loss: 1.0794919729232788\n",
      "Step 101/1000, Loss: 0.8991515636444092, Validation Loss: 1.0778948068618774\n",
      "Step 102/1000, Loss: 0.8973191976547241, Validation Loss: 1.077091097831726\n",
      "Step 103/1000, Loss: 0.8963989019393921, Validation Loss: 1.0762850046157837\n",
      "Step 104/1000, Loss: 0.8954761624336243, Validation Loss: 1.075476050376892\n",
      "Step 105/1000, Loss: 0.8945510983467102, Validation Loss: 1.0746644735336304\n",
      "Step 106/1000, Loss: 0.8936238884925842, Validation Loss: 1.073850393295288\n",
      "Step 107/1000, Loss: 0.8926945328712463, Validation Loss: 1.0730338096618652\n",
      "Step 108/1000, Loss: 0.8917630314826965, Validation Loss: 1.0722148418426514\n",
      "Step 109/1000, Loss: 0.8908297419548035, Validation Loss: 1.071393609046936\n",
      "Step 110/1000, Loss: 0.8898943662643433, Validation Loss: 1.0705697536468506\n",
      "Step 111/1000, Loss: 0.8889573812484741, Validation Loss: 1.0697438716888428\n",
      "Step 112/1000, Loss: 0.8880184292793274, Validation Loss: 1.068915605545044\n",
      "Step 113/1000, Loss: 0.8870775699615479, Validation Loss: 1.0680848360061646\n",
      "Step 114/1000, Loss: 0.8861349821090698, Validation Loss: 1.0672520399093628\n",
      "Step 115/1000, Loss: 0.8851907253265381, Validation Loss: 1.0664167404174805\n",
      "Step 116/1000, Loss: 0.8842445611953735, Validation Loss: 1.0655795335769653\n",
      "Step 117/1000, Loss: 0.8832966089248657, Validation Loss: 1.0647398233413696\n",
      "Step 118/1000, Loss: 0.8823468685150146, Validation Loss: 1.063897967338562\n",
      "Step 119/1000, Loss: 0.8813953399658203, Validation Loss: 1.0630537271499634\n",
      "Step 120/1000, Loss: 0.8804421424865723, Validation Loss: 1.0622073411941528\n",
      "Step 121/1000, Loss: 0.8794869780540466, Validation Loss: 1.06135892868042\n",
      "Step 122/1000, Loss: 0.878529965877533, Validation Loss: 1.0605080127716064\n",
      "Step 123/1000, Loss: 0.877571165561676, Validation Loss: 1.0596551895141602\n",
      "Step 124/1000, Loss: 0.876610517501831, Validation Loss: 1.0587999820709229\n",
      "Step 125/1000, Loss: 0.8756477236747742, Validation Loss: 1.0579426288604736\n",
      "Step 126/1000, Loss: 0.8746830224990845, Validation Loss: 1.0570831298828125\n",
      "Step 127/1000, Loss: 0.8737164735794067, Validation Loss: 1.0562212467193604\n",
      "Step 128/1000, Loss: 0.8727478981018066, Validation Loss: 1.0553572177886963\n",
      "Step 129/1000, Loss: 0.8717772364616394, Validation Loss: 1.0544910430908203\n",
      "Step 130/1000, Loss: 0.870804488658905, Validation Loss: 1.0536227226257324\n",
      "Step 131/1000, Loss: 0.869829535484314, Validation Loss: 1.052751898765564\n",
      "Step 132/1000, Loss: 0.8688526153564453, Validation Loss: 1.0518791675567627\n",
      "Step 133/1000, Loss: 0.86787348985672, Validation Loss: 1.05100417137146\n",
      "Step 134/1000, Loss: 0.8668920993804932, Validation Loss: 1.0501267910003662\n",
      "Step 135/1000, Loss: 0.8659085631370544, Validation Loss: 1.049247145652771\n",
      "Step 136/1000, Loss: 0.8649225234985352, Validation Loss: 1.0483653545379639\n",
      "Step 137/1000, Loss: 0.863934338092804, Validation Loss: 1.0474812984466553\n",
      "Step 138/1000, Loss: 0.862943708896637, Validation Loss: 1.0465947389602661\n",
      "Step 139/1000, Loss: 0.8619508147239685, Validation Loss: 1.045706033706665\n",
      "Step 140/1000, Loss: 0.8609554171562195, Validation Loss: 1.0448150634765625\n",
      "Step 141/1000, Loss: 0.8599575161933899, Validation Loss: 1.0439215898513794\n",
      "Step 142/1000, Loss: 0.8589571118354797, Validation Loss: 1.0430259704589844\n",
      "Step 143/1000, Loss: 0.857954204082489, Validation Loss: 1.0421278476715088\n",
      "Step 144/1000, Loss: 0.8569486141204834, Validation Loss: 1.0412273406982422\n",
      "Step 145/1000, Loss: 0.8559404611587524, Validation Loss: 1.0403244495391846\n",
      "Step 146/1000, Loss: 0.8549295663833618, Validation Loss: 1.0394190549850464\n",
      "Step 147/1000, Loss: 0.8539161682128906, Validation Loss: 1.038511037826538\n",
      "Step 148/1000, Loss: 0.8529000282287598, Validation Loss: 1.0376007556915283\n",
      "Step 149/1000, Loss: 0.8518810868263245, Validation Loss: 1.036687970161438\n",
      "Step 150/1000, Loss: 0.8508593440055847, Validation Loss: 1.0357725620269775\n",
      "Step 151/1000, Loss: 0.8498347401618958, Validation Loss: 1.0348544120788574\n",
      "Step 152/1000, Loss: 0.8488073348999023, Validation Loss: 1.0339338779449463\n",
      "Step 153/1000, Loss: 0.8477770686149597, Validation Loss: 1.0330106019973755\n",
      "Step 154/1000, Loss: 0.8467438220977783, Validation Loss: 1.0320847034454346\n",
      "Step 155/1000, Loss: 0.8457075953483582, Validation Loss: 1.031156063079834\n",
      "Step 156/1000, Loss: 0.8446683883666992, Validation Loss: 1.0302248001098633\n",
      "Step 157/1000, Loss: 0.8436263799667358, Validation Loss: 1.0292904376983643\n",
      "Step 158/1000, Loss: 0.8425810933113098, Validation Loss: 1.0283536911010742\n",
      "Step 159/1000, Loss: 0.8415328860282898, Validation Loss: 1.0274138450622559\n",
      "Step 160/1000, Loss: 0.8404815793037415, Validation Loss: 1.0264712572097778\n",
      "Step 161/1000, Loss: 0.8394272327423096, Validation Loss: 1.025525689125061\n",
      "Step 162/1000, Loss: 0.8383696675300598, Validation Loss: 1.024577260017395\n",
      "Step 163/1000, Loss: 0.8373088836669922, Validation Loss: 1.0236258506774902\n",
      "Step 164/1000, Loss: 0.8362449407577515, Validation Loss: 1.0226715803146362\n",
      "Step 165/1000, Loss: 0.8351777791976929, Validation Loss: 1.0217140913009644\n",
      "Step 166/1000, Loss: 0.8341074585914612, Validation Loss: 1.0207536220550537\n",
      "Step 167/1000, Loss: 0.8330338597297668, Validation Loss: 1.0197901725769043\n",
      "Step 168/1000, Loss: 0.8319569826126099, Validation Loss: 1.0188236236572266\n",
      "Step 169/1000, Loss: 0.8308767676353455, Validation Loss: 1.0178539752960205\n",
      "Step 170/1000, Loss: 0.8297933340072632, Validation Loss: 1.016880989074707\n",
      "Step 171/1000, Loss: 0.828706681728363, Validation Loss: 1.0159049034118652\n",
      "Step 172/1000, Loss: 0.8276164531707764, Validation Loss: 1.0149257183074951\n",
      "Step 173/1000, Loss: 0.826522946357727, Validation Loss: 1.0139433145523071\n",
      "Step 174/1000, Loss: 0.8254261016845703, Validation Loss: 1.0129574537277222\n",
      "Step 175/1000, Loss: 0.8243258595466614, Validation Loss: 1.0119684934616089\n",
      "Step 176/1000, Loss: 0.8232221603393555, Validation Loss: 1.0109763145446777\n",
      "Step 177/1000, Loss: 0.8221150636672974, Validation Loss: 1.0099807977676392\n",
      "Step 178/1000, Loss: 0.8210045695304871, Validation Loss: 1.0089818239212036\n",
      "Step 179/1000, Loss: 0.8198906779289246, Validation Loss: 1.0079796314239502\n",
      "Step 180/1000, Loss: 0.8187733888626099, Validation Loss: 1.0069741010665894\n",
      "Step 181/1000, Loss: 0.8176525235176086, Validation Loss: 1.0059651136398315\n",
      "Step 182/1000, Loss: 0.8165280818939209, Validation Loss: 1.0049526691436768\n",
      "Step 183/1000, Loss: 0.8154004216194153, Validation Loss: 1.0039371252059937\n",
      "Step 184/1000, Loss: 0.8142690062522888, Validation Loss: 1.0029181241989136\n",
      "Step 185/1000, Loss: 0.8131343126296997, Validation Loss: 1.001895546913147\n",
      "Step 186/1000, Loss: 0.8119960427284241, Validation Loss: 1.0008693933486938\n",
      "Step 187/1000, Loss: 0.8108543157577515, Validation Loss: 0.9998401999473572\n",
      "Step 188/1000, Loss: 0.809708833694458, Validation Loss: 0.9988073110580444\n",
      "Step 189/1000, Loss: 0.8085600137710571, Validation Loss: 0.997771143913269\n",
      "Step 190/1000, Loss: 0.8074077367782593, Validation Loss: 0.9967315793037415\n",
      "Step 191/1000, Loss: 0.8062517642974854, Validation Loss: 0.9956884980201721\n",
      "Step 192/1000, Loss: 0.8050923943519592, Validation Loss: 0.9946420192718506\n",
      "Step 193/1000, Loss: 0.8039295077323914, Validation Loss: 0.9935920238494873\n",
      "Step 194/1000, Loss: 0.8027629852294922, Validation Loss: 0.9925385117530823\n",
      "Step 195/1000, Loss: 0.801593005657196, Validation Loss: 0.9914817214012146\n",
      "Step 196/1000, Loss: 0.8004194498062134, Validation Loss: 0.9904215335845947\n",
      "Step 197/1000, Loss: 0.7992423176765442, Validation Loss: 0.9893578290939331\n",
      "Step 198/1000, Loss: 0.798061728477478, Validation Loss: 0.9882907271385193\n",
      "Step 199/1000, Loss: 0.7968777418136597, Validation Loss: 0.9872201681137085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200/1000, Loss: 0.7956900596618652, Validation Loss: 0.9861462712287903\n",
      "Step 201/1000, Loss: 0.7944989204406738, Validation Loss: 0.9850688576698303\n",
      "Step 202/1000, Loss: 0.7933042645454407, Validation Loss: 0.9845291376113892\n",
      "Step 203/1000, Loss: 0.7927058339118958, Validation Loss: 0.9839891195297241\n",
      "Step 204/1000, Loss: 0.7921068668365479, Validation Loss: 0.9834486246109009\n",
      "Step 205/1000, Loss: 0.7915077209472656, Validation Loss: 0.982907772064209\n",
      "Step 206/1000, Loss: 0.7909082174301147, Validation Loss: 0.9823666214942932\n",
      "Step 207/1000, Loss: 0.79030841588974, Validation Loss: 0.9818254113197327\n",
      "Step 208/1000, Loss: 0.7897083759307861, Validation Loss: 0.9812837243080139\n",
      "Step 209/1000, Loss: 0.7891080975532532, Validation Loss: 0.9807419180870056\n",
      "Step 210/1000, Loss: 0.7885076999664307, Validation Loss: 0.9801999926567078\n",
      "Step 211/1000, Loss: 0.7879071831703186, Validation Loss: 0.979657769203186\n",
      "Step 212/1000, Loss: 0.7873064279556274, Validation Loss: 0.9791153073310852\n",
      "Step 213/1000, Loss: 0.7867056131362915, Validation Loss: 0.9785727858543396\n",
      "Step 214/1000, Loss: 0.7861045598983765, Validation Loss: 0.9780299663543701\n",
      "Step 215/1000, Loss: 0.7855035662651062, Validation Loss: 0.9774870872497559\n",
      "Step 216/1000, Loss: 0.7849022746086121, Validation Loss: 0.9769439697265625\n",
      "Step 217/1000, Loss: 0.7843010425567627, Validation Loss: 0.9764007925987244\n",
      "Step 218/1000, Loss: 0.783699631690979, Validation Loss: 0.9758573174476624\n",
      "Step 219/1000, Loss: 0.7830982804298401, Validation Loss: 0.9753138422966003\n",
      "Step 220/1000, Loss: 0.7824966907501221, Validation Loss: 0.974770188331604\n",
      "Step 221/1000, Loss: 0.7818951606750488, Validation Loss: 0.9742262959480286\n",
      "Step 222/1000, Loss: 0.781293511390686, Validation Loss: 0.9736824035644531\n",
      "Step 223/1000, Loss: 0.7806916832923889, Validation Loss: 0.9731380939483643\n",
      "Step 224/1000, Loss: 0.780089795589447, Validation Loss: 0.9725938439369202\n",
      "Step 225/1000, Loss: 0.7794879674911499, Validation Loss: 0.9720492959022522\n",
      "Step 226/1000, Loss: 0.7788859009742737, Validation Loss: 0.971504807472229\n",
      "Step 227/1000, Loss: 0.7782838344573975, Validation Loss: 0.9709601402282715\n",
      "Step 228/1000, Loss: 0.7776817083358765, Validation Loss: 0.9704151153564453\n",
      "Step 229/1000, Loss: 0.7770794630050659, Validation Loss: 0.9698700308799744\n",
      "Step 230/1000, Loss: 0.7764770984649658, Validation Loss: 0.9693247079849243\n",
      "Step 231/1000, Loss: 0.7758746147155762, Validation Loss: 0.9687793850898743\n",
      "Step 232/1000, Loss: 0.7752721309661865, Validation Loss: 0.9682337045669556\n",
      "Step 233/1000, Loss: 0.7746694684028625, Validation Loss: 0.9676880240440369\n",
      "Step 234/1000, Loss: 0.7740667462348938, Validation Loss: 0.9671420454978943\n",
      "Step 235/1000, Loss: 0.7734638452529907, Validation Loss: 0.9665958881378174\n",
      "Step 236/1000, Loss: 0.7728609442710876, Validation Loss: 0.9660494923591614\n",
      "Step 237/1000, Loss: 0.7722578048706055, Validation Loss: 0.9655030369758606\n",
      "Step 238/1000, Loss: 0.7716546654701233, Validation Loss: 0.9649562239646912\n",
      "Step 239/1000, Loss: 0.771051287651062, Validation Loss: 0.9644095301628113\n",
      "Step 240/1000, Loss: 0.7704477906227112, Validation Loss: 0.9638622999191284\n",
      "Step 241/1000, Loss: 0.7698442339897156, Validation Loss: 0.9633150696754456\n",
      "Step 242/1000, Loss: 0.7692405581474304, Validation Loss: 0.9627675414085388\n",
      "Step 243/1000, Loss: 0.7686366438865662, Validation Loss: 0.9622198343276978\n",
      "Step 244/1000, Loss: 0.7680326700210571, Validation Loss: 0.9616719484329224\n",
      "Step 245/1000, Loss: 0.7674285173416138, Validation Loss: 0.9611237645149231\n",
      "Step 246/1000, Loss: 0.7668242454528809, Validation Loss: 0.9605754613876343\n",
      "Step 247/1000, Loss: 0.7662196755409241, Validation Loss: 0.9600269198417664\n",
      "Step 248/1000, Loss: 0.7656151652336121, Validation Loss: 0.9594780802726746\n",
      "Step 249/1000, Loss: 0.7650102972984314, Validation Loss: 0.9589290618896484\n",
      "Step 250/1000, Loss: 0.764405369758606, Validation Loss: 0.9583799242973328\n",
      "Step 251/1000, Loss: 0.763800323009491, Validation Loss: 0.9578304290771484\n",
      "Step 252/1000, Loss: 0.7631950974464417, Validation Loss: 0.9572807550430298\n",
      "Step 253/1000, Loss: 0.7625896334648132, Validation Loss: 0.9567307829856873\n",
      "Step 254/1000, Loss: 0.7619840502738953, Validation Loss: 0.9561806917190552\n",
      "Step 255/1000, Loss: 0.7613781094551086, Validation Loss: 0.9556302428245544\n",
      "Step 256/1000, Loss: 0.7607722282409668, Validation Loss: 0.9550794959068298\n",
      "Step 257/1000, Loss: 0.7601659893989563, Validation Loss: 0.9545286297798157\n",
      "Step 258/1000, Loss: 0.7595596313476562, Validation Loss: 0.9539774060249329\n",
      "Step 259/1000, Loss: 0.7589530944824219, Validation Loss: 0.9534260034561157\n",
      "Step 260/1000, Loss: 0.7583463788032532, Validation Loss: 0.9528743624687195\n",
      "Step 261/1000, Loss: 0.7577394247055054, Validation Loss: 0.9523225426673889\n",
      "Step 262/1000, Loss: 0.7571322917938232, Validation Loss: 0.9517702460289001\n",
      "Step 263/1000, Loss: 0.7565250396728516, Validation Loss: 0.9512178897857666\n",
      "Step 264/1000, Loss: 0.755917489528656, Validation Loss: 0.9506651759147644\n",
      "Step 265/1000, Loss: 0.7553096413612366, Validation Loss: 0.9501121044158936\n",
      "Step 266/1000, Loss: 0.7547017931938171, Validation Loss: 0.9495588541030884\n",
      "Step 267/1000, Loss: 0.7540937066078186, Validation Loss: 0.9490053057670593\n",
      "Step 268/1000, Loss: 0.7534852623939514, Validation Loss: 0.9484514594078064\n",
      "Step 269/1000, Loss: 0.7528766989707947, Validation Loss: 0.9478974938392639\n",
      "Step 270/1000, Loss: 0.7522680163383484, Validation Loss: 0.9473432302474976\n",
      "Step 271/1000, Loss: 0.7516588568687439, Validation Loss: 0.946788489818573\n",
      "Step 272/1000, Loss: 0.7510496973991394, Validation Loss: 0.9462334513664246\n",
      "Step 273/1000, Loss: 0.7504402995109558, Validation Loss: 0.9456784129142761\n",
      "Step 274/1000, Loss: 0.7498306035995483, Validation Loss: 0.9451228976249695\n",
      "Step 275/1000, Loss: 0.7492206692695618, Validation Loss: 0.944567084312439\n",
      "Step 276/1000, Loss: 0.7486105561256409, Validation Loss: 0.9440109133720398\n",
      "Step 277/1000, Loss: 0.7480002045631409, Validation Loss: 0.9434545636177063\n",
      "Step 278/1000, Loss: 0.7473896145820618, Validation Loss: 0.9428977966308594\n",
      "Step 279/1000, Loss: 0.7467789649963379, Validation Loss: 0.9423407316207886\n",
      "Step 280/1000, Loss: 0.7461678385734558, Validation Loss: 0.941783607006073\n",
      "Step 281/1000, Loss: 0.7455565929412842, Validation Loss: 0.9412258267402649\n",
      "Step 282/1000, Loss: 0.7449449896812439, Validation Loss: 0.9406679272651672\n",
      "Step 283/1000, Loss: 0.7443333268165588, Validation Loss: 0.9401096105575562\n",
      "Step 284/1000, Loss: 0.7437213659286499, Validation Loss: 0.9395511746406555\n",
      "Step 285/1000, Loss: 0.7431090474128723, Validation Loss: 0.9389922022819519\n",
      "Step 286/1000, Loss: 0.7424967885017395, Validation Loss: 0.938433051109314\n",
      "Step 287/1000, Loss: 0.7418841123580933, Validation Loss: 0.9378734827041626\n",
      "Step 288/1000, Loss: 0.7412709593772888, Validation Loss: 0.9373134970664978\n",
      "Step 289/1000, Loss: 0.7406579256057739, Validation Loss: 0.9367533922195435\n",
      "Step 290/1000, Loss: 0.7400445342063904, Validation Loss: 0.9361929297447205\n",
      "Step 291/1000, Loss: 0.739430844783783, Validation Loss: 0.9356319904327393\n",
      "Step 292/1000, Loss: 0.7388169169425964, Validation Loss: 0.9350707530975342\n",
      "Step 293/1000, Loss: 0.7382027506828308, Validation Loss: 0.9345092177391052\n",
      "Step 294/1000, Loss: 0.7375884056091309, Validation Loss: 0.9339473843574524\n",
      "Step 295/1000, Loss: 0.7369737029075623, Validation Loss: 0.9333851337432861\n",
      "Step 296/1000, Loss: 0.7363587021827698, Validation Loss: 0.9328223466873169\n",
      "Step 297/1000, Loss: 0.7357435822486877, Validation Loss: 0.9322595000267029\n",
      "Step 298/1000, Loss: 0.7351282238960266, Validation Loss: 0.9316962361335754\n",
      "Step 299/1000, Loss: 0.7345125079154968, Validation Loss: 0.9311324954032898\n",
      "Step 300/1000, Loss: 0.7338965535163879, Validation Loss: 0.9305684566497803\n",
      "Step 301/1000, Loss: 0.7332804203033447, Validation Loss: 0.9300040602684021\n",
      "Step 302/1000, Loss: 0.7326639890670776, Validation Loss: 0.9297218322753906\n",
      "Step 303/1000, Loss: 0.7323557138442993, Validation Loss: 0.9294394850730896\n",
      "Step 304/1000, Loss: 0.7320476770401001, Validation Loss: 0.9291573762893677\n",
      "Step 305/1000, Loss: 0.7317396998405457, Validation Loss: 0.9288753867149353\n",
      "Step 306/1000, Loss: 0.7314318418502808, Validation Loss: 0.9285932183265686\n",
      "Step 307/1000, Loss: 0.7311241626739502, Validation Loss: 0.928311288356781\n",
      "Step 308/1000, Loss: 0.7308164238929749, Validation Loss: 0.928029477596283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 309/1000, Loss: 0.7305089831352234, Validation Loss: 0.9277475476264954\n",
      "Step 310/1000, Loss: 0.730201780796051, Validation Loss: 0.9274658560752869\n",
      "Step 311/1000, Loss: 0.7298945188522339, Validation Loss: 0.9271841049194336\n",
      "Step 312/1000, Loss: 0.7295874953269958, Validation Loss: 0.9269024133682251\n",
      "Step 313/1000, Loss: 0.7292805910110474, Validation Loss: 0.9266209602355957\n",
      "Step 314/1000, Loss: 0.7289738059043884, Validation Loss: 0.926339328289032\n",
      "Step 315/1000, Loss: 0.728667140007019, Validation Loss: 0.9260578155517578\n",
      "Step 316/1000, Loss: 0.7283605933189392, Validation Loss: 0.9257764220237732\n",
      "Step 317/1000, Loss: 0.7280541658401489, Validation Loss: 0.9254951477050781\n",
      "Step 318/1000, Loss: 0.7277479767799377, Validation Loss: 0.9252138137817383\n",
      "Step 319/1000, Loss: 0.7274417877197266, Validation Loss: 0.9249324798583984\n",
      "Step 320/1000, Loss: 0.7271358370780945, Validation Loss: 0.9246513247489929\n",
      "Step 321/1000, Loss: 0.726830005645752, Validation Loss: 0.9243701696395874\n",
      "Step 322/1000, Loss: 0.726524293422699, Validation Loss: 0.9240890145301819\n",
      "Step 323/1000, Loss: 0.7262186408042908, Validation Loss: 0.9238079190254211\n",
      "Step 324/1000, Loss: 0.7259132266044617, Validation Loss: 0.9235268831253052\n",
      "Step 325/1000, Loss: 0.7256078124046326, Validation Loss: 0.923245906829834\n",
      "Step 326/1000, Loss: 0.7253025770187378, Validation Loss: 0.922964870929718\n",
      "Step 327/1000, Loss: 0.7249974012374878, Validation Loss: 0.9226839542388916\n",
      "Step 328/1000, Loss: 0.7246923446655273, Validation Loss: 0.92240309715271\n",
      "Step 329/1000, Loss: 0.7243874073028564, Validation Loss: 0.9221222400665283\n",
      "Step 330/1000, Loss: 0.7240827083587646, Validation Loss: 0.9218413233757019\n",
      "Step 331/1000, Loss: 0.7237780094146729, Validation Loss: 0.9215604662895203\n",
      "Step 332/1000, Loss: 0.7234734296798706, Validation Loss: 0.9212795495986938\n",
      "Step 333/1000, Loss: 0.7231689691543579, Validation Loss: 0.9209988117218018\n",
      "Step 334/1000, Loss: 0.72286456823349, Validation Loss: 0.9207181334495544\n",
      "Step 335/1000, Loss: 0.7225602865219116, Validation Loss: 0.9204370975494385\n",
      "Step 336/1000, Loss: 0.7222561240196228, Validation Loss: 0.9201564192771912\n",
      "Step 337/1000, Loss: 0.7219520807266235, Validation Loss: 0.9198757410049438\n",
      "Step 338/1000, Loss: 0.721648097038269, Validation Loss: 0.919594943523407\n",
      "Step 339/1000, Loss: 0.7213441729545593, Validation Loss: 0.9193140268325806\n",
      "Step 340/1000, Loss: 0.7210404872894287, Validation Loss: 0.9190332889556885\n",
      "Step 341/1000, Loss: 0.7207368016242981, Validation Loss: 0.9187525510787964\n",
      "Step 342/1000, Loss: 0.720433235168457, Validation Loss: 0.9184718132019043\n",
      "Step 343/1000, Loss: 0.720129668712616, Validation Loss: 0.9181910753250122\n",
      "Step 344/1000, Loss: 0.7198262810707092, Validation Loss: 0.9179102182388306\n",
      "Step 345/1000, Loss: 0.7195228934288025, Validation Loss: 0.9176293611526489\n",
      "Step 346/1000, Loss: 0.7192197442054749, Validation Loss: 0.9173485636711121\n",
      "Step 347/1000, Loss: 0.7189164161682129, Validation Loss: 0.9170677065849304\n",
      "Step 348/1000, Loss: 0.71861332654953, Validation Loss: 0.916786789894104\n",
      "Step 349/1000, Loss: 0.7183104157447815, Validation Loss: 0.9165059924125671\n",
      "Step 350/1000, Loss: 0.718007504940033, Validation Loss: 0.9162251353263855\n",
      "Step 351/1000, Loss: 0.7177045941352844, Validation Loss: 0.9159441590309143\n",
      "Step 352/1000, Loss: 0.7174018025398254, Validation Loss: 0.9156631231307983\n",
      "Step 353/1000, Loss: 0.717099130153656, Validation Loss: 0.9153822660446167\n",
      "Step 354/1000, Loss: 0.7167963981628418, Validation Loss: 0.915101170539856\n",
      "Step 355/1000, Loss: 0.7164938449859619, Validation Loss: 0.9148201942443848\n",
      "Step 356/1000, Loss: 0.7161914110183716, Validation Loss: 0.9145391583442688\n",
      "Step 357/1000, Loss: 0.715889036655426, Validation Loss: 0.9142580032348633\n",
      "Step 358/1000, Loss: 0.7155866622924805, Validation Loss: 0.9139769077301025\n",
      "Step 359/1000, Loss: 0.7152842283248901, Validation Loss: 0.913695752620697\n",
      "Step 360/1000, Loss: 0.7149820923805237, Validation Loss: 0.9134144186973572\n",
      "Step 361/1000, Loss: 0.7146799564361572, Validation Loss: 0.9131332039833069\n",
      "Step 362/1000, Loss: 0.7143778204917908, Validation Loss: 0.9128519296646118\n",
      "Step 363/1000, Loss: 0.7140757441520691, Validation Loss: 0.9125704765319824\n",
      "Step 364/1000, Loss: 0.7137736678123474, Validation Loss: 0.9122891426086426\n",
      "Step 365/1000, Loss: 0.7134718298912048, Validation Loss: 0.9120075702667236\n",
      "Step 366/1000, Loss: 0.7131698727607727, Validation Loss: 0.9117261171340942\n",
      "Step 367/1000, Loss: 0.7128680348396301, Validation Loss: 0.9114446640014648\n",
      "Step 368/1000, Loss: 0.7125663161277771, Validation Loss: 0.911162793636322\n",
      "Step 369/1000, Loss: 0.7122645378112793, Validation Loss: 0.9108811020851135\n",
      "Step 370/1000, Loss: 0.7119628190994263, Validation Loss: 0.9105995297431946\n",
      "Step 371/1000, Loss: 0.7116612195968628, Validation Loss: 0.9103176593780518\n",
      "Step 372/1000, Loss: 0.7113596200942993, Validation Loss: 0.9100357294082642\n",
      "Step 373/1000, Loss: 0.7110580801963806, Validation Loss: 0.9097538590431213\n",
      "Step 374/1000, Loss: 0.7107566595077515, Validation Loss: 0.909471869468689\n",
      "Step 375/1000, Loss: 0.7104551792144775, Validation Loss: 0.909189760684967\n",
      "Step 376/1000, Loss: 0.7101537585258484, Validation Loss: 0.9089076519012451\n",
      "Step 377/1000, Loss: 0.7098524570465088, Validation Loss: 0.9086253643035889\n",
      "Step 378/1000, Loss: 0.7095510363578796, Validation Loss: 0.9083430767059326\n",
      "Step 379/1000, Loss: 0.7092497944831848, Validation Loss: 0.9080606698989868\n",
      "Step 380/1000, Loss: 0.7089486122131348, Validation Loss: 0.907778263092041\n",
      "Step 381/1000, Loss: 0.7086474895477295, Validation Loss: 0.9074957966804504\n",
      "Step 382/1000, Loss: 0.7083462476730347, Validation Loss: 0.9072131514549255\n",
      "Step 383/1000, Loss: 0.7080451250076294, Validation Loss: 0.9069305062294006\n",
      "Step 384/1000, Loss: 0.7077440619468689, Validation Loss: 0.9066477417945862\n",
      "Step 385/1000, Loss: 0.7074430584907532, Validation Loss: 0.906364917755127\n",
      "Step 386/1000, Loss: 0.7071420550346375, Validation Loss: 0.9060819149017334\n",
      "Step 387/1000, Loss: 0.7068411111831665, Validation Loss: 0.9057989716529846\n",
      "Step 388/1000, Loss: 0.7065401673316956, Validation Loss: 0.9055158495903015\n",
      "Step 389/1000, Loss: 0.7062392830848694, Validation Loss: 0.9052326083183289\n",
      "Step 390/1000, Loss: 0.7059383392333984, Validation Loss: 0.9049493670463562\n",
      "Step 391/1000, Loss: 0.705637514591217, Validation Loss: 0.904666006565094\n",
      "Step 392/1000, Loss: 0.7053366899490356, Validation Loss: 0.9043826460838318\n",
      "Step 393/1000, Loss: 0.705035924911499, Validation Loss: 0.9040988683700562\n",
      "Step 394/1000, Loss: 0.7047351598739624, Validation Loss: 0.9038152694702148\n",
      "Step 395/1000, Loss: 0.7044344544410706, Validation Loss: 0.9035316109657288\n",
      "Step 396/1000, Loss: 0.7041336894035339, Validation Loss: 0.9032478332519531\n",
      "Step 397/1000, Loss: 0.7038330435752869, Validation Loss: 0.9029638767242432\n",
      "Step 398/1000, Loss: 0.7035323977470398, Validation Loss: 0.9026797413825989\n",
      "Step 399/1000, Loss: 0.7032318711280823, Validation Loss: 0.9023955464363098\n",
      "Step 400/1000, Loss: 0.7029311656951904, Validation Loss: 0.9021114110946655\n",
      "Step 401/1000, Loss: 0.7026305794715881, Validation Loss: 0.9018269777297974\n",
      "Step 402/1000, Loss: 0.7023300528526306, Validation Loss: 0.9016847610473633\n",
      "Step 403/1000, Loss: 0.7021797895431519, Validation Loss: 0.9015425443649292\n",
      "Step 404/1000, Loss: 0.7020296454429626, Validation Loss: 0.9014003276824951\n",
      "Step 405/1000, Loss: 0.7018795013427734, Validation Loss: 0.9012582302093506\n",
      "Step 406/1000, Loss: 0.701729416847229, Validation Loss: 0.9011160135269165\n",
      "Step 407/1000, Loss: 0.7015792727470398, Validation Loss: 0.9009737968444824\n",
      "Step 408/1000, Loss: 0.7014294266700745, Validation Loss: 0.9008316397666931\n",
      "Step 409/1000, Loss: 0.7012795209884644, Validation Loss: 0.900689423084259\n",
      "Step 410/1000, Loss: 0.7011296153068542, Validation Loss: 0.9005472660064697\n",
      "Step 411/1000, Loss: 0.7009797692298889, Validation Loss: 0.9004050493240356\n",
      "Step 412/1000, Loss: 0.7008300423622131, Validation Loss: 0.9002629518508911\n",
      "Step 413/1000, Loss: 0.7006803154945374, Validation Loss: 0.9001208543777466\n",
      "Step 414/1000, Loss: 0.7005306482315063, Validation Loss: 0.8999786972999573\n",
      "Step 415/1000, Loss: 0.7003811597824097, Validation Loss: 0.8998365998268127\n",
      "Step 416/1000, Loss: 0.7002315521240234, Validation Loss: 0.8996944427490234\n",
      "Step 417/1000, Loss: 0.7000821828842163, Validation Loss: 0.8995524048805237\n",
      "Step 418/1000, Loss: 0.6999326944351196, Validation Loss: 0.8994102478027344\n",
      "Step 419/1000, Loss: 0.6997832655906677, Validation Loss: 0.8992681503295898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 420/1000, Loss: 0.6996339559555054, Validation Loss: 0.8991260528564453\n",
      "Step 421/1000, Loss: 0.6994845867156982, Validation Loss: 0.8989840149879456\n",
      "Step 422/1000, Loss: 0.6993353962898254, Validation Loss: 0.8988419771194458\n",
      "Step 423/1000, Loss: 0.6991862654685974, Validation Loss: 0.8986998200416565\n",
      "Step 424/1000, Loss: 0.6990371346473694, Validation Loss: 0.8985577821731567\n",
      "Step 425/1000, Loss: 0.6988880634307861, Validation Loss: 0.8984158039093018\n",
      "Step 426/1000, Loss: 0.6987389922142029, Validation Loss: 0.898273766040802\n",
      "Step 427/1000, Loss: 0.698590099811554, Validation Loss: 0.8981317281723022\n",
      "Step 428/1000, Loss: 0.6984410285949707, Validation Loss: 0.8979896306991577\n",
      "Step 429/1000, Loss: 0.6982921957969666, Validation Loss: 0.8978476524353027\n",
      "Step 430/1000, Loss: 0.6981433033943176, Validation Loss: 0.8977056741714478\n",
      "Step 431/1000, Loss: 0.6979945302009583, Validation Loss: 0.8975638151168823\n",
      "Step 432/1000, Loss: 0.6978458166122437, Validation Loss: 0.8974217176437378\n",
      "Step 433/1000, Loss: 0.6976971626281738, Validation Loss: 0.8972797393798828\n",
      "Step 434/1000, Loss: 0.6975483894348145, Validation Loss: 0.8971378207206726\n",
      "Step 435/1000, Loss: 0.6973997950553894, Validation Loss: 0.8969958424568176\n",
      "Step 436/1000, Loss: 0.6972512602806091, Validation Loss: 0.8968538641929626\n",
      "Step 437/1000, Loss: 0.6971026062965393, Validation Loss: 0.8967118859291077\n",
      "Step 438/1000, Loss: 0.6969541907310486, Validation Loss: 0.8965697884559631\n",
      "Step 439/1000, Loss: 0.6968057155609131, Validation Loss: 0.8964279294013977\n",
      "Step 440/1000, Loss: 0.6966573596000671, Validation Loss: 0.8962859511375427\n",
      "Step 441/1000, Loss: 0.6965089440345764, Validation Loss: 0.8961440324783325\n",
      "Step 442/1000, Loss: 0.6963606476783752, Validation Loss: 0.8960020542144775\n",
      "Step 443/1000, Loss: 0.6962122917175293, Validation Loss: 0.8958600759506226\n",
      "Step 444/1000, Loss: 0.6960640549659729, Validation Loss: 0.8957181572914124\n",
      "Step 445/1000, Loss: 0.6959158182144165, Validation Loss: 0.8955761790275574\n",
      "Step 446/1000, Loss: 0.6957676410675049, Validation Loss: 0.8954343199729919\n",
      "Step 447/1000, Loss: 0.6956196427345276, Validation Loss: 0.895292341709137\n",
      "Step 448/1000, Loss: 0.695471465587616, Validation Loss: 0.895150363445282\n",
      "Step 449/1000, Loss: 0.6953233480453491, Validation Loss: 0.895008385181427\n",
      "Step 450/1000, Loss: 0.6951754093170166, Validation Loss: 0.8948664665222168\n",
      "Step 451/1000, Loss: 0.6950274109840393, Validation Loss: 0.8947244882583618\n",
      "Step 452/1000, Loss: 0.6948795318603516, Validation Loss: 0.8945825099945068\n",
      "Step 453/1000, Loss: 0.6947314739227295, Validation Loss: 0.8944405317306519\n",
      "Step 454/1000, Loss: 0.6945836544036865, Validation Loss: 0.8942987322807312\n",
      "Step 455/1000, Loss: 0.6944357752799988, Validation Loss: 0.8941567540168762\n",
      "Step 456/1000, Loss: 0.6942880153656006, Validation Loss: 0.8940147757530212\n",
      "Step 457/1000, Loss: 0.6941401958465576, Validation Loss: 0.8938727378845215\n",
      "Step 458/1000, Loss: 0.693992555141449, Validation Loss: 0.8937308192253113\n",
      "Step 459/1000, Loss: 0.6938447952270508, Validation Loss: 0.8935889005661011\n",
      "Step 460/1000, Loss: 0.6936972141265869, Validation Loss: 0.8934468626976013\n",
      "Step 461/1000, Loss: 0.6935495138168335, Validation Loss: 0.8933048248291016\n",
      "Step 462/1000, Loss: 0.6934018731117249, Validation Loss: 0.8931629657745361\n",
      "Step 463/1000, Loss: 0.693254292011261, Validation Loss: 0.8930209279060364\n",
      "Step 464/1000, Loss: 0.6931067109107971, Validation Loss: 0.892879068851471\n",
      "Step 465/1000, Loss: 0.692959189414978, Validation Loss: 0.8927369713783264\n",
      "Step 466/1000, Loss: 0.6928116679191589, Validation Loss: 0.8925949931144714\n",
      "Step 467/1000, Loss: 0.6926643252372742, Validation Loss: 0.8924529552459717\n",
      "Step 468/1000, Loss: 0.6925168037414551, Validation Loss: 0.8923109769821167\n",
      "Step 469/1000, Loss: 0.6923694610595703, Validation Loss: 0.8921689391136169\n",
      "Step 470/1000, Loss: 0.6922221183776855, Validation Loss: 0.892026960849762\n",
      "Step 471/1000, Loss: 0.6920748353004456, Validation Loss: 0.891884982585907\n",
      "Step 472/1000, Loss: 0.691927433013916, Validation Loss: 0.8917428255081177\n",
      "Step 473/1000, Loss: 0.6917802095413208, Validation Loss: 0.8916007876396179\n",
      "Step 474/1000, Loss: 0.6916329264640808, Validation Loss: 0.8914587497711182\n",
      "Step 475/1000, Loss: 0.6914856433868408, Validation Loss: 0.8913167715072632\n",
      "Step 476/1000, Loss: 0.6913385391235352, Validation Loss: 0.8911746144294739\n",
      "Step 477/1000, Loss: 0.6911913752555847, Validation Loss: 0.8910325765609741\n",
      "Step 478/1000, Loss: 0.6910443305969238, Validation Loss: 0.8908905386924744\n",
      "Step 479/1000, Loss: 0.6908971071243286, Validation Loss: 0.8907483816146851\n",
      "Step 480/1000, Loss: 0.6907500624656677, Validation Loss: 0.8906063437461853\n",
      "Step 481/1000, Loss: 0.6906030178070068, Validation Loss: 0.8904641270637512\n",
      "Step 482/1000, Loss: 0.6904559135437012, Validation Loss: 0.8903219699859619\n",
      "Step 483/1000, Loss: 0.6903088688850403, Validation Loss: 0.8901799321174622\n",
      "Step 484/1000, Loss: 0.690161943435669, Validation Loss: 0.8900377154350281\n",
      "Step 485/1000, Loss: 0.6900148987770081, Validation Loss: 0.8898956775665283\n",
      "Step 486/1000, Loss: 0.6898680329322815, Validation Loss: 0.889753520488739\n",
      "Step 487/1000, Loss: 0.6897211670875549, Validation Loss: 0.8896114230155945\n",
      "Step 488/1000, Loss: 0.6895743608474731, Validation Loss: 0.8894691467285156\n",
      "Step 489/1000, Loss: 0.6894274353981018, Validation Loss: 0.8893269896507263\n",
      "Step 490/1000, Loss: 0.68928062915802, Validation Loss: 0.8891847729682922\n",
      "Step 491/1000, Loss: 0.6891337633132935, Validation Loss: 0.8890425562858582\n",
      "Step 492/1000, Loss: 0.6889870166778564, Validation Loss: 0.8889003992080688\n",
      "Step 493/1000, Loss: 0.6888402104377747, Validation Loss: 0.8887581825256348\n",
      "Step 494/1000, Loss: 0.6886934638023376, Validation Loss: 0.8886159658432007\n",
      "Step 495/1000, Loss: 0.6885468363761902, Validation Loss: 0.8884736895561218\n",
      "Step 496/1000, Loss: 0.6884000897407532, Validation Loss: 0.8883314728736877\n",
      "Step 497/1000, Loss: 0.6882534623146057, Validation Loss: 0.8881891965866089\n",
      "Step 498/1000, Loss: 0.6881067752838135, Validation Loss: 0.8880469799041748\n",
      "Step 499/1000, Loss: 0.687960147857666, Validation Loss: 0.8879045844078064\n",
      "Step 500/1000, Loss: 0.6878135800361633, Validation Loss: 0.8877622485160828\n",
      "Step 501/1000, Loss: 0.6876668334007263, Validation Loss: 0.8876200318336487\n",
      "Step 502/1000, Loss: 0.6875203251838684, Validation Loss: 0.8875488638877869\n",
      "Step 503/1000, Loss: 0.6874471306800842, Validation Loss: 0.8874776363372803\n",
      "Step 504/1000, Loss: 0.6873738169670105, Validation Loss: 0.8874064683914185\n",
      "Step 505/1000, Loss: 0.6873005032539368, Validation Loss: 0.8873353600502014\n",
      "Step 506/1000, Loss: 0.6872273683547974, Validation Loss: 0.8872642517089844\n",
      "Step 507/1000, Loss: 0.6871541738510132, Validation Loss: 0.8871931433677673\n",
      "Step 508/1000, Loss: 0.687080979347229, Validation Loss: 0.8871219754219055\n",
      "Step 509/1000, Loss: 0.6870077848434448, Validation Loss: 0.887050986289978\n",
      "Step 510/1000, Loss: 0.6869347095489502, Validation Loss: 0.8869797587394714\n",
      "Step 511/1000, Loss: 0.6868615746498108, Validation Loss: 0.8869086503982544\n",
      "Step 512/1000, Loss: 0.6867884993553162, Validation Loss: 0.8868376612663269\n",
      "Step 513/1000, Loss: 0.6867153644561768, Validation Loss: 0.8867666125297546\n",
      "Step 514/1000, Loss: 0.6866423487663269, Validation Loss: 0.8866956830024719\n",
      "Step 515/1000, Loss: 0.6865692734718323, Validation Loss: 0.8866243958473206\n",
      "Step 516/1000, Loss: 0.6864963173866272, Validation Loss: 0.8865534663200378\n",
      "Step 517/1000, Loss: 0.6864231824874878, Validation Loss: 0.8864823579788208\n",
      "Step 518/1000, Loss: 0.6863501667976379, Validation Loss: 0.8864114880561829\n",
      "Step 519/1000, Loss: 0.6862772703170776, Validation Loss: 0.8863404393196106\n",
      "Step 520/1000, Loss: 0.6862043142318726, Validation Loss: 0.8862693309783936\n",
      "Step 521/1000, Loss: 0.6861312389373779, Validation Loss: 0.8861984014511108\n",
      "Step 522/1000, Loss: 0.6860583424568176, Validation Loss: 0.8861274719238281\n",
      "Step 523/1000, Loss: 0.6859854459762573, Validation Loss: 0.8860563635826111\n",
      "Step 524/1000, Loss: 0.685912549495697, Validation Loss: 0.8859853148460388\n",
      "Step 525/1000, Loss: 0.6858395934104919, Validation Loss: 0.8859144449234009\n",
      "Step 526/1000, Loss: 0.6857668161392212, Validation Loss: 0.8858435153961182\n",
      "Step 527/1000, Loss: 0.6856939196586609, Validation Loss: 0.8857725262641907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 528/1000, Loss: 0.6856210827827454, Validation Loss: 0.885701596736908\n",
      "Step 529/1000, Loss: 0.6855482459068298, Validation Loss: 0.88563072681427\n",
      "Step 530/1000, Loss: 0.6854753494262695, Validation Loss: 0.8855597972869873\n",
      "Step 531/1000, Loss: 0.6854026317596436, Validation Loss: 0.8854889273643494\n",
      "Step 532/1000, Loss: 0.6853298544883728, Validation Loss: 0.8854181170463562\n",
      "Step 533/1000, Loss: 0.6852571368217468, Validation Loss: 0.8853471875190735\n",
      "Step 534/1000, Loss: 0.6851843595504761, Validation Loss: 0.8852763772010803\n",
      "Step 535/1000, Loss: 0.6851118206977844, Validation Loss: 0.8852054476737976\n",
      "Step 536/1000, Loss: 0.6850389838218689, Validation Loss: 0.8851346373558044\n",
      "Step 537/1000, Loss: 0.6849663257598877, Validation Loss: 0.8850638270378113\n",
      "Step 538/1000, Loss: 0.6848936080932617, Validation Loss: 0.8849929571151733\n",
      "Step 539/1000, Loss: 0.6848209500312805, Validation Loss: 0.8849221467971802\n",
      "Step 540/1000, Loss: 0.6847482919692993, Validation Loss: 0.8848514556884766\n",
      "Step 541/1000, Loss: 0.6846756935119629, Validation Loss: 0.8847805261611938\n",
      "Step 542/1000, Loss: 0.6846030354499817, Validation Loss: 0.8847097158432007\n",
      "Step 543/1000, Loss: 0.6845304369926453, Validation Loss: 0.8846389055252075\n",
      "Step 544/1000, Loss: 0.6844578385353088, Validation Loss: 0.8845680952072144\n",
      "Step 545/1000, Loss: 0.6843852400779724, Validation Loss: 0.8844974040985107\n",
      "Step 546/1000, Loss: 0.6843127012252808, Validation Loss: 0.8844265937805176\n",
      "Step 547/1000, Loss: 0.6842402219772339, Validation Loss: 0.8843557834625244\n",
      "Step 548/1000, Loss: 0.6841676235198975, Validation Loss: 0.884285032749176\n",
      "Step 549/1000, Loss: 0.6840950846672058, Validation Loss: 0.8842142820358276\n",
      "Step 550/1000, Loss: 0.6840226054191589, Validation Loss: 0.884143590927124\n",
      "Step 551/1000, Loss: 0.6839500069618225, Validation Loss: 0.8840728402137756\n",
      "Step 552/1000, Loss: 0.6838776469230652, Validation Loss: 0.8840020895004272\n",
      "Step 553/1000, Loss: 0.6838051676750183, Validation Loss: 0.8839313387870789\n",
      "Step 554/1000, Loss: 0.6837326288223267, Validation Loss: 0.8838606476783752\n",
      "Step 555/1000, Loss: 0.6836602687835693, Validation Loss: 0.8837900161743164\n",
      "Step 556/1000, Loss: 0.6835877895355225, Validation Loss: 0.883719265460968\n",
      "Step 557/1000, Loss: 0.6835153698921204, Validation Loss: 0.8836485743522644\n",
      "Step 558/1000, Loss: 0.683443009853363, Validation Loss: 0.883577823638916\n",
      "Step 559/1000, Loss: 0.6833704710006714, Validation Loss: 0.8835071325302124\n",
      "Step 560/1000, Loss: 0.6832981109619141, Validation Loss: 0.883436381816864\n",
      "Step 561/1000, Loss: 0.6832257509231567, Validation Loss: 0.8833657503128052\n",
      "Step 562/1000, Loss: 0.6831533908843994, Validation Loss: 0.8832951188087463\n",
      "Step 563/1000, Loss: 0.6830809712409973, Validation Loss: 0.8832244873046875\n",
      "Step 564/1000, Loss: 0.6830086708068848, Validation Loss: 0.8831538558006287\n",
      "Step 565/1000, Loss: 0.6829363107681274, Validation Loss: 0.8830831050872803\n",
      "Step 566/1000, Loss: 0.6828639507293701, Validation Loss: 0.8830124735832214\n",
      "Step 567/1000, Loss: 0.6827917695045471, Validation Loss: 0.8829418420791626\n",
      "Step 568/1000, Loss: 0.6827194690704346, Validation Loss: 0.8828712701797485\n",
      "Step 569/1000, Loss: 0.6826471090316772, Validation Loss: 0.8828006982803345\n",
      "Step 570/1000, Loss: 0.6825749278068542, Validation Loss: 0.8827300071716309\n",
      "Step 571/1000, Loss: 0.6825026273727417, Validation Loss: 0.8826594352722168\n",
      "Step 572/1000, Loss: 0.6824304461479187, Validation Loss: 0.882588803768158\n",
      "Step 573/1000, Loss: 0.6823581457138062, Validation Loss: 0.8825182318687439\n",
      "Step 574/1000, Loss: 0.6822859644889832, Validation Loss: 0.8824475407600403\n",
      "Step 575/1000, Loss: 0.6822137236595154, Validation Loss: 0.882377028465271\n",
      "Step 576/1000, Loss: 0.6821416020393372, Validation Loss: 0.8823063969612122\n",
      "Step 577/1000, Loss: 0.6820693612098694, Validation Loss: 0.8822358250617981\n",
      "Step 578/1000, Loss: 0.6819972395896912, Validation Loss: 0.882165253162384\n",
      "Step 579/1000, Loss: 0.6819249987602234, Validation Loss: 0.8820946216583252\n",
      "Step 580/1000, Loss: 0.6818528175354004, Validation Loss: 0.8820239901542664\n",
      "Step 581/1000, Loss: 0.6817806959152222, Validation Loss: 0.8819535374641418\n",
      "Step 582/1000, Loss: 0.681708574295044, Validation Loss: 0.8818829655647278\n",
      "Step 583/1000, Loss: 0.6816363334655762, Validation Loss: 0.8818123936653137\n",
      "Step 584/1000, Loss: 0.681564211845398, Validation Loss: 0.8817417621612549\n",
      "Step 585/1000, Loss: 0.6814922094345093, Validation Loss: 0.8816712498664856\n",
      "Step 586/1000, Loss: 0.681420087814331, Validation Loss: 0.8816006183624268\n",
      "Step 587/1000, Loss: 0.6813479065895081, Validation Loss: 0.8815300464630127\n",
      "Step 588/1000, Loss: 0.6812757849693298, Validation Loss: 0.8814594745635986\n",
      "Step 589/1000, Loss: 0.6812038421630859, Validation Loss: 0.8813890218734741\n",
      "Step 590/1000, Loss: 0.6811317205429077, Validation Loss: 0.8813184499740601\n",
      "Step 591/1000, Loss: 0.6810596585273743, Validation Loss: 0.881247878074646\n",
      "Step 592/1000, Loss: 0.6809875965118408, Validation Loss: 0.8811774253845215\n",
      "Step 593/1000, Loss: 0.6809155941009521, Validation Loss: 0.8811068534851074\n",
      "Step 594/1000, Loss: 0.6808435916900635, Validation Loss: 0.8810363411903381\n",
      "Step 595/1000, Loss: 0.6807714700698853, Validation Loss: 0.8809658885002136\n",
      "Step 596/1000, Loss: 0.6806995272636414, Validation Loss: 0.8808953166007996\n",
      "Step 597/1000, Loss: 0.6806275248527527, Validation Loss: 0.8808247447013855\n",
      "Step 598/1000, Loss: 0.6805554628372192, Validation Loss: 0.880754292011261\n",
      "Step 599/1000, Loss: 0.6804835796356201, Validation Loss: 0.8806837797164917\n",
      "Step 600/1000, Loss: 0.6804115176200867, Validation Loss: 0.8806132078170776\n",
      "Step 601/1000, Loss: 0.6803396344184875, Validation Loss: 0.8805428743362427\n",
      "Step 602/1000, Loss: 0.6802676916122437, Validation Loss: 0.8805076479911804\n",
      "Step 603/1000, Loss: 0.6802316904067993, Validation Loss: 0.8804722428321838\n",
      "Step 604/1000, Loss: 0.680195689201355, Validation Loss: 0.8804370760917664\n",
      "Step 605/1000, Loss: 0.6801598072052002, Validation Loss: 0.8804019093513489\n",
      "Step 606/1000, Loss: 0.6801238059997559, Validation Loss: 0.8803666830062866\n",
      "Step 607/1000, Loss: 0.6800879240036011, Validation Loss: 0.8803315162658691\n",
      "Step 608/1000, Loss: 0.6800519824028015, Validation Loss: 0.8802961707115173\n",
      "Step 609/1000, Loss: 0.680016040802002, Validation Loss: 0.8802611231803894\n",
      "Step 610/1000, Loss: 0.6799800992012024, Validation Loss: 0.8802258372306824\n",
      "Step 611/1000, Loss: 0.6799442172050476, Validation Loss: 0.8801906108856201\n",
      "Step 612/1000, Loss: 0.679908275604248, Validation Loss: 0.8801553845405579\n",
      "Step 613/1000, Loss: 0.6798723936080933, Validation Loss: 0.8801202178001404\n",
      "Step 614/1000, Loss: 0.6798364520072937, Validation Loss: 0.8800851106643677\n",
      "Step 615/1000, Loss: 0.6798005700111389, Validation Loss: 0.8800498247146606\n",
      "Step 616/1000, Loss: 0.6797646880149841, Validation Loss: 0.8800146579742432\n",
      "Step 617/1000, Loss: 0.6797287464141846, Validation Loss: 0.8799793720245361\n",
      "Step 618/1000, Loss: 0.6796927452087402, Validation Loss: 0.8799443244934082\n",
      "Step 619/1000, Loss: 0.6796569228172302, Validation Loss: 0.8799091577529907\n",
      "Step 620/1000, Loss: 0.6796211004257202, Validation Loss: 0.8798739314079285\n",
      "Step 621/1000, Loss: 0.6795852184295654, Validation Loss: 0.8798388242721558\n",
      "Step 622/1000, Loss: 0.6795493960380554, Validation Loss: 0.8798036575317383\n",
      "Step 623/1000, Loss: 0.6795134544372559, Validation Loss: 0.8797684907913208\n",
      "Step 624/1000, Loss: 0.6794776916503906, Validation Loss: 0.8797333836555481\n",
      "Step 625/1000, Loss: 0.6794418692588806, Validation Loss: 0.8796983361244202\n",
      "Step 626/1000, Loss: 0.6794059872627258, Validation Loss: 0.8796630501747131\n",
      "Step 627/1000, Loss: 0.6793700456619263, Validation Loss: 0.8796278834342957\n",
      "Step 628/1000, Loss: 0.6793342232704163, Validation Loss: 0.8795927166938782\n",
      "Step 629/1000, Loss: 0.6792984008789062, Validation Loss: 0.8795576691627502\n",
      "Step 630/1000, Loss: 0.6792627573013306, Validation Loss: 0.8795225620269775\n",
      "Step 631/1000, Loss: 0.679226815700531, Validation Loss: 0.8794874548912048\n",
      "Step 632/1000, Loss: 0.679190993309021, Validation Loss: 0.8794522881507874\n",
      "Step 633/1000, Loss: 0.6791552305221558, Validation Loss: 0.8794172406196594\n",
      "Step 634/1000, Loss: 0.6791194081306458, Validation Loss: 0.8793820738792419\n",
      "Step 635/1000, Loss: 0.6790835857391357, Validation Loss: 0.879347026348114\n",
      "Step 636/1000, Loss: 0.6790478229522705, Validation Loss: 0.8793118596076965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 637/1000, Loss: 0.6790120005607605, Validation Loss: 0.879276692867279\n",
      "Step 638/1000, Loss: 0.6789762377738953, Validation Loss: 0.8792417645454407\n",
      "Step 639/1000, Loss: 0.6789404153823853, Validation Loss: 0.8792067766189575\n",
      "Step 640/1000, Loss: 0.6789047718048096, Validation Loss: 0.8791716694831848\n",
      "Step 641/1000, Loss: 0.6788690090179443, Validation Loss: 0.8791364431381226\n",
      "Step 642/1000, Loss: 0.6788331866264343, Validation Loss: 0.8791014552116394\n",
      "Step 643/1000, Loss: 0.6787973046302795, Validation Loss: 0.8790664076805115\n",
      "Step 644/1000, Loss: 0.6787616014480591, Validation Loss: 0.879031240940094\n",
      "Step 645/1000, Loss: 0.6787258386611938, Validation Loss: 0.8789962530136108\n",
      "Step 646/1000, Loss: 0.6786900758743286, Validation Loss: 0.8789611458778381\n",
      "Step 647/1000, Loss: 0.6786543130874634, Validation Loss: 0.8789262175559998\n",
      "Step 648/1000, Loss: 0.6786186695098877, Validation Loss: 0.8788910508155823\n",
      "Step 649/1000, Loss: 0.6785829067230225, Validation Loss: 0.8788561224937439\n",
      "Step 650/1000, Loss: 0.6785471439361572, Validation Loss: 0.8788209557533264\n",
      "Step 651/1000, Loss: 0.6785114407539368, Validation Loss: 0.8787859082221985\n",
      "Step 652/1000, Loss: 0.6784757375717163, Validation Loss: 0.8787508606910706\n",
      "Step 653/1000, Loss: 0.6784399747848511, Validation Loss: 0.8787158131599426\n",
      "Step 654/1000, Loss: 0.6784042716026306, Validation Loss: 0.8786808252334595\n",
      "Step 655/1000, Loss: 0.6783686280250549, Validation Loss: 0.8786457777023315\n",
      "Step 656/1000, Loss: 0.6783328652381897, Validation Loss: 0.8786108493804932\n",
      "Step 657/1000, Loss: 0.6782971024513245, Validation Loss: 0.8785756826400757\n",
      "Step 658/1000, Loss: 0.6782614588737488, Validation Loss: 0.8785407543182373\n",
      "Step 659/1000, Loss: 0.6782257556915283, Validation Loss: 0.8785056471824646\n",
      "Step 660/1000, Loss: 0.6781901121139526, Validation Loss: 0.8784707188606262\n",
      "Step 661/1000, Loss: 0.6781544089317322, Validation Loss: 0.8784357905387878\n",
      "Step 662/1000, Loss: 0.6781187057495117, Validation Loss: 0.8784006834030151\n",
      "Step 663/1000, Loss: 0.678083062171936, Validation Loss: 0.878365695476532\n",
      "Step 664/1000, Loss: 0.6780472993850708, Validation Loss: 0.8783307671546936\n",
      "Step 665/1000, Loss: 0.6780117154121399, Validation Loss: 0.8782958388328552\n",
      "Step 666/1000, Loss: 0.6779760122299194, Validation Loss: 0.8782607913017273\n",
      "Step 667/1000, Loss: 0.6779404282569885, Validation Loss: 0.8782258033752441\n",
      "Step 668/1000, Loss: 0.6779047846794128, Validation Loss: 0.8781908750534058\n",
      "Step 669/1000, Loss: 0.6778690814971924, Validation Loss: 0.8781559467315674\n",
      "Step 670/1000, Loss: 0.6778334975242615, Validation Loss: 0.878121018409729\n",
      "Step 671/1000, Loss: 0.6777979135513306, Validation Loss: 0.8780859708786011\n",
      "Step 672/1000, Loss: 0.6777622699737549, Validation Loss: 0.8780511617660522\n",
      "Step 673/1000, Loss: 0.6777266263961792, Validation Loss: 0.8780162334442139\n",
      "Step 674/1000, Loss: 0.6776910424232483, Validation Loss: 0.8779811859130859\n",
      "Step 675/1000, Loss: 0.6776553988456726, Validation Loss: 0.8779463171958923\n",
      "Step 676/1000, Loss: 0.6776197552680969, Validation Loss: 0.8779112696647644\n",
      "Step 677/1000, Loss: 0.6775840520858765, Validation Loss: 0.8778764009475708\n",
      "Step 678/1000, Loss: 0.6775485277175903, Validation Loss: 0.877841591835022\n",
      "Step 679/1000, Loss: 0.6775129437446594, Validation Loss: 0.877806544303894\n",
      "Step 680/1000, Loss: 0.6774773597717285, Validation Loss: 0.8777716755867004\n",
      "Step 681/1000, Loss: 0.6774418354034424, Validation Loss: 0.8777367472648621\n",
      "Step 682/1000, Loss: 0.6774062514305115, Validation Loss: 0.8777017593383789\n",
      "Step 683/1000, Loss: 0.6773706674575806, Validation Loss: 0.8776669502258301\n",
      "Step 684/1000, Loss: 0.6773350238800049, Validation Loss: 0.8776319622993469\n",
      "Step 685/1000, Loss: 0.677299439907074, Validation Loss: 0.8775970339775085\n",
      "Step 686/1000, Loss: 0.6772639155387878, Validation Loss: 0.8775621652603149\n",
      "Step 687/1000, Loss: 0.6772283911705017, Validation Loss: 0.8775272965431213\n",
      "Step 688/1000, Loss: 0.6771928071975708, Validation Loss: 0.877492368221283\n",
      "Step 689/1000, Loss: 0.6771572232246399, Validation Loss: 0.8774574398994446\n",
      "Step 690/1000, Loss: 0.6771216988563538, Validation Loss: 0.8774225115776062\n",
      "Step 691/1000, Loss: 0.6770861148834229, Validation Loss: 0.8773875832557678\n",
      "Step 692/1000, Loss: 0.6770505309104919, Validation Loss: 0.8773527145385742\n",
      "Step 693/1000, Loss: 0.6770150065422058, Validation Loss: 0.8773178458213806\n",
      "Step 694/1000, Loss: 0.6769794821739197, Validation Loss: 0.8772828578948975\n",
      "Step 695/1000, Loss: 0.6769437789916992, Validation Loss: 0.8772480487823486\n",
      "Step 696/1000, Loss: 0.6769083738327026, Validation Loss: 0.8772131204605103\n",
      "Step 697/1000, Loss: 0.676872730255127, Validation Loss: 0.8771781921386719\n",
      "Step 698/1000, Loss: 0.6768372058868408, Validation Loss: 0.877143383026123\n",
      "Step 699/1000, Loss: 0.6768018007278442, Validation Loss: 0.8771085739135742\n",
      "Step 700/1000, Loss: 0.6767662167549133, Validation Loss: 0.8770734667778015\n",
      "Step 701/1000, Loss: 0.6767306923866272, Validation Loss: 0.8770386576652527\n",
      "Step 702/1000, Loss: 0.6766951680183411, Validation Loss: 0.877021312713623\n",
      "Step 703/1000, Loss: 0.676677405834198, Validation Loss: 0.8770038485527039\n",
      "Step 704/1000, Loss: 0.6766596436500549, Validation Loss: 0.8769863247871399\n",
      "Step 705/1000, Loss: 0.6766418814659119, Validation Loss: 0.8769689202308655\n",
      "Step 706/1000, Loss: 0.6766241192817688, Validation Loss: 0.8769515156745911\n",
      "Step 707/1000, Loss: 0.676606297492981, Validation Loss: 0.8769340515136719\n",
      "Step 708/1000, Loss: 0.6765885353088379, Validation Loss: 0.8769166469573975\n",
      "Step 709/1000, Loss: 0.6765708327293396, Validation Loss: 0.876899242401123\n",
      "Step 710/1000, Loss: 0.6765530705451965, Validation Loss: 0.8768817782402039\n",
      "Step 711/1000, Loss: 0.6765354871749878, Validation Loss: 0.876864492893219\n",
      "Step 712/1000, Loss: 0.6765176057815552, Validation Loss: 0.8768470287322998\n",
      "Step 713/1000, Loss: 0.6764998435974121, Validation Loss: 0.8768296837806702\n",
      "Step 714/1000, Loss: 0.676482081413269, Validation Loss: 0.876812219619751\n",
      "Step 715/1000, Loss: 0.6764644384384155, Validation Loss: 0.8767948150634766\n",
      "Step 716/1000, Loss: 0.6764466762542725, Validation Loss: 0.8767774701118469\n",
      "Step 717/1000, Loss: 0.6764289736747742, Validation Loss: 0.876759946346283\n",
      "Step 718/1000, Loss: 0.6764111518859863, Validation Loss: 0.8767426013946533\n",
      "Step 719/1000, Loss: 0.676393449306488, Validation Loss: 0.8767252564430237\n",
      "Step 720/1000, Loss: 0.676375687122345, Validation Loss: 0.8767076730728149\n",
      "Step 721/1000, Loss: 0.6763579249382019, Validation Loss: 0.8766903877258301\n",
      "Step 722/1000, Loss: 0.6763403415679932, Validation Loss: 0.8766729831695557\n",
      "Step 723/1000, Loss: 0.6763225793838501, Validation Loss: 0.8766555190086365\n",
      "Step 724/1000, Loss: 0.676304817199707, Validation Loss: 0.8766381740570068\n",
      "Step 725/1000, Loss: 0.676287055015564, Validation Loss: 0.8766207695007324\n",
      "Step 726/1000, Loss: 0.6762693524360657, Validation Loss: 0.8766034841537476\n",
      "Step 727/1000, Loss: 0.6762517094612122, Validation Loss: 0.8765859603881836\n",
      "Step 728/1000, Loss: 0.6762339472770691, Validation Loss: 0.876568615436554\n",
      "Step 729/1000, Loss: 0.676216185092926, Validation Loss: 0.8765512704849243\n",
      "Step 730/1000, Loss: 0.6761985421180725, Validation Loss: 0.8765338063240051\n",
      "Step 731/1000, Loss: 0.6761807799339294, Validation Loss: 0.8765164017677307\n",
      "Step 732/1000, Loss: 0.6761631369590759, Validation Loss: 0.8764989972114563\n",
      "Step 733/1000, Loss: 0.6761453747749329, Validation Loss: 0.8764817118644714\n",
      "Step 734/1000, Loss: 0.6761277914047241, Validation Loss: 0.876464307308197\n",
      "Step 735/1000, Loss: 0.6761099100112915, Validation Loss: 0.8764469623565674\n",
      "Step 736/1000, Loss: 0.676092267036438, Validation Loss: 0.8764294981956482\n",
      "Step 737/1000, Loss: 0.6760745644569397, Validation Loss: 0.8764120936393738\n",
      "Step 738/1000, Loss: 0.6760568022727966, Validation Loss: 0.8763947486877441\n",
      "Step 739/1000, Loss: 0.6760390996932983, Validation Loss: 0.8763774037361145\n",
      "Step 740/1000, Loss: 0.6760215163230896, Validation Loss: 0.8763599395751953\n",
      "Step 741/1000, Loss: 0.6760037541389465, Validation Loss: 0.8763425946235657\n",
      "Step 742/1000, Loss: 0.6759860515594482, Validation Loss: 0.876325249671936\n",
      "Step 743/1000, Loss: 0.6759682893753052, Validation Loss: 0.8763077855110168\n",
      "Step 744/1000, Loss: 0.6759506464004517, Validation Loss: 0.876290500164032\n",
      "Step 745/1000, Loss: 0.6759330034255981, Validation Loss: 0.8762731552124023\n",
      "Step 746/1000, Loss: 0.6759152412414551, Validation Loss: 0.8762558102607727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 747/1000, Loss: 0.6758975982666016, Validation Loss: 0.8762384653091431\n",
      "Step 748/1000, Loss: 0.675879955291748, Validation Loss: 0.8762210011482239\n",
      "Step 749/1000, Loss: 0.675862193107605, Validation Loss: 0.876203715801239\n",
      "Step 750/1000, Loss: 0.6758446097373962, Validation Loss: 0.8761863112449646\n",
      "Step 751/1000, Loss: 0.6758268475532532, Validation Loss: 0.8761690258979797\n",
      "Step 752/1000, Loss: 0.6758091449737549, Validation Loss: 0.8761516213417053\n",
      "Step 753/1000, Loss: 0.6757915019989014, Validation Loss: 0.8761343955993652\n",
      "Step 754/1000, Loss: 0.6757738590240479, Validation Loss: 0.876116931438446\n",
      "Step 755/1000, Loss: 0.6757562160491943, Validation Loss: 0.8760996460914612\n",
      "Step 756/1000, Loss: 0.6757384538650513, Validation Loss: 0.8760822415351868\n",
      "Step 757/1000, Loss: 0.6757208108901978, Validation Loss: 0.8760648369789124\n",
      "Step 758/1000, Loss: 0.675703227519989, Validation Loss: 0.8760476112365723\n",
      "Step 759/1000, Loss: 0.675685465335846, Validation Loss: 0.8760302662849426\n",
      "Step 760/1000, Loss: 0.6756677627563477, Validation Loss: 0.8760128021240234\n",
      "Step 761/1000, Loss: 0.6756500005722046, Validation Loss: 0.8759955167770386\n",
      "Step 762/1000, Loss: 0.6756324172019958, Validation Loss: 0.8759781718254089\n",
      "Step 763/1000, Loss: 0.6756148338317871, Validation Loss: 0.8759608268737793\n",
      "Step 764/1000, Loss: 0.6755971312522888, Validation Loss: 0.8759434819221497\n",
      "Step 765/1000, Loss: 0.6755794286727905, Validation Loss: 0.8759261965751648\n",
      "Step 766/1000, Loss: 0.675561785697937, Validation Loss: 0.8759087920188904\n",
      "Step 767/1000, Loss: 0.6755440831184387, Validation Loss: 0.8758915066719055\n",
      "Step 768/1000, Loss: 0.67552649974823, Validation Loss: 0.8758741617202759\n",
      "Step 769/1000, Loss: 0.6755087971687317, Validation Loss: 0.8758568167686462\n",
      "Step 770/1000, Loss: 0.6754910945892334, Validation Loss: 0.8758395314216614\n",
      "Step 771/1000, Loss: 0.6754734516143799, Validation Loss: 0.8758221864700317\n",
      "Step 772/1000, Loss: 0.6754558086395264, Validation Loss: 0.8758047819137573\n",
      "Step 773/1000, Loss: 0.6754382252693176, Validation Loss: 0.8757874965667725\n",
      "Step 774/1000, Loss: 0.6754204630851746, Validation Loss: 0.8757701516151428\n",
      "Step 775/1000, Loss: 0.6754028797149658, Validation Loss: 0.875752866268158\n",
      "Step 776/1000, Loss: 0.6753851771354675, Validation Loss: 0.8757355213165283\n",
      "Step 777/1000, Loss: 0.6753675937652588, Validation Loss: 0.8757181763648987\n",
      "Step 778/1000, Loss: 0.6753498911857605, Validation Loss: 0.875700831413269\n",
      "Step 779/1000, Loss: 0.6753323078155518, Validation Loss: 0.8756835460662842\n",
      "Step 780/1000, Loss: 0.6753146052360535, Validation Loss: 0.8756661415100098\n",
      "Step 781/1000, Loss: 0.6752970218658447, Validation Loss: 0.8756489157676697\n",
      "Step 782/1000, Loss: 0.6752793192863464, Validation Loss: 0.8756315112113953\n",
      "Step 783/1000, Loss: 0.6752617359161377, Validation Loss: 0.8756142854690552\n",
      "Step 784/1000, Loss: 0.6752440333366394, Validation Loss: 0.8755968809127808\n",
      "Step 785/1000, Loss: 0.6752263307571411, Validation Loss: 0.8755795955657959\n",
      "Step 786/1000, Loss: 0.6752087473869324, Validation Loss: 0.875562310218811\n",
      "Step 787/1000, Loss: 0.6751911640167236, Validation Loss: 0.8755449652671814\n",
      "Step 788/1000, Loss: 0.6751735210418701, Validation Loss: 0.8755276203155518\n",
      "Step 789/1000, Loss: 0.6751558780670166, Validation Loss: 0.8755102753639221\n",
      "Step 790/1000, Loss: 0.6751382350921631, Validation Loss: 0.8754929900169373\n",
      "Step 791/1000, Loss: 0.6751206517219543, Validation Loss: 0.8754756450653076\n",
      "Step 792/1000, Loss: 0.6751030087471008, Validation Loss: 0.8754583597183228\n",
      "Step 793/1000, Loss: 0.6750853657722473, Validation Loss: 0.8754411339759827\n",
      "Step 794/1000, Loss: 0.6750677227973938, Validation Loss: 0.8754238486289978\n",
      "Step 795/1000, Loss: 0.6750500798225403, Validation Loss: 0.8754065036773682\n",
      "Step 796/1000, Loss: 0.6750325560569763, Validation Loss: 0.8753891587257385\n",
      "Step 797/1000, Loss: 0.6750149130821228, Validation Loss: 0.8753718733787537\n",
      "Step 798/1000, Loss: 0.6749972701072693, Validation Loss: 0.8753545880317688\n",
      "Step 799/1000, Loss: 0.6749796271324158, Validation Loss: 0.8753373026847839\n",
      "Step 800/1000, Loss: 0.674962043762207, Validation Loss: 0.8753200769424438\n",
      "Step 801/1000, Loss: 0.6749444603919983, Validation Loss: 0.875302791595459\n",
      "Step 802/1000, Loss: 0.6749268174171448, Validation Loss: 0.875294029712677\n",
      "Step 803/1000, Loss: 0.6749180555343628, Validation Loss: 0.8752853274345398\n",
      "Step 804/1000, Loss: 0.6749091744422913, Validation Loss: 0.8752766847610474\n",
      "Step 805/1000, Loss: 0.6749002933502197, Validation Loss: 0.8752681016921997\n",
      "Step 806/1000, Loss: 0.6748915314674377, Validation Loss: 0.8752593994140625\n",
      "Step 807/1000, Loss: 0.6748827695846558, Validation Loss: 0.8752506375312805\n",
      "Step 808/1000, Loss: 0.674873948097229, Validation Loss: 0.8752420544624329\n",
      "Step 809/1000, Loss: 0.6748651266098022, Validation Loss: 0.8752334713935852\n",
      "Step 810/1000, Loss: 0.6748562455177307, Validation Loss: 0.875224769115448\n",
      "Step 811/1000, Loss: 0.674847424030304, Validation Loss: 0.8752161264419556\n",
      "Step 812/1000, Loss: 0.6748386025428772, Validation Loss: 0.8752074241638184\n",
      "Step 813/1000, Loss: 0.6748297214508057, Validation Loss: 0.8751988410949707\n",
      "Step 814/1000, Loss: 0.6748209595680237, Validation Loss: 0.8751901388168335\n",
      "Step 815/1000, Loss: 0.6748121976852417, Validation Loss: 0.8751814961433411\n",
      "Step 816/1000, Loss: 0.6748033761978149, Validation Loss: 0.8751727938652039\n",
      "Step 817/1000, Loss: 0.674794614315033, Validation Loss: 0.8751642107963562\n",
      "Step 818/1000, Loss: 0.674785852432251, Validation Loss: 0.8751556873321533\n",
      "Step 819/1000, Loss: 0.674777090549469, Validation Loss: 0.8751469850540161\n",
      "Step 820/1000, Loss: 0.6747682094573975, Validation Loss: 0.8751383423805237\n",
      "Step 821/1000, Loss: 0.6747594475746155, Validation Loss: 0.875129759311676\n",
      "Step 822/1000, Loss: 0.6747506856918335, Validation Loss: 0.8751210570335388\n",
      "Step 823/1000, Loss: 0.6747418642044067, Validation Loss: 0.8751124739646912\n",
      "Step 824/1000, Loss: 0.6747331023216248, Validation Loss: 0.8751038908958435\n",
      "Step 825/1000, Loss: 0.674724280834198, Validation Loss: 0.8750951290130615\n",
      "Step 826/1000, Loss: 0.674715518951416, Validation Loss: 0.8750866055488586\n",
      "Step 827/1000, Loss: 0.6747066378593445, Validation Loss: 0.8750778436660767\n",
      "Step 828/1000, Loss: 0.6746978759765625, Validation Loss: 0.875069260597229\n",
      "Step 829/1000, Loss: 0.6746891140937805, Validation Loss: 0.8750606775283813\n",
      "Step 830/1000, Loss: 0.6746802926063538, Validation Loss: 0.8750520348548889\n",
      "Step 831/1000, Loss: 0.674671471118927, Validation Loss: 0.8750433921813965\n",
      "Step 832/1000, Loss: 0.6746627688407898, Validation Loss: 0.8750346899032593\n",
      "Step 833/1000, Loss: 0.674653947353363, Validation Loss: 0.8750261664390564\n",
      "Step 834/1000, Loss: 0.674645185470581, Validation Loss: 0.8750175833702087\n",
      "Step 835/1000, Loss: 0.6746364235877991, Validation Loss: 0.8750090003013611\n",
      "Step 836/1000, Loss: 0.6746275424957275, Validation Loss: 0.8750001788139343\n",
      "Step 837/1000, Loss: 0.6746187806129456, Validation Loss: 0.8749916553497314\n",
      "Step 838/1000, Loss: 0.6746100187301636, Validation Loss: 0.874983012676239\n",
      "Step 839/1000, Loss: 0.6746012568473816, Validation Loss: 0.8749743700027466\n",
      "Step 840/1000, Loss: 0.6745924353599548, Validation Loss: 0.8749656677246094\n",
      "Step 841/1000, Loss: 0.6745836734771729, Validation Loss: 0.8749570846557617\n",
      "Step 842/1000, Loss: 0.6745747923851013, Validation Loss: 0.8749485015869141\n",
      "Step 843/1000, Loss: 0.6745661497116089, Validation Loss: 0.8749399185180664\n",
      "Step 844/1000, Loss: 0.6745572686195374, Validation Loss: 0.8749312162399292\n",
      "Step 845/1000, Loss: 0.6745484471321106, Validation Loss: 0.8749225735664368\n",
      "Step 846/1000, Loss: 0.6745396852493286, Validation Loss: 0.8749139308929443\n",
      "Step 847/1000, Loss: 0.6745309233665466, Validation Loss: 0.8749052882194519\n",
      "Step 848/1000, Loss: 0.6745221614837646, Validation Loss: 0.8748966455459595\n",
      "Step 849/1000, Loss: 0.6745132207870483, Validation Loss: 0.8748881816864014\n",
      "Step 850/1000, Loss: 0.6745045781135559, Validation Loss: 0.8748794794082642\n",
      "Step 851/1000, Loss: 0.6744958162307739, Validation Loss: 0.874870777130127\n",
      "Step 852/1000, Loss: 0.6744870543479919, Validation Loss: 0.8748621940612793\n",
      "Step 853/1000, Loss: 0.6744781732559204, Validation Loss: 0.8748536109924316\n",
      "Step 854/1000, Loss: 0.6744694113731384, Validation Loss: 0.8748450875282288\n",
      "Step 855/1000, Loss: 0.6744606494903564, Validation Loss: 0.8748363256454468\n",
      "Step 856/1000, Loss: 0.6744518280029297, Validation Loss: 0.8748279213905334\n",
      "Step 857/1000, Loss: 0.6744431257247925, Validation Loss: 0.8748191595077515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 858/1000, Loss: 0.6744343042373657, Validation Loss: 0.874810516834259\n",
      "Step 859/1000, Loss: 0.6744254231452942, Validation Loss: 0.8748019337654114\n",
      "Step 860/1000, Loss: 0.6744167804718018, Validation Loss: 0.8747933506965637\n",
      "Step 861/1000, Loss: 0.6744078993797302, Validation Loss: 0.8747847676277161\n",
      "Step 862/1000, Loss: 0.6743991374969482, Validation Loss: 0.8747760653495789\n",
      "Step 863/1000, Loss: 0.6743903756141663, Validation Loss: 0.8747674226760864\n",
      "Step 864/1000, Loss: 0.6743817329406738, Validation Loss: 0.874758780002594\n",
      "Step 865/1000, Loss: 0.6743728518486023, Validation Loss: 0.8747502565383911\n",
      "Step 866/1000, Loss: 0.6743640303611755, Validation Loss: 0.8747416138648987\n",
      "Step 867/1000, Loss: 0.6743553280830383, Validation Loss: 0.874733030796051\n",
      "Step 868/1000, Loss: 0.6743465662002563, Validation Loss: 0.8747243881225586\n",
      "Step 869/1000, Loss: 0.6743377447128296, Validation Loss: 0.8747157454490662\n",
      "Step 870/1000, Loss: 0.6743290424346924, Validation Loss: 0.8747071623802185\n",
      "Step 871/1000, Loss: 0.6743202805519104, Validation Loss: 0.8746985197067261\n",
      "Step 872/1000, Loss: 0.6743114590644836, Validation Loss: 0.8746898770332336\n",
      "Step 873/1000, Loss: 0.6743026971817017, Validation Loss: 0.8746813535690308\n",
      "Step 874/1000, Loss: 0.6742938160896301, Validation Loss: 0.8746726512908936\n",
      "Step 875/1000, Loss: 0.6742851734161377, Validation Loss: 0.8746640682220459\n",
      "Step 876/1000, Loss: 0.6742762923240662, Validation Loss: 0.8746554255485535\n",
      "Step 877/1000, Loss: 0.6742676496505737, Validation Loss: 0.8746469020843506\n",
      "Step 878/1000, Loss: 0.6742588877677917, Validation Loss: 0.8746381998062134\n",
      "Step 879/1000, Loss: 0.6742500066757202, Validation Loss: 0.8746296167373657\n",
      "Step 880/1000, Loss: 0.6742411851882935, Validation Loss: 0.8746209144592285\n",
      "Step 881/1000, Loss: 0.6742324233055115, Validation Loss: 0.8746123313903809\n",
      "Step 882/1000, Loss: 0.6742237210273743, Validation Loss: 0.8746036291122437\n",
      "Step 883/1000, Loss: 0.6742148995399475, Validation Loss: 0.8745951652526855\n",
      "Step 884/1000, Loss: 0.6742061972618103, Validation Loss: 0.8745865225791931\n",
      "Step 885/1000, Loss: 0.6741974353790283, Validation Loss: 0.8745779395103455\n",
      "Step 886/1000, Loss: 0.6741886138916016, Validation Loss: 0.874569296836853\n",
      "Step 887/1000, Loss: 0.6741798520088196, Validation Loss: 0.8745606541633606\n",
      "Step 888/1000, Loss: 0.6741710901260376, Validation Loss: 0.8745520114898682\n",
      "Step 889/1000, Loss: 0.6741623282432556, Validation Loss: 0.8745434880256653\n",
      "Step 890/1000, Loss: 0.6741535663604736, Validation Loss: 0.8745347857475281\n",
      "Step 891/1000, Loss: 0.6741448044776917, Validation Loss: 0.8745262622833252\n",
      "Step 892/1000, Loss: 0.6741360425949097, Validation Loss: 0.8745176196098328\n",
      "Step 893/1000, Loss: 0.6741272807121277, Validation Loss: 0.8745089173316956\n",
      "Step 894/1000, Loss: 0.6741184592247009, Validation Loss: 0.8745003938674927\n",
      "Step 895/1000, Loss: 0.674109697341919, Validation Loss: 0.8744917511940002\n",
      "Step 896/1000, Loss: 0.674100935459137, Validation Loss: 0.8744832277297974\n",
      "Step 897/1000, Loss: 0.674092173576355, Validation Loss: 0.8744745254516602\n",
      "Step 898/1000, Loss: 0.674083411693573, Validation Loss: 0.8744659423828125\n",
      "Step 899/1000, Loss: 0.674074649810791, Validation Loss: 0.8744572401046753\n",
      "Step 900/1000, Loss: 0.674065887928009, Validation Loss: 0.8744486570358276\n",
      "Step 901/1000, Loss: 0.674057126045227, Validation Loss: 0.87444007396698\n",
      "Step 902/1000, Loss: 0.6740483641624451, Validation Loss: 0.8744357824325562\n",
      "Step 903/1000, Loss: 0.6740440130233765, Validation Loss: 0.8744315505027771\n",
      "Step 904/1000, Loss: 0.6740396022796631, Validation Loss: 0.8744271397590637\n",
      "Step 905/1000, Loss: 0.6740351915359497, Validation Loss: 0.8744229674339294\n",
      "Step 906/1000, Loss: 0.6740308403968811, Validation Loss: 0.8744186162948608\n",
      "Step 907/1000, Loss: 0.6740264892578125, Validation Loss: 0.8744144439697266\n",
      "Step 908/1000, Loss: 0.6740221381187439, Validation Loss: 0.874410092830658\n",
      "Step 909/1000, Loss: 0.6740177273750305, Validation Loss: 0.8744057416915894\n",
      "Step 910/1000, Loss: 0.6740133166313171, Validation Loss: 0.8744015097618103\n",
      "Step 911/1000, Loss: 0.6740090847015381, Validation Loss: 0.8743970990180969\n",
      "Step 912/1000, Loss: 0.6740046143531799, Validation Loss: 0.8743929266929626\n",
      "Step 913/1000, Loss: 0.6740003228187561, Validation Loss: 0.8743886947631836\n",
      "Step 914/1000, Loss: 0.6739957928657532, Validation Loss: 0.8743842840194702\n",
      "Step 915/1000, Loss: 0.6739915609359741, Validation Loss: 0.8743800520896912\n",
      "Step 916/1000, Loss: 0.673987090587616, Validation Loss: 0.8743757605552673\n",
      "Step 917/1000, Loss: 0.6739828586578369, Validation Loss: 0.8743714690208435\n",
      "Step 918/1000, Loss: 0.673978328704834, Validation Loss: 0.8743671178817749\n",
      "Step 919/1000, Loss: 0.6739739775657654, Validation Loss: 0.8743629455566406\n",
      "Step 920/1000, Loss: 0.6739696860313416, Validation Loss: 0.8743586540222168\n",
      "Step 921/1000, Loss: 0.673965334892273, Validation Loss: 0.8743543028831482\n",
      "Step 922/1000, Loss: 0.6739609241485596, Validation Loss: 0.8743500709533691\n",
      "Step 923/1000, Loss: 0.673956573009491, Validation Loss: 0.8743457794189453\n",
      "Step 924/1000, Loss: 0.6739521622657776, Validation Loss: 0.8743414878845215\n",
      "Step 925/1000, Loss: 0.673947811126709, Validation Loss: 0.8743373155593872\n",
      "Step 926/1000, Loss: 0.6739433407783508, Validation Loss: 0.8743329644203186\n",
      "Step 927/1000, Loss: 0.673939049243927, Validation Loss: 0.8743286728858948\n",
      "Step 928/1000, Loss: 0.6739345788955688, Validation Loss: 0.8743244409561157\n",
      "Step 929/1000, Loss: 0.6739303469657898, Validation Loss: 0.8743200302124023\n",
      "Step 930/1000, Loss: 0.6739259362220764, Validation Loss: 0.8743157386779785\n",
      "Step 931/1000, Loss: 0.673921525478363, Validation Loss: 0.8743115067481995\n",
      "Step 932/1000, Loss: 0.6739171743392944, Validation Loss: 0.8743072152137756\n",
      "Step 933/1000, Loss: 0.6739128232002258, Validation Loss: 0.8743029236793518\n",
      "Step 934/1000, Loss: 0.6739084124565125, Validation Loss: 0.874298632144928\n",
      "Step 935/1000, Loss: 0.6739040613174438, Validation Loss: 0.8742944002151489\n",
      "Step 936/1000, Loss: 0.6738996505737305, Validation Loss: 0.8742900490760803\n",
      "Step 937/1000, Loss: 0.6738952994346619, Validation Loss: 0.874285876750946\n",
      "Step 938/1000, Loss: 0.6738909482955933, Validation Loss: 0.8742814660072327\n",
      "Step 939/1000, Loss: 0.6738864183425903, Validation Loss: 0.8742771744728088\n",
      "Step 940/1000, Loss: 0.6738821268081665, Validation Loss: 0.8742730021476746\n",
      "Step 941/1000, Loss: 0.6738777756690979, Validation Loss: 0.874268651008606\n",
      "Step 942/1000, Loss: 0.6738734245300293, Validation Loss: 0.8742642998695374\n",
      "Step 943/1000, Loss: 0.6738690137863159, Validation Loss: 0.8742600679397583\n",
      "Step 944/1000, Loss: 0.6738646030426025, Validation Loss: 0.8742557168006897\n",
      "Step 945/1000, Loss: 0.6738602519035339, Validation Loss: 0.8742514848709106\n",
      "Step 946/1000, Loss: 0.6738558411598206, Validation Loss: 0.8742473125457764\n",
      "Step 947/1000, Loss: 0.673851490020752, Validation Loss: 0.8742429614067078\n",
      "Step 948/1000, Loss: 0.6738470196723938, Validation Loss: 0.8742386102676392\n",
      "Step 949/1000, Loss: 0.67384272813797, Validation Loss: 0.8742343783378601\n",
      "Step 950/1000, Loss: 0.6738383769989014, Validation Loss: 0.8742300868034363\n",
      "Step 951/1000, Loss: 0.6738340258598328, Validation Loss: 0.8742257356643677\n",
      "Step 952/1000, Loss: 0.6738296151161194, Validation Loss: 0.8742214441299438\n",
      "Step 953/1000, Loss: 0.673825204372406, Validation Loss: 0.87421715259552\n",
      "Step 954/1000, Loss: 0.6738209128379822, Validation Loss: 0.8742129802703857\n",
      "Step 955/1000, Loss: 0.6738165020942688, Validation Loss: 0.8742086291313171\n",
      "Step 956/1000, Loss: 0.6738121509552002, Validation Loss: 0.8742044568061829\n",
      "Step 957/1000, Loss: 0.6738077998161316, Validation Loss: 0.8742000460624695\n",
      "Step 958/1000, Loss: 0.673803448677063, Validation Loss: 0.8741958737373352\n",
      "Step 959/1000, Loss: 0.6737990975379944, Validation Loss: 0.8741915225982666\n",
      "Step 960/1000, Loss: 0.6737948060035706, Validation Loss: 0.8741873502731323\n",
      "Step 961/1000, Loss: 0.6737903356552124, Validation Loss: 0.8741830587387085\n",
      "Step 962/1000, Loss: 0.673785924911499, Validation Loss: 0.8741787075996399\n",
      "Step 963/1000, Loss: 0.6737815737724304, Validation Loss: 0.8741744160652161\n",
      "Step 964/1000, Loss: 0.6737772226333618, Validation Loss: 0.8741702437400818\n",
      "Step 965/1000, Loss: 0.673772931098938, Validation Loss: 0.8741657733917236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 966/1000, Loss: 0.6737684607505798, Validation Loss: 0.8741617202758789\n",
      "Step 967/1000, Loss: 0.673764169216156, Validation Loss: 0.8741573691368103\n",
      "Step 968/1000, Loss: 0.6737598180770874, Validation Loss: 0.8741530776023865\n",
      "Step 969/1000, Loss: 0.6737554669380188, Validation Loss: 0.8741486668586731\n",
      "Step 970/1000, Loss: 0.6737511157989502, Validation Loss: 0.8741444945335388\n",
      "Step 971/1000, Loss: 0.6737467050552368, Validation Loss: 0.8741402626037598\n",
      "Step 972/1000, Loss: 0.6737423539161682, Validation Loss: 0.8741359710693359\n",
      "Step 973/1000, Loss: 0.6737379431724548, Validation Loss: 0.8741317391395569\n",
      "Step 974/1000, Loss: 0.6737335920333862, Validation Loss: 0.8741275668144226\n",
      "Step 975/1000, Loss: 0.6737292408943176, Validation Loss: 0.8741230964660645\n",
      "Step 976/1000, Loss: 0.673724889755249, Validation Loss: 0.8741188645362854\n",
      "Step 977/1000, Loss: 0.67372065782547, Validation Loss: 0.8741145133972168\n",
      "Step 978/1000, Loss: 0.673716127872467, Validation Loss: 0.8741104006767273\n",
      "Step 979/1000, Loss: 0.6737117767333984, Validation Loss: 0.8741059899330139\n",
      "Step 980/1000, Loss: 0.6737074255943298, Validation Loss: 0.8741018772125244\n",
      "Step 981/1000, Loss: 0.673703134059906, Validation Loss: 0.874097466468811\n",
      "Step 982/1000, Loss: 0.6736987829208374, Validation Loss: 0.874093234539032\n",
      "Step 983/1000, Loss: 0.673694372177124, Validation Loss: 0.8740890622138977\n",
      "Step 984/1000, Loss: 0.6736900210380554, Validation Loss: 0.8740847110748291\n",
      "Step 985/1000, Loss: 0.6736856698989868, Validation Loss: 0.8740804195404053\n",
      "Step 986/1000, Loss: 0.6736813187599182, Validation Loss: 0.8740761876106262\n",
      "Step 987/1000, Loss: 0.6736769676208496, Validation Loss: 0.8740718960762024\n",
      "Step 988/1000, Loss: 0.6736725568771362, Validation Loss: 0.8740676045417786\n",
      "Step 989/1000, Loss: 0.6736682057380676, Validation Loss: 0.8740633130073547\n",
      "Step 990/1000, Loss: 0.6736637949943542, Validation Loss: 0.8740591406822205\n",
      "Step 991/1000, Loss: 0.6736595034599304, Validation Loss: 0.8740547895431519\n",
      "Step 992/1000, Loss: 0.6736551523208618, Validation Loss: 0.874050498008728\n",
      "Step 993/1000, Loss: 0.6736506819725037, Validation Loss: 0.8740461468696594\n",
      "Step 994/1000, Loss: 0.6736463308334351, Validation Loss: 0.8740418553352356\n",
      "Step 995/1000, Loss: 0.6736419796943665, Validation Loss: 0.8740375638008118\n",
      "Step 996/1000, Loss: 0.6736376285552979, Validation Loss: 0.8740333318710327\n",
      "Step 997/1000, Loss: 0.6736332178115845, Validation Loss: 0.8740289211273193\n",
      "Step 998/1000, Loss: 0.6736288666725159, Validation Loss: 0.8740246891975403\n",
      "Step 999/1000, Loss: 0.6736245155334473, Validation Loss: 0.8740203976631165\n",
      "Step 1000/1000, Loss: 0.6736202239990234, Validation Loss: 0.8740161657333374\n",
      "Step 1/1000, Loss: 1.0001459121704102, Validation Loss: 1.1631892919540405\n",
      "Step 2/1000, Loss: 0.9988601207733154, Validation Loss: 1.1620981693267822\n",
      "Step 3/1000, Loss: 0.9975792765617371, Validation Loss: 1.1610110998153687\n",
      "Step 4/1000, Loss: 0.9963030815124512, Validation Loss: 1.159928560256958\n",
      "Step 5/1000, Loss: 0.9950308799743652, Validation Loss: 1.1588495969772339\n",
      "Step 6/1000, Loss: 0.9937623739242554, Validation Loss: 1.1577742099761963\n",
      "Step 7/1000, Loss: 0.9924970269203186, Validation Loss: 1.1567018032073975\n",
      "Step 8/1000, Loss: 0.9912340641021729, Validation Loss: 1.1556320190429688\n",
      "Step 9/1000, Loss: 0.9899725317955017, Validation Loss: 1.1545647382736206\n",
      "Step 10/1000, Loss: 0.9887117743492126, Validation Loss: 1.1534996032714844\n",
      "Step 11/1000, Loss: 0.9874507784843445, Validation Loss: 1.1524362564086914\n",
      "Step 12/1000, Loss: 0.986188530921936, Validation Loss: 1.151374340057373\n",
      "Step 13/1000, Loss: 0.98492431640625, Validation Loss: 1.1503137350082397\n",
      "Step 14/1000, Loss: 0.98365718126297, Validation Loss: 1.1492537260055542\n",
      "Step 15/1000, Loss: 0.9823861122131348, Validation Loss: 1.1481938362121582\n",
      "Step 16/1000, Loss: 0.9811108112335205, Validation Loss: 1.1471335887908936\n",
      "Step 17/1000, Loss: 0.9798301458358765, Validation Loss: 1.146072268486023\n",
      "Step 18/1000, Loss: 0.9785436391830444, Validation Loss: 1.1450092792510986\n",
      "Step 19/1000, Loss: 0.9772506356239319, Validation Loss: 1.1439436674118042\n",
      "Step 20/1000, Loss: 0.9759508371353149, Validation Loss: 1.1428749561309814\n",
      "Step 21/1000, Loss: 0.9746438264846802, Validation Loss: 1.1418025493621826\n",
      "Step 22/1000, Loss: 0.9733291268348694, Validation Loss: 1.1407254934310913\n",
      "Step 23/1000, Loss: 0.9720064401626587, Validation Loss: 1.139643669128418\n",
      "Step 24/1000, Loss: 0.9706756472587585, Validation Loss: 1.138556718826294\n",
      "Step 25/1000, Loss: 0.9693363308906555, Validation Loss: 1.1374644041061401\n",
      "Step 26/1000, Loss: 0.9679887294769287, Validation Loss: 1.136366844177246\n",
      "Step 27/1000, Loss: 0.9666323065757751, Validation Loss: 1.1352635622024536\n",
      "Step 28/1000, Loss: 0.9652674794197083, Validation Loss: 1.1341549158096313\n",
      "Step 29/1000, Loss: 0.9638937711715698, Validation Loss: 1.133040428161621\n",
      "Step 30/1000, Loss: 0.9625114798545837, Validation Loss: 1.131920576095581\n",
      "Step 31/1000, Loss: 0.9611206650733948, Validation Loss: 1.1307951211929321\n",
      "Step 32/1000, Loss: 0.9597212672233582, Validation Loss: 1.1296641826629639\n",
      "Step 33/1000, Loss: 0.9583137035369873, Validation Loss: 1.1285279989242554\n",
      "Step 34/1000, Loss: 0.9568976163864136, Validation Loss: 1.1273863315582275\n",
      "Step 35/1000, Loss: 0.9554736614227295, Validation Loss: 1.1262396574020386\n",
      "Step 36/1000, Loss: 0.9540417790412903, Validation Loss: 1.125087857246399\n",
      "Step 37/1000, Loss: 0.9526021480560303, Validation Loss: 1.1239310503005981\n",
      "Step 38/1000, Loss: 0.9511550664901733, Validation Loss: 1.1227694749832153\n",
      "Step 39/1000, Loss: 0.9497007727622986, Validation Loss: 1.1216033697128296\n",
      "Step 40/1000, Loss: 0.9482393860816956, Validation Loss: 1.120432734489441\n",
      "Step 41/1000, Loss: 0.9467712044715881, Validation Loss: 1.119257926940918\n",
      "Step 42/1000, Loss: 0.9452967047691345, Validation Loss: 1.1180790662765503\n",
      "Step 43/1000, Loss: 0.9438158869743347, Validation Loss: 1.116896390914917\n",
      "Step 44/1000, Loss: 0.9423290491104126, Validation Loss: 1.115709900856018\n",
      "Step 45/1000, Loss: 0.9408366680145264, Validation Loss: 1.1145199537277222\n",
      "Step 46/1000, Loss: 0.9393388628959656, Validation Loss: 1.1133267879486084\n",
      "Step 47/1000, Loss: 0.9378360509872437, Validation Loss: 1.1121305227279663\n",
      "Step 48/1000, Loss: 0.9363281726837158, Validation Loss: 1.110931396484375\n",
      "Step 49/1000, Loss: 0.9348158240318298, Validation Loss: 1.1097296476364136\n",
      "Step 50/1000, Loss: 0.93329918384552, Validation Loss: 1.108525276184082\n",
      "Step 51/1000, Loss: 0.9317786693572998, Validation Loss: 1.1073187589645386\n",
      "Step 52/1000, Loss: 0.9302542805671692, Validation Loss: 1.1061099767684937\n",
      "Step 53/1000, Loss: 0.9287265539169312, Validation Loss: 1.1048991680145264\n",
      "Step 54/1000, Loss: 0.9271955490112305, Validation Loss: 1.1036869287490845\n",
      "Step 55/1000, Loss: 0.9256615042686462, Validation Loss: 1.1024726629257202\n",
      "Step 56/1000, Loss: 0.9241248965263367, Validation Loss: 1.1012566089630127\n",
      "Step 57/1000, Loss: 0.9225857257843018, Validation Loss: 1.1000393629074097\n",
      "Step 58/1000, Loss: 0.9210442900657654, Validation Loss: 1.098820447921753\n",
      "Step 59/1000, Loss: 0.9195008277893066, Validation Loss: 1.0976003408432007\n",
      "Step 60/1000, Loss: 0.9179555773735046, Validation Loss: 1.0963785648345947\n",
      "Step 61/1000, Loss: 0.9164088368415833, Validation Loss: 1.0951555967330933\n",
      "Step 62/1000, Loss: 0.9148602485656738, Validation Loss: 1.0939311981201172\n",
      "Step 63/1000, Loss: 0.9133105278015137, Validation Loss: 1.092705249786377\n",
      "Step 64/1000, Loss: 0.911759614944458, Validation Loss: 1.0914781093597412\n",
      "Step 65/1000, Loss: 0.9102078080177307, Validation Loss: 1.0902494192123413\n",
      "Step 66/1000, Loss: 0.908655047416687, Validation Loss: 1.0890189409255981\n",
      "Step 67/1000, Loss: 0.9071013927459717, Validation Loss: 1.0877870321273804\n",
      "Step 68/1000, Loss: 0.9055473804473877, Validation Loss: 1.0865534543991089\n",
      "Step 69/1000, Loss: 0.9039927124977112, Validation Loss: 1.0853182077407837\n",
      "Step 70/1000, Loss: 0.9024375081062317, Validation Loss: 1.0840811729431152\n",
      "Step 71/1000, Loss: 0.9008819460868835, Validation Loss: 1.082842230796814\n",
      "Step 72/1000, Loss: 0.8993260264396667, Validation Loss: 1.0816015005111694\n",
      "Step 73/1000, Loss: 0.8977699279785156, Validation Loss: 1.080358862876892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 74/1000, Loss: 0.8962136507034302, Validation Loss: 1.0791144371032715\n",
      "Step 75/1000, Loss: 0.8946570158004761, Validation Loss: 1.0778679847717285\n",
      "Step 76/1000, Loss: 0.893100380897522, Validation Loss: 1.0766197443008423\n",
      "Step 77/1000, Loss: 0.8915436267852783, Validation Loss: 1.0753694772720337\n",
      "Step 78/1000, Loss: 0.8899868130683899, Validation Loss: 1.0741175413131714\n",
      "Step 79/1000, Loss: 0.8884298801422119, Validation Loss: 1.0728638172149658\n",
      "Step 80/1000, Loss: 0.8868727684020996, Validation Loss: 1.071608304977417\n",
      "Step 81/1000, Loss: 0.8853156566619873, Validation Loss: 1.0703511238098145\n",
      "Step 82/1000, Loss: 0.8837584257125854, Validation Loss: 1.0690923929214478\n",
      "Step 83/1000, Loss: 0.882201075553894, Validation Loss: 1.0678319931030273\n",
      "Step 84/1000, Loss: 0.8806437253952026, Validation Loss: 1.0665702819824219\n",
      "Step 85/1000, Loss: 0.8790860772132874, Validation Loss: 1.0653070211410522\n",
      "Step 86/1000, Loss: 0.8775282502174377, Validation Loss: 1.0640426874160767\n",
      "Step 87/1000, Loss: 0.8759703636169434, Validation Loss: 1.062777042388916\n",
      "Step 88/1000, Loss: 0.8744122385978699, Validation Loss: 1.0615103244781494\n",
      "Step 89/1000, Loss: 0.872853696346283, Validation Loss: 1.0602424144744873\n",
      "Step 90/1000, Loss: 0.8712949156761169, Validation Loss: 1.0589734315872192\n",
      "Step 91/1000, Loss: 0.8697357773780823, Validation Loss: 1.0577032566070557\n",
      "Step 92/1000, Loss: 0.8681759834289551, Validation Loss: 1.0564321279525757\n",
      "Step 93/1000, Loss: 0.8666160106658936, Validation Loss: 1.0551599264144897\n",
      "Step 94/1000, Loss: 0.8650554418563843, Validation Loss: 1.0538864135742188\n",
      "Step 95/1000, Loss: 0.8634943962097168, Validation Loss: 1.0526118278503418\n",
      "Step 96/1000, Loss: 0.8619325757026672, Validation Loss: 1.0513359308242798\n",
      "Step 97/1000, Loss: 0.8603702783584595, Validation Loss: 1.0500587224960327\n",
      "Step 98/1000, Loss: 0.8588069677352905, Validation Loss: 1.048780083656311\n",
      "Step 99/1000, Loss: 0.8572430610656738, Validation Loss: 1.047499656677246\n",
      "Step 100/1000, Loss: 0.855678141117096, Validation Loss: 1.046217679977417\n",
      "Step 101/1000, Loss: 0.8541122674942017, Validation Loss: 1.0449336767196655\n",
      "Step 102/1000, Loss: 0.8525452613830566, Validation Loss: 1.0442909002304077\n",
      "Step 103/1000, Loss: 0.851761519908905, Validation Loss: 1.043647289276123\n",
      "Step 104/1000, Loss: 0.8509775996208191, Validation Loss: 1.0430028438568115\n",
      "Step 105/1000, Loss: 0.8501935005187988, Validation Loss: 1.0423578023910522\n",
      "Step 106/1000, Loss: 0.8494093418121338, Validation Loss: 1.0417121648788452\n",
      "Step 107/1000, Loss: 0.848625123500824, Validation Loss: 1.0410655736923218\n",
      "Step 108/1000, Loss: 0.8478409051895142, Validation Loss: 1.0404186248779297\n",
      "Step 109/1000, Loss: 0.8470566272735596, Validation Loss: 1.0397708415985107\n",
      "Step 110/1000, Loss: 0.846272349357605, Validation Loss: 1.0391223430633545\n",
      "Step 111/1000, Loss: 0.8454882502555847, Validation Loss: 1.0384732484817505\n",
      "Step 112/1000, Loss: 0.8447040915489197, Validation Loss: 1.0378234386444092\n",
      "Step 113/1000, Loss: 0.8439199328422546, Validation Loss: 1.0371729135513306\n",
      "Step 114/1000, Loss: 0.8431358337402344, Validation Loss: 1.036521553993225\n",
      "Step 115/1000, Loss: 0.8423516750335693, Validation Loss: 1.0358695983886719\n",
      "Step 116/1000, Loss: 0.8415677547454834, Validation Loss: 1.0352166891098022\n",
      "Step 117/1000, Loss: 0.8407837152481079, Validation Loss: 1.0345631837844849\n",
      "Step 118/1000, Loss: 0.839999794960022, Validation Loss: 1.033908724784851\n",
      "Step 119/1000, Loss: 0.839215874671936, Validation Loss: 1.0332534313201904\n",
      "Step 120/1000, Loss: 0.8384319543838501, Validation Loss: 1.0325974225997925\n",
      "Step 121/1000, Loss: 0.8376480340957642, Validation Loss: 1.0319404602050781\n",
      "Step 122/1000, Loss: 0.8368641138076782, Validation Loss: 1.0312823057174683\n",
      "Step 123/1000, Loss: 0.8360801935195923, Validation Loss: 1.030623435974121\n",
      "Step 124/1000, Loss: 0.8352962732315063, Validation Loss: 1.0299633741378784\n",
      "Step 125/1000, Loss: 0.8345122337341309, Validation Loss: 1.0293022394180298\n",
      "Step 126/1000, Loss: 0.8337283134460449, Validation Loss: 1.0286400318145752\n",
      "Step 127/1000, Loss: 0.8329441547393799, Validation Loss: 1.0279765129089355\n",
      "Step 128/1000, Loss: 0.8321600556373596, Validation Loss: 1.02731192111969\n",
      "Step 129/1000, Loss: 0.831375777721405, Validation Loss: 1.0266458988189697\n",
      "Step 130/1000, Loss: 0.8305913805961609, Validation Loss: 1.0259785652160645\n",
      "Step 131/1000, Loss: 0.8298069834709167, Validation Loss: 1.0253098011016846\n",
      "Step 132/1000, Loss: 0.8290222883224487, Validation Loss: 1.02463960647583\n",
      "Step 133/1000, Loss: 0.8282375931739807, Validation Loss: 1.0239677429199219\n",
      "Step 134/1000, Loss: 0.8274527788162231, Validation Loss: 1.0232945680618286\n",
      "Step 135/1000, Loss: 0.8266676068305969, Validation Loss: 1.0226194858551025\n",
      "Step 136/1000, Loss: 0.8258823156356812, Validation Loss: 1.0219427347183228\n",
      "Step 137/1000, Loss: 0.8250969052314758, Validation Loss: 1.0212639570236206\n",
      "Step 138/1000, Loss: 0.8243111371994019, Validation Loss: 1.0205835103988647\n",
      "Step 139/1000, Loss: 0.8235250115394592, Validation Loss: 1.0199010372161865\n",
      "Step 140/1000, Loss: 0.8227388858795166, Validation Loss: 1.0192166566848755\n",
      "Step 141/1000, Loss: 0.821952223777771, Validation Loss: 1.0185298919677734\n",
      "Step 142/1000, Loss: 0.8211652636528015, Validation Loss: 1.0178409814834595\n",
      "Step 143/1000, Loss: 0.8203781247138977, Validation Loss: 1.0171499252319336\n",
      "Step 144/1000, Loss: 0.8195904493331909, Validation Loss: 1.0164562463760376\n",
      "Step 145/1000, Loss: 0.818802535533905, Validation Loss: 1.0157601833343506\n",
      "Step 146/1000, Loss: 0.8180140852928162, Validation Loss: 1.015061616897583\n",
      "Step 147/1000, Loss: 0.8172252178192139, Validation Loss: 1.0143604278564453\n",
      "Step 148/1000, Loss: 0.8164359331130981, Validation Loss: 1.0136563777923584\n",
      "Step 149/1000, Loss: 0.8156461119651794, Validation Loss: 1.0129497051239014\n",
      "Step 150/1000, Loss: 0.8148558735847473, Validation Loss: 1.012239933013916\n",
      "Step 151/1000, Loss: 0.8140649795532227, Validation Loss: 1.011527180671692\n",
      "Step 152/1000, Loss: 0.8132734298706055, Validation Loss: 1.0108113288879395\n",
      "Step 153/1000, Loss: 0.8124814629554749, Validation Loss: 1.0100922584533691\n",
      "Step 154/1000, Loss: 0.8116888403892517, Validation Loss: 1.0093698501586914\n",
      "Step 155/1000, Loss: 0.8108953833580017, Validation Loss: 1.0086439847946167\n",
      "Step 156/1000, Loss: 0.810101330280304, Validation Loss: 1.007914662361145\n",
      "Step 157/1000, Loss: 0.8093065023422241, Validation Loss: 1.0071817636489868\n",
      "Step 158/1000, Loss: 0.8085107803344727, Validation Loss: 1.006445050239563\n",
      "Step 159/1000, Loss: 0.8077143430709839, Validation Loss: 1.0057045221328735\n",
      "Step 160/1000, Loss: 0.8069170117378235, Validation Loss: 1.0049599409103394\n",
      "Step 161/1000, Loss: 0.8061187863349915, Validation Loss: 1.0042115449905396\n",
      "Step 162/1000, Loss: 0.8053195476531982, Validation Loss: 1.0034587383270264\n",
      "Step 163/1000, Loss: 0.8045193552970886, Validation Loss: 1.0027018785476685\n",
      "Step 164/1000, Loss: 0.803718090057373, Validation Loss: 1.0019406080245972\n",
      "Step 165/1000, Loss: 0.802915632724762, Validation Loss: 1.0011745691299438\n",
      "Step 166/1000, Loss: 0.8021119832992554, Validation Loss: 1.0004041194915771\n",
      "Step 167/1000, Loss: 0.8013070821762085, Validation Loss: 0.9996289014816284\n",
      "Step 168/1000, Loss: 0.8005009889602661, Validation Loss: 0.9988490343093872\n",
      "Step 169/1000, Loss: 0.7996934652328491, Validation Loss: 0.9980639815330505\n",
      "Step 170/1000, Loss: 0.7988845109939575, Validation Loss: 0.9972739219665527\n",
      "Step 171/1000, Loss: 0.7980740666389465, Validation Loss: 0.9964790344238281\n",
      "Step 172/1000, Loss: 0.7972620725631714, Validation Loss: 0.9956786632537842\n",
      "Step 173/1000, Loss: 0.7964483499526978, Validation Loss: 0.9948733448982239\n",
      "Step 174/1000, Loss: 0.7956331968307495, Validation Loss: 0.9940625429153442\n",
      "Step 175/1000, Loss: 0.7948160767555237, Validation Loss: 0.9932466149330139\n",
      "Step 176/1000, Loss: 0.7939972877502441, Validation Loss: 0.992425262928009\n",
      "Step 177/1000, Loss: 0.7931765913963318, Validation Loss: 0.9915986061096191\n",
      "Step 178/1000, Loss: 0.7923539280891418, Validation Loss: 0.9907665848731995\n",
      "Step 179/1000, Loss: 0.7915293574333191, Validation Loss: 0.9899293780326843\n",
      "Step 180/1000, Loss: 0.7907026410102844, Validation Loss: 0.9890868067741394\n",
      "Step 181/1000, Loss: 0.7898739576339722, Validation Loss: 0.9882391095161438\n",
      "Step 182/1000, Loss: 0.7890430092811584, Validation Loss: 0.9873862266540527\n",
      "Step 183/1000, Loss: 0.788209855556488, Validation Loss: 0.9865283370018005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 184/1000, Loss: 0.7873746156692505, Validation Loss: 0.9856655597686768\n",
      "Step 185/1000, Loss: 0.7865370512008667, Validation Loss: 0.9847980737686157\n",
      "Step 186/1000, Loss: 0.7856971025466919, Validation Loss: 0.9839258193969727\n",
      "Step 187/1000, Loss: 0.7848547697067261, Validation Loss: 0.9830490350723267\n",
      "Step 188/1000, Loss: 0.7840101718902588, Validation Loss: 0.9821679592132568\n",
      "Step 189/1000, Loss: 0.7831629514694214, Validation Loss: 0.9812827110290527\n",
      "Step 190/1000, Loss: 0.782313346862793, Validation Loss: 0.9803934693336487\n",
      "Step 191/1000, Loss: 0.7814613580703735, Validation Loss: 0.9795002341270447\n",
      "Step 192/1000, Loss: 0.7806066274642944, Validation Loss: 0.9786033630371094\n",
      "Step 193/1000, Loss: 0.7797495722770691, Validation Loss: 0.9777029752731323\n",
      "Step 194/1000, Loss: 0.7788897752761841, Validation Loss: 0.9767993688583374\n",
      "Step 195/1000, Loss: 0.7780274748802185, Validation Loss: 0.9758924841880798\n",
      "Step 196/1000, Loss: 0.7771627902984619, Validation Loss: 0.9749826192855835\n",
      "Step 197/1000, Loss: 0.7762953042984009, Validation Loss: 0.9740703105926514\n",
      "Step 198/1000, Loss: 0.7754253149032593, Validation Loss: 0.9731552004814148\n",
      "Step 199/1000, Loss: 0.7745528817176819, Validation Loss: 0.972237765789032\n",
      "Step 200/1000, Loss: 0.7736777663230896, Validation Loss: 0.9713183045387268\n",
      "Step 201/1000, Loss: 0.7728001475334167, Validation Loss: 0.9703966975212097\n",
      "Step 202/1000, Loss: 0.7719200849533081, Validation Loss: 0.9699352383613586\n",
      "Step 203/1000, Loss: 0.7714791297912598, Validation Loss: 0.9694733619689941\n",
      "Step 204/1000, Loss: 0.7710375189781189, Validation Loss: 0.9690111875534058\n",
      "Step 205/1000, Loss: 0.7705956101417542, Validation Loss: 0.9685489535331726\n",
      "Step 206/1000, Loss: 0.770153284072876, Validation Loss: 0.9680865406990051\n",
      "Step 207/1000, Loss: 0.7697107195854187, Validation Loss: 0.9676242470741272\n",
      "Step 208/1000, Loss: 0.769267737865448, Validation Loss: 0.9671621322631836\n",
      "Step 209/1000, Loss: 0.768824577331543, Validation Loss: 0.9667001962661743\n",
      "Step 210/1000, Loss: 0.7683811783790588, Validation Loss: 0.9662385582923889\n",
      "Step 211/1000, Loss: 0.7679377198219299, Validation Loss: 0.9657772183418274\n",
      "Step 212/1000, Loss: 0.7674942016601562, Validation Loss: 0.9653162360191345\n",
      "Step 213/1000, Loss: 0.7670504450798035, Validation Loss: 0.9648557305335999\n",
      "Step 214/1000, Loss: 0.7666067481040955, Validation Loss: 0.9643955826759338\n",
      "Step 215/1000, Loss: 0.7661629319190979, Validation Loss: 0.9639359712600708\n",
      "Step 216/1000, Loss: 0.7657191753387451, Validation Loss: 0.9634767174720764\n",
      "Step 217/1000, Loss: 0.7652753591537476, Validation Loss: 0.9630181193351746\n",
      "Step 218/1000, Loss: 0.7648317217826843, Validation Loss: 0.9625600576400757\n",
      "Step 219/1000, Loss: 0.7643879055976868, Validation Loss: 0.9621025919914246\n",
      "Step 220/1000, Loss: 0.7639443278312683, Validation Loss: 0.9616456627845764\n",
      "Step 221/1000, Loss: 0.7635008096694946, Validation Loss: 0.961189329624176\n",
      "Step 222/1000, Loss: 0.7630572319030762, Validation Loss: 0.9607335329055786\n",
      "Step 223/1000, Loss: 0.7626138925552368, Validation Loss: 0.9602785110473633\n",
      "Step 224/1000, Loss: 0.7621707320213318, Validation Loss: 0.9598239660263062\n",
      "Step 225/1000, Loss: 0.7617275714874268, Validation Loss: 0.9593701958656311\n",
      "Step 226/1000, Loss: 0.761284589767456, Validation Loss: 0.958916962146759\n",
      "Step 227/1000, Loss: 0.7608416676521301, Validation Loss: 0.958464503288269\n",
      "Step 228/1000, Loss: 0.7603989839553833, Validation Loss: 0.9580127596855164\n",
      "Step 229/1000, Loss: 0.7599565386772156, Validation Loss: 0.9575616717338562\n",
      "Step 230/1000, Loss: 0.7595142722129822, Validation Loss: 0.9571111798286438\n",
      "Step 231/1000, Loss: 0.7590721249580383, Validation Loss: 0.9566614031791687\n",
      "Step 232/1000, Loss: 0.7586302161216736, Validation Loss: 0.9562124609947205\n",
      "Step 233/1000, Loss: 0.7581884264945984, Validation Loss: 0.9557642340660095\n",
      "Step 234/1000, Loss: 0.7577468752861023, Validation Loss: 0.9553166031837463\n",
      "Step 235/1000, Loss: 0.7573056221008301, Validation Loss: 0.9548696875572205\n",
      "Step 236/1000, Loss: 0.7568644285202026, Validation Loss: 0.9544237852096558\n",
      "Step 237/1000, Loss: 0.7564236521720886, Validation Loss: 0.953978419303894\n",
      "Step 238/1000, Loss: 0.7559829354286194, Validation Loss: 0.9535338878631592\n",
      "Step 239/1000, Loss: 0.7555425763130188, Validation Loss: 0.9530901312828064\n",
      "Step 240/1000, Loss: 0.7551024556159973, Validation Loss: 0.9526470899581909\n",
      "Step 241/1000, Loss: 0.7546624541282654, Validation Loss: 0.9522048234939575\n",
      "Step 242/1000, Loss: 0.7542229890823364, Validation Loss: 0.951763391494751\n",
      "Step 243/1000, Loss: 0.753783643245697, Validation Loss: 0.9513227343559265\n",
      "Step 244/1000, Loss: 0.7533445358276367, Validation Loss: 0.9508829116821289\n",
      "Step 245/1000, Loss: 0.7529057860374451, Validation Loss: 0.9504438638687134\n",
      "Step 246/1000, Loss: 0.7524672150611877, Validation Loss: 0.9500057101249695\n",
      "Step 247/1000, Loss: 0.7520291209220886, Validation Loss: 0.9495682120323181\n",
      "Step 248/1000, Loss: 0.7515912055969238, Validation Loss: 0.9491316676139832\n",
      "Step 249/1000, Loss: 0.7511536478996277, Validation Loss: 0.9486960768699646\n",
      "Step 250/1000, Loss: 0.7507162690162659, Validation Loss: 0.9482610821723938\n",
      "Step 251/1000, Loss: 0.7502793669700623, Validation Loss: 0.947827160358429\n",
      "Step 252/1000, Loss: 0.7498427033424377, Validation Loss: 0.9473938345909119\n",
      "Step 253/1000, Loss: 0.7494063973426819, Validation Loss: 0.946961522102356\n",
      "Step 254/1000, Loss: 0.7489703893661499, Validation Loss: 0.9465300440788269\n",
      "Step 255/1000, Loss: 0.7485347390174866, Validation Loss: 0.9460994005203247\n",
      "Step 256/1000, Loss: 0.7480994462966919, Validation Loss: 0.9456695914268494\n",
      "Step 257/1000, Loss: 0.7476645112037659, Validation Loss: 0.9452406167984009\n",
      "Step 258/1000, Loss: 0.7472299337387085, Validation Loss: 0.9448126554489136\n",
      "Step 259/1000, Loss: 0.7467957735061646, Validation Loss: 0.9443855881690979\n",
      "Step 260/1000, Loss: 0.7463617920875549, Validation Loss: 0.9439593553543091\n",
      "Step 261/1000, Loss: 0.7459283471107483, Validation Loss: 0.9435339570045471\n",
      "Step 262/1000, Loss: 0.7454952001571655, Validation Loss: 0.9431095123291016\n",
      "Step 263/1000, Loss: 0.7450624108314514, Validation Loss: 0.9426859617233276\n",
      "Step 264/1000, Loss: 0.7446301579475403, Validation Loss: 0.9422633647918701\n",
      "Step 265/1000, Loss: 0.7441980242729187, Validation Loss: 0.9418414831161499\n",
      "Step 266/1000, Loss: 0.7437664866447449, Validation Loss: 0.9414205551147461\n",
      "Step 267/1000, Loss: 0.7433352470397949, Validation Loss: 0.941000759601593\n",
      "Step 268/1000, Loss: 0.7429044246673584, Validation Loss: 0.9405815601348877\n",
      "Step 269/1000, Loss: 0.7424739003181458, Validation Loss: 0.9401634931564331\n",
      "Step 270/1000, Loss: 0.7420439720153809, Validation Loss: 0.9397462010383606\n",
      "Step 271/1000, Loss: 0.7416144013404846, Validation Loss: 0.9393299221992493\n",
      "Step 272/1000, Loss: 0.7411851286888123, Validation Loss: 0.9389143586158752\n",
      "Step 273/1000, Loss: 0.7407563328742981, Validation Loss: 0.938499927520752\n",
      "Step 274/1000, Loss: 0.7403279542922974, Validation Loss: 0.9380863308906555\n",
      "Step 275/1000, Loss: 0.7398999333381653, Validation Loss: 0.9376736283302307\n",
      "Step 276/1000, Loss: 0.7394723892211914, Validation Loss: 0.9372618198394775\n",
      "Step 277/1000, Loss: 0.739045262336731, Validation Loss: 0.9368510246276855\n",
      "Step 278/1000, Loss: 0.7386185526847839, Validation Loss: 0.9364410042762756\n",
      "Step 279/1000, Loss: 0.7381923198699951, Validation Loss: 0.9360320568084717\n",
      "Step 280/1000, Loss: 0.737766444683075, Validation Loss: 0.9356240034103394\n",
      "Step 281/1000, Loss: 0.737341046333313, Validation Loss: 0.9352168440818787\n",
      "Step 282/1000, Loss: 0.7369160056114197, Validation Loss: 0.9348104000091553\n",
      "Step 283/1000, Loss: 0.7364915609359741, Validation Loss: 0.9344052672386169\n",
      "Step 284/1000, Loss: 0.7360674738883972, Validation Loss: 0.9340007901191711\n",
      "Step 285/1000, Loss: 0.7356438040733337, Validation Loss: 0.933597207069397\n",
      "Step 286/1000, Loss: 0.7352205514907837, Validation Loss: 0.933194637298584\n",
      "Step 287/1000, Loss: 0.7347978353500366, Validation Loss: 0.9327928423881531\n",
      "Step 288/1000, Loss: 0.7343754768371582, Validation Loss: 0.9323921799659729\n",
      "Step 289/1000, Loss: 0.7339534759521484, Validation Loss: 0.9319923520088196\n",
      "Step 290/1000, Loss: 0.7335320711135864, Validation Loss: 0.9315933585166931\n",
      "Step 291/1000, Loss: 0.7331110835075378, Validation Loss: 0.9311951994895935\n",
      "Step 292/1000, Loss: 0.7326905131340027, Validation Loss: 0.9307982325553894\n",
      "Step 293/1000, Loss: 0.7322704792022705, Validation Loss: 0.9304018616676331\n",
      "Step 294/1000, Loss: 0.7318507432937622, Validation Loss: 0.9300066232681274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 295/1000, Loss: 0.7314316630363464, Validation Loss: 0.9296121597290039\n",
      "Step 296/1000, Loss: 0.7310128808021545, Validation Loss: 0.929218590259552\n",
      "Step 297/1000, Loss: 0.7305946350097656, Validation Loss: 0.9288259744644165\n",
      "Step 298/1000, Loss: 0.7301767468452454, Validation Loss: 0.9284342527389526\n",
      "Step 299/1000, Loss: 0.7297594547271729, Validation Loss: 0.9280434846878052\n",
      "Step 300/1000, Loss: 0.729342520236969, Validation Loss: 0.9276534914970398\n",
      "Step 301/1000, Loss: 0.7289259433746338, Validation Loss: 0.9272644519805908\n",
      "Step 302/1000, Loss: 0.7285100221633911, Validation Loss: 0.9270704388618469\n",
      "Step 303/1000, Loss: 0.7283022999763489, Validation Loss: 0.9268767833709717\n",
      "Step 304/1000, Loss: 0.728094756603241, Validation Loss: 0.9266835451126099\n",
      "Step 305/1000, Loss: 0.7278874516487122, Validation Loss: 0.926490843296051\n",
      "Step 306/1000, Loss: 0.727680504322052, Validation Loss: 0.9262984991073608\n",
      "Step 307/1000, Loss: 0.7274739146232605, Validation Loss: 0.9261065721511841\n",
      "Step 308/1000, Loss: 0.7272674441337585, Validation Loss: 0.925915002822876\n",
      "Step 309/1000, Loss: 0.7270613312721252, Validation Loss: 0.9257238507270813\n",
      "Step 310/1000, Loss: 0.7268555164337158, Validation Loss: 0.9255332350730896\n",
      "Step 311/1000, Loss: 0.7266498804092407, Validation Loss: 0.925342857837677\n",
      "Step 312/1000, Loss: 0.7264446020126343, Validation Loss: 0.9251527786254883\n",
      "Step 313/1000, Loss: 0.7262396216392517, Validation Loss: 0.9249631762504578\n",
      "Step 314/1000, Loss: 0.7260348796844482, Validation Loss: 0.9247738122940063\n",
      "Step 315/1000, Loss: 0.7258304953575134, Validation Loss: 0.9245846271514893\n",
      "Step 316/1000, Loss: 0.7256262898445129, Validation Loss: 0.9243960976600647\n",
      "Step 317/1000, Loss: 0.7254223227500916, Validation Loss: 0.9242077469825745\n",
      "Step 318/1000, Loss: 0.725218653678894, Validation Loss: 0.9240197539329529\n",
      "Step 319/1000, Loss: 0.7250153422355652, Validation Loss: 0.9238319993019104\n",
      "Step 320/1000, Loss: 0.7248122692108154, Validation Loss: 0.9236447215080261\n",
      "Step 321/1000, Loss: 0.7246094346046448, Validation Loss: 0.9234577417373657\n",
      "Step 322/1000, Loss: 0.724406898021698, Validation Loss: 0.9232710003852844\n",
      "Step 323/1000, Loss: 0.7242045998573303, Validation Loss: 0.9230846166610718\n",
      "Step 324/1000, Loss: 0.7240025997161865, Validation Loss: 0.9228984117507935\n",
      "Step 325/1000, Loss: 0.723800778388977, Validation Loss: 0.9227126240730286\n",
      "Step 326/1000, Loss: 0.7235993146896362, Validation Loss: 0.9225271940231323\n",
      "Step 327/1000, Loss: 0.7233981490135193, Validation Loss: 0.9223418831825256\n",
      "Step 328/1000, Loss: 0.7231971025466919, Validation Loss: 0.9221569895744324\n",
      "Step 329/1000, Loss: 0.7229963541030884, Validation Loss: 0.9219723343849182\n",
      "Step 330/1000, Loss: 0.722795844078064, Validation Loss: 0.9217880964279175\n",
      "Step 331/1000, Loss: 0.7225956320762634, Validation Loss: 0.9216040372848511\n",
      "Step 332/1000, Loss: 0.722395658493042, Validation Loss: 0.9214202761650085\n",
      "Step 333/1000, Loss: 0.7221959829330444, Validation Loss: 0.9212367534637451\n",
      "Step 334/1000, Loss: 0.7219964861869812, Validation Loss: 0.9210535883903503\n",
      "Step 335/1000, Loss: 0.7217971682548523, Validation Loss: 0.9208707213401794\n",
      "Step 336/1000, Loss: 0.7215982675552368, Validation Loss: 0.9206880927085876\n",
      "Step 337/1000, Loss: 0.7213994860649109, Validation Loss: 0.9205057621002197\n",
      "Step 338/1000, Loss: 0.7212009429931641, Validation Loss: 0.9203236103057861\n",
      "Step 339/1000, Loss: 0.7210026383399963, Validation Loss: 0.920141875743866\n",
      "Step 340/1000, Loss: 0.7208045721054077, Validation Loss: 0.9199603199958801\n",
      "Step 341/1000, Loss: 0.720606803894043, Validation Loss: 0.9197790622711182\n",
      "Step 342/1000, Loss: 0.7204092144966125, Validation Loss: 0.9195981025695801\n",
      "Step 343/1000, Loss: 0.7202118039131165, Validation Loss: 0.9194175004959106\n",
      "Step 344/1000, Loss: 0.7200146913528442, Validation Loss: 0.9192370176315308\n",
      "Step 345/1000, Loss: 0.7198178172111511, Validation Loss: 0.91905677318573\n",
      "Step 346/1000, Loss: 0.7196210622787476, Validation Loss: 0.9188769459724426\n",
      "Step 347/1000, Loss: 0.7194246649742126, Validation Loss: 0.9186972975730896\n",
      "Step 348/1000, Loss: 0.7192284464836121, Validation Loss: 0.9185178875923157\n",
      "Step 349/1000, Loss: 0.7190324664115906, Validation Loss: 0.9183388352394104\n",
      "Step 350/1000, Loss: 0.7188366055488586, Validation Loss: 0.9181601405143738\n",
      "Step 351/1000, Loss: 0.7186410427093506, Validation Loss: 0.9179814457893372\n",
      "Step 352/1000, Loss: 0.7184457778930664, Validation Loss: 0.9178032279014587\n",
      "Step 353/1000, Loss: 0.718250572681427, Validation Loss: 0.9176251292228699\n",
      "Step 354/1000, Loss: 0.7180556058883667, Validation Loss: 0.9174473881721497\n",
      "Step 355/1000, Loss: 0.7178609371185303, Validation Loss: 0.917269766330719\n",
      "Step 356/1000, Loss: 0.7176663875579834, Validation Loss: 0.9170925617218018\n",
      "Step 357/1000, Loss: 0.7174721360206604, Validation Loss: 0.9169155955314636\n",
      "Step 358/1000, Loss: 0.717278003692627, Validation Loss: 0.9167388081550598\n",
      "Step 359/1000, Loss: 0.7170841097831726, Validation Loss: 0.9165623188018799\n",
      "Step 360/1000, Loss: 0.7168903350830078, Validation Loss: 0.9163860082626343\n",
      "Step 361/1000, Loss: 0.7166969776153564, Validation Loss: 0.9162101745605469\n",
      "Step 362/1000, Loss: 0.7165036201477051, Validation Loss: 0.916034460067749\n",
      "Step 363/1000, Loss: 0.7163105607032776, Validation Loss: 0.9158588647842407\n",
      "Step 364/1000, Loss: 0.7161176800727844, Validation Loss: 0.9156836271286011\n",
      "Step 365/1000, Loss: 0.7159250378608704, Validation Loss: 0.9155086874961853\n",
      "Step 366/1000, Loss: 0.7157325148582458, Validation Loss: 0.9153339862823486\n",
      "Step 367/1000, Loss: 0.7155401110649109, Validation Loss: 0.9151594042778015\n",
      "Step 368/1000, Loss: 0.7153480648994446, Validation Loss: 0.9149852991104126\n",
      "Step 369/1000, Loss: 0.7151561379432678, Validation Loss: 0.9148111939430237\n",
      "Step 370/1000, Loss: 0.7149644494056702, Validation Loss: 0.9146375060081482\n",
      "Step 371/1000, Loss: 0.7147728800773621, Validation Loss: 0.9144640564918518\n",
      "Step 372/1000, Loss: 0.7145815491676331, Validation Loss: 0.9142907857894897\n",
      "Step 373/1000, Loss: 0.7143902778625488, Validation Loss: 0.9141178131103516\n",
      "Step 374/1000, Loss: 0.714199423789978, Validation Loss: 0.9139450788497925\n",
      "Step 375/1000, Loss: 0.714008629322052, Validation Loss: 0.9137724041938782\n",
      "Step 376/1000, Loss: 0.7138180136680603, Validation Loss: 0.9136000871658325\n",
      "Step 377/1000, Loss: 0.7136276364326477, Validation Loss: 0.9134280681610107\n",
      "Step 378/1000, Loss: 0.7134373188018799, Validation Loss: 0.9132563471794128\n",
      "Step 379/1000, Loss: 0.7132472991943359, Validation Loss: 0.9130846858024597\n",
      "Step 380/1000, Loss: 0.7130574584007263, Validation Loss: 0.9129132628440857\n",
      "Step 381/1000, Loss: 0.7128677368164062, Validation Loss: 0.9127421975135803\n",
      "Step 382/1000, Loss: 0.7126781940460205, Validation Loss: 0.912571370601654\n",
      "Step 383/1000, Loss: 0.7124888300895691, Validation Loss: 0.9124007821083069\n",
      "Step 384/1000, Loss: 0.712299644947052, Validation Loss: 0.912230372428894\n",
      "Step 385/1000, Loss: 0.7121107578277588, Validation Loss: 0.9120602011680603\n",
      "Step 386/1000, Loss: 0.7119218707084656, Validation Loss: 0.9118902683258057\n",
      "Step 387/1000, Loss: 0.7117332220077515, Validation Loss: 0.9117205739021301\n",
      "Step 388/1000, Loss: 0.7115447521209717, Validation Loss: 0.9115509390830994\n",
      "Step 389/1000, Loss: 0.7113564610481262, Validation Loss: 0.9113817811012268\n",
      "Step 390/1000, Loss: 0.7111682295799255, Validation Loss: 0.9112127423286438\n",
      "Step 391/1000, Loss: 0.7109802961349487, Validation Loss: 0.9110440611839294\n",
      "Step 392/1000, Loss: 0.7107924818992615, Validation Loss: 0.9108754396438599\n",
      "Step 393/1000, Loss: 0.7106047868728638, Validation Loss: 0.9107070565223694\n",
      "Step 394/1000, Loss: 0.7104173302650452, Validation Loss: 0.9105388522148132\n",
      "Step 395/1000, Loss: 0.7102300524711609, Validation Loss: 0.910370945930481\n",
      "Step 396/1000, Loss: 0.7100428938865662, Validation Loss: 0.9102033376693726\n",
      "Step 397/1000, Loss: 0.7098559141159058, Validation Loss: 0.9100359082221985\n",
      "Step 398/1000, Loss: 0.7096691131591797, Validation Loss: 0.9098686575889587\n",
      "Step 399/1000, Loss: 0.7094824314117432, Validation Loss: 0.9097015857696533\n",
      "Step 400/1000, Loss: 0.7092959880828857, Validation Loss: 0.909534752368927\n",
      "Step 401/1000, Loss: 0.7091096043586731, Validation Loss: 0.9093682169914246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 402/1000, Loss: 0.7089233994483948, Validation Loss: 0.9092850089073181\n",
      "Step 403/1000, Loss: 0.7088304162025452, Validation Loss: 0.909201979637146\n",
      "Step 404/1000, Loss: 0.7087374925613403, Validation Loss: 0.9091190099716187\n",
      "Step 405/1000, Loss: 0.708644688129425, Validation Loss: 0.9090362191200256\n",
      "Step 406/1000, Loss: 0.7085517644882202, Validation Loss: 0.9089533686637878\n",
      "Step 407/1000, Loss: 0.7084590792655945, Validation Loss: 0.9088706970214844\n",
      "Step 408/1000, Loss: 0.7083665132522583, Validation Loss: 0.9087880849838257\n",
      "Step 409/1000, Loss: 0.7082740068435669, Validation Loss: 0.9087055921554565\n",
      "Step 410/1000, Loss: 0.7081815600395203, Validation Loss: 0.908623218536377\n",
      "Step 411/1000, Loss: 0.7080891132354736, Validation Loss: 0.9085409045219421\n",
      "Step 412/1000, Loss: 0.7079967856407166, Validation Loss: 0.9084587097167969\n",
      "Step 413/1000, Loss: 0.707904577255249, Validation Loss: 0.9083764553070068\n",
      "Step 414/1000, Loss: 0.7078124284744263, Validation Loss: 0.9082944393157959\n",
      "Step 415/1000, Loss: 0.7077203392982483, Validation Loss: 0.9082126021385193\n",
      "Step 416/1000, Loss: 0.7076283693313599, Validation Loss: 0.9081306457519531\n",
      "Step 417/1000, Loss: 0.7075363397598267, Validation Loss: 0.9080488085746765\n",
      "Step 418/1000, Loss: 0.7074445486068726, Validation Loss: 0.9079670310020447\n",
      "Step 419/1000, Loss: 0.7073526382446289, Validation Loss: 0.9078853726387024\n",
      "Step 420/1000, Loss: 0.7072609663009644, Validation Loss: 0.9078037738800049\n",
      "Step 421/1000, Loss: 0.7071694731712341, Validation Loss: 0.9077221155166626\n",
      "Step 422/1000, Loss: 0.7070778012275696, Validation Loss: 0.9076406955718994\n",
      "Step 423/1000, Loss: 0.7069862484931946, Validation Loss: 0.9075593948364258\n",
      "Step 424/1000, Loss: 0.7068949341773987, Validation Loss: 0.9074779748916626\n",
      "Step 425/1000, Loss: 0.7068035006523132, Validation Loss: 0.9073967933654785\n",
      "Step 426/1000, Loss: 0.7067121863365173, Validation Loss: 0.9073154926300049\n",
      "Step 427/1000, Loss: 0.706620991230011, Validation Loss: 0.9072343707084656\n",
      "Step 428/1000, Loss: 0.7065298557281494, Validation Loss: 0.9071533679962158\n",
      "Step 429/1000, Loss: 0.7064388394355774, Validation Loss: 0.9070724248886108\n",
      "Step 430/1000, Loss: 0.7063478231430054, Validation Loss: 0.9069914817810059\n",
      "Step 431/1000, Loss: 0.7062568664550781, Validation Loss: 0.9069105982780457\n",
      "Step 432/1000, Loss: 0.7061660289764404, Validation Loss: 0.906829833984375\n",
      "Step 433/1000, Loss: 0.7060751914978027, Validation Loss: 0.9067491292953491\n",
      "Step 434/1000, Loss: 0.7059844732284546, Validation Loss: 0.9066684246063232\n",
      "Step 435/1000, Loss: 0.7058937549591064, Validation Loss: 0.9065878391265869\n",
      "Step 436/1000, Loss: 0.7058031558990479, Validation Loss: 0.9065073728561401\n",
      "Step 437/1000, Loss: 0.7057126760482788, Validation Loss: 0.9064269065856934\n",
      "Step 438/1000, Loss: 0.7056222558021545, Validation Loss: 0.9063464999198914\n",
      "Step 439/1000, Loss: 0.7055318355560303, Validation Loss: 0.9062660932540894\n",
      "Step 440/1000, Loss: 0.7054414749145508, Validation Loss: 0.906186044216156\n",
      "Step 441/1000, Loss: 0.7053512930870056, Validation Loss: 0.9061056971549988\n",
      "Step 442/1000, Loss: 0.7052610516548157, Validation Loss: 0.9060255289077759\n",
      "Step 443/1000, Loss: 0.7051708698272705, Validation Loss: 0.9059455394744873\n",
      "Step 444/1000, Loss: 0.7050807476043701, Validation Loss: 0.9058656096458435\n",
      "Step 445/1000, Loss: 0.704990804195404, Validation Loss: 0.9057855010032654\n",
      "Step 446/1000, Loss: 0.704900860786438, Validation Loss: 0.9057056307792664\n",
      "Step 447/1000, Loss: 0.7048109769821167, Validation Loss: 0.9056258797645569\n",
      "Step 448/1000, Loss: 0.7047211527824402, Validation Loss: 0.9055460095405579\n",
      "Step 449/1000, Loss: 0.7046313881874084, Validation Loss: 0.9054663777351379\n",
      "Step 450/1000, Loss: 0.7045416831970215, Validation Loss: 0.905386745929718\n",
      "Step 451/1000, Loss: 0.7044520378112793, Validation Loss: 0.9053072333335876\n",
      "Step 452/1000, Loss: 0.7043623924255371, Validation Loss: 0.9052276611328125\n",
      "Step 453/1000, Loss: 0.7042728662490845, Validation Loss: 0.9051482081413269\n",
      "Step 454/1000, Loss: 0.7041834592819214, Validation Loss: 0.9050686955451965\n",
      "Step 455/1000, Loss: 0.7040939331054688, Validation Loss: 0.90498948097229\n",
      "Step 456/1000, Loss: 0.70400470495224, Validation Loss: 0.9049100875854492\n",
      "Step 457/1000, Loss: 0.7039154171943665, Validation Loss: 0.9048309922218323\n",
      "Step 458/1000, Loss: 0.7038261890411377, Validation Loss: 0.9047518372535706\n",
      "Step 459/1000, Loss: 0.7037369608879089, Validation Loss: 0.9046727418899536\n",
      "Step 460/1000, Loss: 0.7036478519439697, Validation Loss: 0.9045937657356262\n",
      "Step 461/1000, Loss: 0.7035588622093201, Validation Loss: 0.904514729976654\n",
      "Step 462/1000, Loss: 0.7034698724746704, Validation Loss: 0.9044358134269714\n",
      "Step 463/1000, Loss: 0.7033809423446655, Validation Loss: 0.9043570756912231\n",
      "Step 464/1000, Loss: 0.7032921314239502, Validation Loss: 0.9042782187461853\n",
      "Step 465/1000, Loss: 0.7032032608985901, Validation Loss: 0.9041994214057922\n",
      "Step 466/1000, Loss: 0.7031144499778748, Validation Loss: 0.904120922088623\n",
      "Step 467/1000, Loss: 0.703025758266449, Validation Loss: 0.90404212474823\n",
      "Step 468/1000, Loss: 0.7029370665550232, Validation Loss: 0.9039637446403503\n",
      "Step 469/1000, Loss: 0.7028484344482422, Validation Loss: 0.9038851261138916\n",
      "Step 470/1000, Loss: 0.702759861946106, Validation Loss: 0.903806746006012\n",
      "Step 471/1000, Loss: 0.7026714086532593, Validation Loss: 0.9037284255027771\n",
      "Step 472/1000, Loss: 0.7025830745697021, Validation Loss: 0.9036501049995422\n",
      "Step 473/1000, Loss: 0.7024946212768555, Validation Loss: 0.9035717844963074\n",
      "Step 474/1000, Loss: 0.7024062871932983, Validation Loss: 0.9034936428070068\n",
      "Step 475/1000, Loss: 0.7023179531097412, Validation Loss: 0.9034155011177063\n",
      "Step 476/1000, Loss: 0.7022297978401184, Validation Loss: 0.9033374190330505\n",
      "Step 477/1000, Loss: 0.7021415829658508, Validation Loss: 0.9032595157623291\n",
      "Step 478/1000, Loss: 0.702053427696228, Validation Loss: 0.9031814336776733\n",
      "Step 479/1000, Loss: 0.7019653916358948, Validation Loss: 0.9031034111976624\n",
      "Step 480/1000, Loss: 0.7018774151802063, Validation Loss: 0.90302574634552\n",
      "Step 481/1000, Loss: 0.7017894387245178, Validation Loss: 0.9029479622840881\n",
      "Step 482/1000, Loss: 0.7017015814781189, Validation Loss: 0.902870237827301\n",
      "Step 483/1000, Loss: 0.7016136050224304, Validation Loss: 0.9027925133705139\n",
      "Step 484/1000, Loss: 0.7015258073806763, Validation Loss: 0.9027149081230164\n",
      "Step 485/1000, Loss: 0.7014380693435669, Validation Loss: 0.9026373028755188\n",
      "Step 486/1000, Loss: 0.7013503909111023, Validation Loss: 0.9025598764419556\n",
      "Step 487/1000, Loss: 0.7012627720832825, Validation Loss: 0.9024823904037476\n",
      "Step 488/1000, Loss: 0.7011750340461731, Validation Loss: 0.9024050831794739\n",
      "Step 489/1000, Loss: 0.701087474822998, Validation Loss: 0.9023277163505554\n",
      "Step 490/1000, Loss: 0.7010000348091125, Validation Loss: 0.9022504687309265\n",
      "Step 491/1000, Loss: 0.700912594795227, Validation Loss: 0.9021732807159424\n",
      "Step 492/1000, Loss: 0.7008252143859863, Validation Loss: 0.9020962119102478\n",
      "Step 493/1000, Loss: 0.7007378935813904, Validation Loss: 0.9020191431045532\n",
      "Step 494/1000, Loss: 0.7006505727767944, Validation Loss: 0.9019420146942139\n",
      "Step 495/1000, Loss: 0.7005632519721985, Validation Loss: 0.9018650650978088\n",
      "Step 496/1000, Loss: 0.7004759907722473, Validation Loss: 0.9017882347106934\n",
      "Step 497/1000, Loss: 0.7003889083862305, Validation Loss: 0.9017114043235779\n",
      "Step 498/1000, Loss: 0.7003017663955688, Validation Loss: 0.9016345739364624\n",
      "Step 499/1000, Loss: 0.7002148628234863, Validation Loss: 0.9015578627586365\n",
      "Step 500/1000, Loss: 0.7001278400421143, Validation Loss: 0.9014812707901001\n",
      "Step 501/1000, Loss: 0.700040876865387, Validation Loss: 0.9014046788215637\n",
      "Step 502/1000, Loss: 0.6999540328979492, Validation Loss: 0.9013664126396179\n",
      "Step 503/1000, Loss: 0.6999107003211975, Validation Loss: 0.9013281464576721\n",
      "Step 504/1000, Loss: 0.6998672485351562, Validation Loss: 0.9012900590896606\n",
      "Step 505/1000, Loss: 0.6998239159584045, Validation Loss: 0.9012517929077148\n",
      "Step 506/1000, Loss: 0.6997805237770081, Validation Loss: 0.9012136459350586\n",
      "Step 507/1000, Loss: 0.6997372508049011, Validation Loss: 0.9011754989624023\n",
      "Step 508/1000, Loss: 0.6996939778327942, Validation Loss: 0.9011374711990356\n",
      "Step 509/1000, Loss: 0.6996507048606873, Validation Loss: 0.9010993242263794\n",
      "Step 510/1000, Loss: 0.6996074318885803, Validation Loss: 0.9010614156723022\n",
      "Step 511/1000, Loss: 0.6995642781257629, Validation Loss: 0.9010234475135803\n",
      "Step 512/1000, Loss: 0.6995210647583008, Validation Loss: 0.9009853601455688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 513/1000, Loss: 0.6994778513908386, Validation Loss: 0.9009473919868469\n",
      "Step 514/1000, Loss: 0.699434757232666, Validation Loss: 0.900909423828125\n",
      "Step 515/1000, Loss: 0.6993916034698486, Validation Loss: 0.9008714556694031\n",
      "Step 516/1000, Loss: 0.6993485689163208, Validation Loss: 0.9008334875106812\n",
      "Step 517/1000, Loss: 0.6993053555488586, Validation Loss: 0.9007956385612488\n",
      "Step 518/1000, Loss: 0.6992623209953308, Validation Loss: 0.9007577896118164\n",
      "Step 519/1000, Loss: 0.6992193460464478, Validation Loss: 0.900719940662384\n",
      "Step 520/1000, Loss: 0.6991763114929199, Validation Loss: 0.9006821513175964\n",
      "Step 521/1000, Loss: 0.6991333365440369, Validation Loss: 0.9006443023681641\n",
      "Step 522/1000, Loss: 0.6990904808044434, Validation Loss: 0.9006065726280212\n",
      "Step 523/1000, Loss: 0.6990475058555603, Validation Loss: 0.9005687832832336\n",
      "Step 524/1000, Loss: 0.6990046501159668, Validation Loss: 0.9005310535430908\n",
      "Step 525/1000, Loss: 0.6989615559577942, Validation Loss: 0.900493323802948\n",
      "Step 526/1000, Loss: 0.6989187598228455, Validation Loss: 0.9004555940628052\n",
      "Step 527/1000, Loss: 0.6988759636878967, Validation Loss: 0.9004178643226624\n",
      "Step 528/1000, Loss: 0.698833167552948, Validation Loss: 0.9003802537918091\n",
      "Step 529/1000, Loss: 0.698790431022644, Validation Loss: 0.900342583656311\n",
      "Step 530/1000, Loss: 0.6987475156784058, Validation Loss: 0.9003050327301025\n",
      "Step 531/1000, Loss: 0.6987047791481018, Validation Loss: 0.9002674221992493\n",
      "Step 532/1000, Loss: 0.6986620426177979, Validation Loss: 0.9002296924591064\n",
      "Step 533/1000, Loss: 0.6986193060874939, Validation Loss: 0.9001920819282532\n",
      "Step 534/1000, Loss: 0.6985766887664795, Validation Loss: 0.9001545906066895\n",
      "Step 535/1000, Loss: 0.6985339522361755, Validation Loss: 0.900117039680481\n",
      "Step 536/1000, Loss: 0.6984912753105164, Validation Loss: 0.9000795483589172\n",
      "Step 537/1000, Loss: 0.698448657989502, Validation Loss: 0.9000419974327087\n",
      "Step 538/1000, Loss: 0.6984059810638428, Validation Loss: 0.9000046849250793\n",
      "Step 539/1000, Loss: 0.6983634233474731, Validation Loss: 0.8999670743942261\n",
      "Step 540/1000, Loss: 0.6983208060264587, Validation Loss: 0.8999295830726624\n",
      "Step 541/1000, Loss: 0.6982782483100891, Validation Loss: 0.8998922109603882\n",
      "Step 542/1000, Loss: 0.6982357501983643, Validation Loss: 0.899854838848114\n",
      "Step 543/1000, Loss: 0.6981932520866394, Validation Loss: 0.8998174667358398\n",
      "Step 544/1000, Loss: 0.6981506943702698, Validation Loss: 0.8997800946235657\n",
      "Step 545/1000, Loss: 0.6981083154678345, Validation Loss: 0.899742603302002\n",
      "Step 546/1000, Loss: 0.6980658173561096, Validation Loss: 0.8997052907943726\n",
      "Step 547/1000, Loss: 0.6980233788490295, Validation Loss: 0.8996680974960327\n",
      "Step 548/1000, Loss: 0.6979808807373047, Validation Loss: 0.8996307253837585\n",
      "Step 549/1000, Loss: 0.6979385614395142, Validation Loss: 0.8995933532714844\n",
      "Step 550/1000, Loss: 0.6978961229324341, Validation Loss: 0.8995561003684998\n",
      "Step 551/1000, Loss: 0.6978538036346436, Validation Loss: 0.8995190262794495\n",
      "Step 552/1000, Loss: 0.6978115439414978, Validation Loss: 0.8994815945625305\n",
      "Step 553/1000, Loss: 0.6977691650390625, Validation Loss: 0.8994444012641907\n",
      "Step 554/1000, Loss: 0.697726845741272, Validation Loss: 0.8994072675704956\n",
      "Step 555/1000, Loss: 0.6976845860481262, Validation Loss: 0.899370014667511\n",
      "Step 556/1000, Loss: 0.6976423859596252, Validation Loss: 0.8993328809738159\n",
      "Step 557/1000, Loss: 0.6976000666618347, Validation Loss: 0.8992956876754761\n",
      "Step 558/1000, Loss: 0.6975578665733337, Validation Loss: 0.8992586731910706\n",
      "Step 559/1000, Loss: 0.697515606880188, Validation Loss: 0.8992214798927307\n",
      "Step 560/1000, Loss: 0.6974734663963318, Validation Loss: 0.8991844058036804\n",
      "Step 561/1000, Loss: 0.6974312663078308, Validation Loss: 0.8991473317146301\n",
      "Step 562/1000, Loss: 0.6973890066146851, Validation Loss: 0.8991102576255798\n",
      "Step 563/1000, Loss: 0.6973469853401184, Validation Loss: 0.8990732431411743\n",
      "Step 564/1000, Loss: 0.6973048448562622, Validation Loss: 0.899036169052124\n",
      "Step 565/1000, Loss: 0.6972627639770508, Validation Loss: 0.8989992141723633\n",
      "Step 566/1000, Loss: 0.6972206234931946, Validation Loss: 0.8989622592926025\n",
      "Step 567/1000, Loss: 0.6971786022186279, Validation Loss: 0.898925244808197\n",
      "Step 568/1000, Loss: 0.6971365809440613, Validation Loss: 0.8988882899284363\n",
      "Step 569/1000, Loss: 0.6970945596694946, Validation Loss: 0.8988512754440308\n",
      "Step 570/1000, Loss: 0.6970524787902832, Validation Loss: 0.8988143801689148\n",
      "Step 571/1000, Loss: 0.6970105767250061, Validation Loss: 0.8987776041030884\n",
      "Step 572/1000, Loss: 0.6969684958457947, Validation Loss: 0.8987405896186829\n",
      "Step 573/1000, Loss: 0.6969265341758728, Validation Loss: 0.8987036347389221\n",
      "Step 574/1000, Loss: 0.6968846917152405, Validation Loss: 0.8986669182777405\n",
      "Step 575/1000, Loss: 0.6968427300453186, Validation Loss: 0.8986300826072693\n",
      "Step 576/1000, Loss: 0.6968008279800415, Validation Loss: 0.8985932469367981\n",
      "Step 577/1000, Loss: 0.6967588663101196, Validation Loss: 0.8985564708709717\n",
      "Step 578/1000, Loss: 0.6967170834541321, Validation Loss: 0.89851975440979\n",
      "Step 579/1000, Loss: 0.6966753005981445, Validation Loss: 0.8984829187393188\n",
      "Step 580/1000, Loss: 0.6966333985328674, Validation Loss: 0.8984462022781372\n",
      "Step 581/1000, Loss: 0.6965916156768799, Validation Loss: 0.8984093070030212\n",
      "Step 582/1000, Loss: 0.6965497732162476, Validation Loss: 0.8983726501464844\n",
      "Step 583/1000, Loss: 0.6965080499649048, Validation Loss: 0.8983360528945923\n",
      "Step 584/1000, Loss: 0.6964662671089172, Validation Loss: 0.8982993364334106\n",
      "Step 585/1000, Loss: 0.6964244842529297, Validation Loss: 0.8982626795768738\n",
      "Step 586/1000, Loss: 0.6963827610015869, Validation Loss: 0.8982260227203369\n",
      "Step 587/1000, Loss: 0.6963410377502441, Validation Loss: 0.8981894254684448\n",
      "Step 588/1000, Loss: 0.6962993741035461, Validation Loss: 0.898152768611908\n",
      "Step 589/1000, Loss: 0.6962577104568481, Validation Loss: 0.8981162309646606\n",
      "Step 590/1000, Loss: 0.6962159276008606, Validation Loss: 0.8980796337127686\n",
      "Step 591/1000, Loss: 0.6961743831634521, Validation Loss: 0.8980429172515869\n",
      "Step 592/1000, Loss: 0.6961327195167542, Validation Loss: 0.8980064392089844\n",
      "Step 593/1000, Loss: 0.6960910558700562, Validation Loss: 0.8979699015617371\n",
      "Step 594/1000, Loss: 0.6960493922233582, Validation Loss: 0.8979334235191345\n",
      "Step 595/1000, Loss: 0.6960078477859497, Validation Loss: 0.8978968858718872\n",
      "Step 596/1000, Loss: 0.6959663033485413, Validation Loss: 0.8978604674339294\n",
      "Step 597/1000, Loss: 0.6959248185157776, Validation Loss: 0.8978239893913269\n",
      "Step 598/1000, Loss: 0.6958832144737244, Validation Loss: 0.8977875113487244\n",
      "Step 599/1000, Loss: 0.6958417296409607, Validation Loss: 0.8977510333061218\n",
      "Step 600/1000, Loss: 0.6958001852035522, Validation Loss: 0.8977145552635193\n",
      "Step 601/1000, Loss: 0.695758581161499, Validation Loss: 0.8976781368255615\n",
      "Step 602/1000, Loss: 0.6957170963287354, Validation Loss: 0.8976600170135498\n",
      "Step 603/1000, Loss: 0.6956964135169983, Validation Loss: 0.8976417183876038\n",
      "Step 604/1000, Loss: 0.6956756114959717, Validation Loss: 0.8976235389709473\n",
      "Step 605/1000, Loss: 0.6956549882888794, Validation Loss: 0.8976054191589355\n",
      "Step 606/1000, Loss: 0.6956342458724976, Validation Loss: 0.8975871801376343\n",
      "Step 607/1000, Loss: 0.6956135630607605, Validation Loss: 0.8975690603256226\n",
      "Step 608/1000, Loss: 0.6955928802490234, Validation Loss: 0.8975509405136108\n",
      "Step 609/1000, Loss: 0.6955722570419312, Validation Loss: 0.8975326418876648\n",
      "Step 610/1000, Loss: 0.6955514550209045, Validation Loss: 0.8975147008895874\n",
      "Step 611/1000, Loss: 0.6955307126045227, Validation Loss: 0.8974965810775757\n",
      "Step 612/1000, Loss: 0.6955100893974304, Validation Loss: 0.8974783420562744\n",
      "Step 613/1000, Loss: 0.6954893469810486, Validation Loss: 0.8974603414535522\n",
      "Step 614/1000, Loss: 0.6954687237739563, Validation Loss: 0.897442102432251\n",
      "Step 615/1000, Loss: 0.6954480409622192, Validation Loss: 0.8974239826202393\n",
      "Step 616/1000, Loss: 0.695427417755127, Validation Loss: 0.8974059224128723\n",
      "Step 617/1000, Loss: 0.6954067945480347, Validation Loss: 0.8973879218101501\n",
      "Step 618/1000, Loss: 0.6953861713409424, Validation Loss: 0.8973696827888489\n",
      "Step 619/1000, Loss: 0.6953656077384949, Validation Loss: 0.8973516225814819\n",
      "Step 620/1000, Loss: 0.6953449845314026, Validation Loss: 0.8973336815834045\n",
      "Step 621/1000, Loss: 0.6953243613243103, Validation Loss: 0.897315502166748\n",
      "Step 622/1000, Loss: 0.695303738117218, Validation Loss: 0.8972974419593811\n",
      "Step 623/1000, Loss: 0.6952831745147705, Validation Loss: 0.8972794413566589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 624/1000, Loss: 0.6952625513076782, Validation Loss: 0.8972610831260681\n",
      "Step 625/1000, Loss: 0.6952419877052307, Validation Loss: 0.8972431421279907\n",
      "Step 626/1000, Loss: 0.6952213644981384, Validation Loss: 0.8972251415252686\n",
      "Step 627/1000, Loss: 0.6952008008956909, Validation Loss: 0.8972070217132568\n",
      "Step 628/1000, Loss: 0.6951801776885986, Validation Loss: 0.8971891403198242\n",
      "Step 629/1000, Loss: 0.6951596140861511, Validation Loss: 0.8971710205078125\n",
      "Step 630/1000, Loss: 0.6951391100883484, Validation Loss: 0.8971529603004456\n",
      "Step 631/1000, Loss: 0.6951185464859009, Validation Loss: 0.8971350789070129\n",
      "Step 632/1000, Loss: 0.6950979828834534, Validation Loss: 0.897117018699646\n",
      "Step 633/1000, Loss: 0.6950775384902954, Validation Loss: 0.897098958492279\n",
      "Step 634/1000, Loss: 0.6950569748878479, Validation Loss: 0.8970809578895569\n",
      "Step 635/1000, Loss: 0.6950364708900452, Validation Loss: 0.8970628976821899\n",
      "Step 636/1000, Loss: 0.6950159072875977, Validation Loss: 0.8970450162887573\n",
      "Step 637/1000, Loss: 0.6949954628944397, Validation Loss: 0.8970269560813904\n",
      "Step 638/1000, Loss: 0.6949748992919922, Validation Loss: 0.8970089554786682\n",
      "Step 639/1000, Loss: 0.6949544548988342, Validation Loss: 0.896990954875946\n",
      "Step 640/1000, Loss: 0.6949339509010315, Validation Loss: 0.8969729542732239\n",
      "Step 641/1000, Loss: 0.694913387298584, Validation Loss: 0.8969550132751465\n",
      "Step 642/1000, Loss: 0.6948930025100708, Validation Loss: 0.8969370126724243\n",
      "Step 643/1000, Loss: 0.6948725581169128, Validation Loss: 0.8969190716743469\n",
      "Step 644/1000, Loss: 0.6948519945144653, Validation Loss: 0.8969011902809143\n",
      "Step 645/1000, Loss: 0.6948316097259521, Validation Loss: 0.8968831896781921\n",
      "Step 646/1000, Loss: 0.6948111057281494, Validation Loss: 0.8968651294708252\n",
      "Step 647/1000, Loss: 0.6947906613349915, Validation Loss: 0.8968471884727478\n",
      "Step 648/1000, Loss: 0.6947701573371887, Validation Loss: 0.8968293070793152\n",
      "Step 649/1000, Loss: 0.6947497725486755, Validation Loss: 0.896811306476593\n",
      "Step 650/1000, Loss: 0.6947292685508728, Validation Loss: 0.8967934250831604\n",
      "Step 651/1000, Loss: 0.6947088241577148, Validation Loss: 0.8967754244804382\n",
      "Step 652/1000, Loss: 0.6946884989738464, Validation Loss: 0.8967574834823608\n",
      "Step 653/1000, Loss: 0.6946679949760437, Validation Loss: 0.8967396020889282\n",
      "Step 654/1000, Loss: 0.6946476101875305, Validation Loss: 0.8967217803001404\n",
      "Step 655/1000, Loss: 0.6946272253990173, Validation Loss: 0.8967037796974182\n",
      "Step 656/1000, Loss: 0.6946068406105042, Validation Loss: 0.8966858386993408\n",
      "Step 657/1000, Loss: 0.694586455821991, Validation Loss: 0.8966679573059082\n",
      "Step 658/1000, Loss: 0.6945659518241882, Validation Loss: 0.8966501951217651\n",
      "Step 659/1000, Loss: 0.6945456266403198, Validation Loss: 0.8966321349143982\n",
      "Step 660/1000, Loss: 0.6945251822471619, Validation Loss: 0.8966142535209656\n",
      "Step 661/1000, Loss: 0.6945048570632935, Validation Loss: 0.896596372127533\n",
      "Step 662/1000, Loss: 0.6944844722747803, Validation Loss: 0.8965785503387451\n",
      "Step 663/1000, Loss: 0.6944641470909119, Validation Loss: 0.8965606689453125\n",
      "Step 664/1000, Loss: 0.6944437623023987, Validation Loss: 0.8965427875518799\n",
      "Step 665/1000, Loss: 0.6944234371185303, Validation Loss: 0.8965249061584473\n",
      "Step 666/1000, Loss: 0.6944029927253723, Validation Loss: 0.8965070843696594\n",
      "Step 667/1000, Loss: 0.6943827271461487, Validation Loss: 0.8964892625808716\n",
      "Step 668/1000, Loss: 0.6943624019622803, Validation Loss: 0.8964712619781494\n",
      "Step 669/1000, Loss: 0.6943420767784119, Validation Loss: 0.8964534997940063\n",
      "Step 670/1000, Loss: 0.6943216919898987, Validation Loss: 0.8964356780052185\n",
      "Step 671/1000, Loss: 0.6943013668060303, Validation Loss: 0.8964177370071411\n",
      "Step 672/1000, Loss: 0.6942810416221619, Validation Loss: 0.896399974822998\n",
      "Step 673/1000, Loss: 0.6942607164382935, Validation Loss: 0.8963820338249207\n",
      "Step 674/1000, Loss: 0.694240391254425, Validation Loss: 0.896364152431488\n",
      "Step 675/1000, Loss: 0.6942200660705566, Validation Loss: 0.8963464498519897\n",
      "Step 676/1000, Loss: 0.694199800491333, Validation Loss: 0.8963285088539124\n",
      "Step 677/1000, Loss: 0.6941795349121094, Validation Loss: 0.8963107466697693\n",
      "Step 678/1000, Loss: 0.694159209728241, Validation Loss: 0.896293044090271\n",
      "Step 679/1000, Loss: 0.6941389441490173, Validation Loss: 0.8962752223014832\n",
      "Step 680/1000, Loss: 0.6941186189651489, Validation Loss: 0.8962574005126953\n",
      "Step 681/1000, Loss: 0.6940982937812805, Validation Loss: 0.8962394595146179\n",
      "Step 682/1000, Loss: 0.6940780878067017, Validation Loss: 0.8962216973304749\n",
      "Step 683/1000, Loss: 0.694057822227478, Validation Loss: 0.8962039351463318\n",
      "Step 684/1000, Loss: 0.6940374970436096, Validation Loss: 0.8961861729621887\n",
      "Step 685/1000, Loss: 0.6940172910690308, Validation Loss: 0.8961684107780457\n",
      "Step 686/1000, Loss: 0.6939970254898071, Validation Loss: 0.8961505889892578\n",
      "Step 687/1000, Loss: 0.6939767003059387, Validation Loss: 0.8961328864097595\n",
      "Step 688/1000, Loss: 0.6939565539360046, Validation Loss: 0.8961150646209717\n",
      "Step 689/1000, Loss: 0.693936288356781, Validation Loss: 0.8960973024368286\n",
      "Step 690/1000, Loss: 0.6939160823822021, Validation Loss: 0.8960795998573303\n",
      "Step 691/1000, Loss: 0.6938958168029785, Validation Loss: 0.8960617780685425\n",
      "Step 692/1000, Loss: 0.6938755512237549, Validation Loss: 0.8960440158843994\n",
      "Step 693/1000, Loss: 0.6938554048538208, Validation Loss: 0.8960263133049011\n",
      "Step 694/1000, Loss: 0.6938351988792419, Validation Loss: 0.8960085511207581\n",
      "Step 695/1000, Loss: 0.6938149333000183, Validation Loss: 0.8959908485412598\n",
      "Step 696/1000, Loss: 0.6937947869300842, Validation Loss: 0.8959731459617615\n",
      "Step 697/1000, Loss: 0.6937745213508606, Validation Loss: 0.8959553241729736\n",
      "Step 698/1000, Loss: 0.6937543749809265, Validation Loss: 0.8959376215934753\n",
      "Step 699/1000, Loss: 0.6937341094017029, Validation Loss: 0.8959197998046875\n",
      "Step 700/1000, Loss: 0.6937139630317688, Validation Loss: 0.8959022164344788\n",
      "Step 701/1000, Loss: 0.6936936974525452, Validation Loss: 0.8958845734596252\n",
      "Step 702/1000, Loss: 0.6936735510826111, Validation Loss: 0.8958757519721985\n",
      "Step 703/1000, Loss: 0.693663477897644, Validation Loss: 0.8958667516708374\n",
      "Step 704/1000, Loss: 0.6936534643173218, Validation Loss: 0.8958579897880554\n",
      "Step 705/1000, Loss: 0.6936432719230652, Validation Loss: 0.8958489894866943\n",
      "Step 706/1000, Loss: 0.6936333179473877, Validation Loss: 0.8958401679992676\n",
      "Step 707/1000, Loss: 0.6936231851577759, Validation Loss: 0.8958314061164856\n",
      "Step 708/1000, Loss: 0.6936130523681641, Validation Loss: 0.8958225250244141\n",
      "Step 709/1000, Loss: 0.693602979183197, Validation Loss: 0.8958137631416321\n",
      "Step 710/1000, Loss: 0.69359290599823, Validation Loss: 0.8958048820495605\n",
      "Step 711/1000, Loss: 0.6935828924179077, Validation Loss: 0.895796000957489\n",
      "Step 712/1000, Loss: 0.6935728192329407, Validation Loss: 0.8957872986793518\n",
      "Step 713/1000, Loss: 0.6935627460479736, Validation Loss: 0.8957782983779907\n",
      "Step 714/1000, Loss: 0.6935526132583618, Validation Loss: 0.8957695364952087\n",
      "Step 715/1000, Loss: 0.6935426592826843, Validation Loss: 0.895760715007782\n",
      "Step 716/1000, Loss: 0.6935325264930725, Validation Loss: 0.895751953125\n",
      "Step 717/1000, Loss: 0.6935224533081055, Validation Loss: 0.8957430720329285\n",
      "Step 718/1000, Loss: 0.6935124397277832, Validation Loss: 0.8957341313362122\n",
      "Step 719/1000, Loss: 0.6935023665428162, Validation Loss: 0.8957253098487854\n",
      "Step 720/1000, Loss: 0.6934922933578491, Validation Loss: 0.8957164883613586\n",
      "Step 721/1000, Loss: 0.6934822201728821, Validation Loss: 0.8957077264785767\n",
      "Step 722/1000, Loss: 0.693472146987915, Validation Loss: 0.8956988453865051\n",
      "Step 723/1000, Loss: 0.6934621334075928, Validation Loss: 0.8956899642944336\n",
      "Step 724/1000, Loss: 0.6934520602226257, Validation Loss: 0.8956812620162964\n",
      "Step 725/1000, Loss: 0.6934420466423035, Validation Loss: 0.8956723809242249\n",
      "Step 726/1000, Loss: 0.6934319734573364, Validation Loss: 0.8956635594367981\n",
      "Step 727/1000, Loss: 0.6934219598770142, Validation Loss: 0.8956547975540161\n",
      "Step 728/1000, Loss: 0.6934118866920471, Validation Loss: 0.8956459760665894\n",
      "Step 729/1000, Loss: 0.6934018731117249, Validation Loss: 0.8956370949745178\n",
      "Step 730/1000, Loss: 0.6933918595314026, Validation Loss: 0.8956283330917358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 731/1000, Loss: 0.6933817267417908, Validation Loss: 0.8956194519996643\n",
      "Step 732/1000, Loss: 0.6933717131614685, Validation Loss: 0.8956105709075928\n",
      "Step 733/1000, Loss: 0.6933616995811462, Validation Loss: 0.8956018686294556\n",
      "Step 734/1000, Loss: 0.693351686000824, Validation Loss: 0.8955931067466736\n",
      "Step 735/1000, Loss: 0.6933416724205017, Validation Loss: 0.8955841660499573\n",
      "Step 736/1000, Loss: 0.6933315396308899, Validation Loss: 0.8955753445625305\n",
      "Step 737/1000, Loss: 0.6933215260505676, Validation Loss: 0.8955665826797485\n",
      "Step 738/1000, Loss: 0.6933115720748901, Validation Loss: 0.8955578207969666\n",
      "Step 739/1000, Loss: 0.6933014988899231, Validation Loss: 0.8955488801002502\n",
      "Step 740/1000, Loss: 0.6932914853096008, Validation Loss: 0.895540177822113\n",
      "Step 741/1000, Loss: 0.6932815313339233, Validation Loss: 0.8955312967300415\n",
      "Step 742/1000, Loss: 0.6932715177536011, Validation Loss: 0.8955227136611938\n",
      "Step 743/1000, Loss: 0.6932615041732788, Validation Loss: 0.8955137729644775\n",
      "Step 744/1000, Loss: 0.6932514309883118, Validation Loss: 0.895504891872406\n",
      "Step 745/1000, Loss: 0.6932414174079895, Validation Loss: 0.8954961895942688\n",
      "Step 746/1000, Loss: 0.693231463432312, Validation Loss: 0.895487368106842\n",
      "Step 747/1000, Loss: 0.693221390247345, Validation Loss: 0.8954785466194153\n",
      "Step 748/1000, Loss: 0.6932113766670227, Validation Loss: 0.8954697251319885\n",
      "Step 749/1000, Loss: 0.6932014226913452, Validation Loss: 0.8954609632492065\n",
      "Step 750/1000, Loss: 0.693191409111023, Validation Loss: 0.8954521417617798\n",
      "Step 751/1000, Loss: 0.6931813955307007, Validation Loss: 0.8954433798789978\n",
      "Step 752/1000, Loss: 0.6931713819503784, Validation Loss: 0.895434558391571\n",
      "Step 753/1000, Loss: 0.6931613683700562, Validation Loss: 0.8954257369041443\n",
      "Step 754/1000, Loss: 0.6931512951850891, Validation Loss: 0.8954169750213623\n",
      "Step 755/1000, Loss: 0.6931412816047668, Validation Loss: 0.8954082131385803\n",
      "Step 756/1000, Loss: 0.6931312680244446, Validation Loss: 0.895399272441864\n",
      "Step 757/1000, Loss: 0.6931213736534119, Validation Loss: 0.8953905701637268\n",
      "Step 758/1000, Loss: 0.6931113600730896, Validation Loss: 0.8953818678855896\n",
      "Step 759/1000, Loss: 0.6931013464927673, Validation Loss: 0.8953729867935181\n",
      "Step 760/1000, Loss: 0.6930913329124451, Validation Loss: 0.8953642249107361\n",
      "Step 761/1000, Loss: 0.6930814385414124, Validation Loss: 0.8953554630279541\n",
      "Step 762/1000, Loss: 0.6930714845657349, Validation Loss: 0.8953467011451721\n",
      "Step 763/1000, Loss: 0.6930614709854126, Validation Loss: 0.8953378796577454\n",
      "Step 764/1000, Loss: 0.6930514574050903, Validation Loss: 0.8953291177749634\n",
      "Step 765/1000, Loss: 0.6930415630340576, Validation Loss: 0.8953203558921814\n",
      "Step 766/1000, Loss: 0.6930315494537354, Validation Loss: 0.895311713218689\n",
      "Step 767/1000, Loss: 0.6930215358734131, Validation Loss: 0.8953028917312622\n",
      "Step 768/1000, Loss: 0.6930116415023804, Validation Loss: 0.8952940106391907\n",
      "Step 769/1000, Loss: 0.6930016875267029, Validation Loss: 0.8952853679656982\n",
      "Step 770/1000, Loss: 0.6929916739463806, Validation Loss: 0.8952766060829163\n",
      "Step 771/1000, Loss: 0.6929817795753479, Validation Loss: 0.8952677845954895\n",
      "Step 772/1000, Loss: 0.6929718255996704, Validation Loss: 0.8952590823173523\n",
      "Step 773/1000, Loss: 0.6929618120193481, Validation Loss: 0.8952503204345703\n",
      "Step 774/1000, Loss: 0.6929519176483154, Validation Loss: 0.8952416181564331\n",
      "Step 775/1000, Loss: 0.6929419636726379, Validation Loss: 0.8952328562736511\n",
      "Step 776/1000, Loss: 0.6929319500923157, Validation Loss: 0.8952241539955139\n",
      "Step 777/1000, Loss: 0.692922055721283, Validation Loss: 0.8952152729034424\n",
      "Step 778/1000, Loss: 0.6929120421409607, Validation Loss: 0.8952065706253052\n",
      "Step 779/1000, Loss: 0.6929022073745728, Validation Loss: 0.8951978087425232\n",
      "Step 780/1000, Loss: 0.6928922533988953, Validation Loss: 0.8951891660690308\n",
      "Step 781/1000, Loss: 0.692882239818573, Validation Loss: 0.895180344581604\n",
      "Step 782/1000, Loss: 0.6928723454475403, Validation Loss: 0.895171582698822\n",
      "Step 783/1000, Loss: 0.6928623914718628, Validation Loss: 0.8951628804206848\n",
      "Step 784/1000, Loss: 0.6928524971008301, Validation Loss: 0.8951540589332581\n",
      "Step 785/1000, Loss: 0.6928424835205078, Validation Loss: 0.8951452970504761\n",
      "Step 786/1000, Loss: 0.6928326487541199, Validation Loss: 0.8951367139816284\n",
      "Step 787/1000, Loss: 0.6928226351737976, Validation Loss: 0.8951278328895569\n",
      "Step 788/1000, Loss: 0.6928126811981201, Validation Loss: 0.8951191902160645\n",
      "Step 789/1000, Loss: 0.6928027868270874, Validation Loss: 0.8951104879379272\n",
      "Step 790/1000, Loss: 0.6927927732467651, Validation Loss: 0.89510178565979\n",
      "Step 791/1000, Loss: 0.6927828192710876, Validation Loss: 0.8950929641723633\n",
      "Step 792/1000, Loss: 0.6927729249000549, Validation Loss: 0.8950841426849365\n",
      "Step 793/1000, Loss: 0.692763090133667, Validation Loss: 0.8950755596160889\n",
      "Step 794/1000, Loss: 0.6927531361579895, Validation Loss: 0.8950667977333069\n",
      "Step 795/1000, Loss: 0.692743182182312, Validation Loss: 0.8950580954551697\n",
      "Step 796/1000, Loss: 0.6927333474159241, Validation Loss: 0.8950493335723877\n",
      "Step 797/1000, Loss: 0.6927233338356018, Validation Loss: 0.8950405716896057\n",
      "Step 798/1000, Loss: 0.6927134394645691, Validation Loss: 0.8950319290161133\n",
      "Step 799/1000, Loss: 0.6927034854888916, Validation Loss: 0.8950231671333313\n",
      "Step 800/1000, Loss: 0.6926935911178589, Validation Loss: 0.8950144052505493\n",
      "Step 801/1000, Loss: 0.6926836967468262, Validation Loss: 0.8950057625770569\n",
      "Step 802/1000, Loss: 0.6926737427711487, Validation Loss: 0.8950013518333435\n",
      "Step 803/1000, Loss: 0.6926688551902771, Validation Loss: 0.8949969410896301\n",
      "Step 804/1000, Loss: 0.692663848400116, Validation Loss: 0.8949926495552063\n",
      "Step 805/1000, Loss: 0.6926589012145996, Validation Loss: 0.8949882388114929\n",
      "Step 806/1000, Loss: 0.6926539540290833, Validation Loss: 0.8949840068817139\n",
      "Step 807/1000, Loss: 0.6926490068435669, Validation Loss: 0.8949795365333557\n",
      "Step 808/1000, Loss: 0.6926440000534058, Validation Loss: 0.8949751257896423\n",
      "Step 809/1000, Loss: 0.6926391124725342, Validation Loss: 0.8949708938598633\n",
      "Step 810/1000, Loss: 0.692634105682373, Validation Loss: 0.8949664831161499\n",
      "Step 811/1000, Loss: 0.6926290988922119, Validation Loss: 0.8949621319770813\n",
      "Step 812/1000, Loss: 0.6926242113113403, Validation Loss: 0.8949577808380127\n",
      "Step 813/1000, Loss: 0.6926192045211792, Validation Loss: 0.8949533700942993\n",
      "Step 814/1000, Loss: 0.6926143169403076, Validation Loss: 0.8949490189552307\n",
      "Step 815/1000, Loss: 0.6926093697547913, Validation Loss: 0.8949446082115173\n",
      "Step 816/1000, Loss: 0.6926044821739197, Validation Loss: 0.8949403166770935\n",
      "Step 817/1000, Loss: 0.6925994753837585, Validation Loss: 0.8949359655380249\n",
      "Step 818/1000, Loss: 0.692594587802887, Validation Loss: 0.8949315547943115\n",
      "Step 819/1000, Loss: 0.6925895810127258, Validation Loss: 0.8949272632598877\n",
      "Step 820/1000, Loss: 0.6925846338272095, Validation Loss: 0.8949229717254639\n",
      "Step 821/1000, Loss: 0.6925796270370483, Validation Loss: 0.8949185013771057\n",
      "Step 822/1000, Loss: 0.692574679851532, Validation Loss: 0.8949142098426819\n",
      "Step 823/1000, Loss: 0.6925697922706604, Validation Loss: 0.8949098587036133\n",
      "Step 824/1000, Loss: 0.692564845085144, Validation Loss: 0.8949055075645447\n",
      "Step 825/1000, Loss: 0.6925599575042725, Validation Loss: 0.8949011564254761\n",
      "Step 826/1000, Loss: 0.6925550103187561, Validation Loss: 0.8948966860771179\n",
      "Step 827/1000, Loss: 0.692550003528595, Validation Loss: 0.8948925137519836\n",
      "Step 828/1000, Loss: 0.6925450563430786, Validation Loss: 0.8948881030082703\n",
      "Step 829/1000, Loss: 0.6925401091575623, Validation Loss: 0.8948836922645569\n",
      "Step 830/1000, Loss: 0.6925351023674011, Validation Loss: 0.8948792815208435\n",
      "Step 831/1000, Loss: 0.6925301551818848, Validation Loss: 0.8948749899864197\n",
      "Step 832/1000, Loss: 0.6925253868103027, Validation Loss: 0.8948706388473511\n",
      "Step 833/1000, Loss: 0.6925203800201416, Validation Loss: 0.8948662281036377\n",
      "Step 834/1000, Loss: 0.6925154328346252, Validation Loss: 0.8948619961738586\n",
      "Step 835/1000, Loss: 0.6925104856491089, Validation Loss: 0.8948575854301453\n",
      "Step 836/1000, Loss: 0.6925054788589478, Validation Loss: 0.8948531746864319\n",
      "Step 837/1000, Loss: 0.6925005316734314, Validation Loss: 0.8948488831520081\n",
      "Step 838/1000, Loss: 0.6924956440925598, Validation Loss: 0.8948444724082947\n",
      "Step 839/1000, Loss: 0.6924906969070435, Validation Loss: 0.8948401212692261\n",
      "Step 840/1000, Loss: 0.6924857497215271, Validation Loss: 0.8948358297348022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 841/1000, Loss: 0.6924808621406555, Validation Loss: 0.8948315382003784\n",
      "Step 842/1000, Loss: 0.6924759745597839, Validation Loss: 0.894827127456665\n",
      "Step 843/1000, Loss: 0.6924709677696228, Validation Loss: 0.8948227167129517\n",
      "Step 844/1000, Loss: 0.6924660205841064, Validation Loss: 0.8948185443878174\n",
      "Step 845/1000, Loss: 0.6924611330032349, Validation Loss: 0.8948141932487488\n",
      "Step 846/1000, Loss: 0.6924561262130737, Validation Loss: 0.8948098421096802\n",
      "Step 847/1000, Loss: 0.6924512982368469, Validation Loss: 0.8948054313659668\n",
      "Step 848/1000, Loss: 0.692446231842041, Validation Loss: 0.8948010802268982\n",
      "Step 849/1000, Loss: 0.6924413442611694, Validation Loss: 0.8947968482971191\n",
      "Step 850/1000, Loss: 0.6924364566802979, Validation Loss: 0.8947924375534058\n",
      "Step 851/1000, Loss: 0.6924315094947815, Validation Loss: 0.8947880268096924\n",
      "Step 852/1000, Loss: 0.6924266219139099, Validation Loss: 0.8947837352752686\n",
      "Step 853/1000, Loss: 0.6924216151237488, Validation Loss: 0.8947794437408447\n",
      "Step 854/1000, Loss: 0.6924167275428772, Validation Loss: 0.8947750329971313\n",
      "Step 855/1000, Loss: 0.6924118399620056, Validation Loss: 0.8947707414627075\n",
      "Step 856/1000, Loss: 0.6924068331718445, Validation Loss: 0.8947663903236389\n",
      "Step 857/1000, Loss: 0.6924018859863281, Validation Loss: 0.8947619795799255\n",
      "Step 858/1000, Loss: 0.6923969984054565, Validation Loss: 0.8947575688362122\n",
      "Step 859/1000, Loss: 0.6923920512199402, Validation Loss: 0.8947532176971436\n",
      "Step 860/1000, Loss: 0.6923871040344238, Validation Loss: 0.8947489857673645\n",
      "Step 861/1000, Loss: 0.6923822164535522, Validation Loss: 0.8947446942329407\n",
      "Step 862/1000, Loss: 0.6923773288726807, Validation Loss: 0.8947402834892273\n",
      "Step 863/1000, Loss: 0.6923723220825195, Validation Loss: 0.8947358727455139\n",
      "Step 864/1000, Loss: 0.6923673748970032, Validation Loss: 0.8947316408157349\n",
      "Step 865/1000, Loss: 0.6923624873161316, Validation Loss: 0.8947272896766663\n",
      "Step 866/1000, Loss: 0.6923575401306152, Validation Loss: 0.8947228193283081\n",
      "Step 867/1000, Loss: 0.6923526525497437, Validation Loss: 0.8947185277938843\n",
      "Step 868/1000, Loss: 0.6923477053642273, Validation Loss: 0.8947141766548157\n",
      "Step 869/1000, Loss: 0.6923428177833557, Validation Loss: 0.8947098851203918\n",
      "Step 870/1000, Loss: 0.6923378109931946, Validation Loss: 0.8947055339813232\n",
      "Step 871/1000, Loss: 0.6923328638076782, Validation Loss: 0.8947011232376099\n",
      "Step 872/1000, Loss: 0.6923279762268066, Validation Loss: 0.8946969509124756\n",
      "Step 873/1000, Loss: 0.6923230886459351, Validation Loss: 0.8946925401687622\n",
      "Step 874/1000, Loss: 0.6923181414604187, Validation Loss: 0.894688069820404\n",
      "Step 875/1000, Loss: 0.6923132538795471, Validation Loss: 0.894683837890625\n",
      "Step 876/1000, Loss: 0.6923083066940308, Validation Loss: 0.8946794867515564\n",
      "Step 877/1000, Loss: 0.692303478717804, Validation Loss: 0.8946751356124878\n",
      "Step 878/1000, Loss: 0.6922985315322876, Validation Loss: 0.894670844078064\n",
      "Step 879/1000, Loss: 0.6922935247421265, Validation Loss: 0.8946664333343506\n",
      "Step 880/1000, Loss: 0.6922886371612549, Validation Loss: 0.8946621417999268\n",
      "Step 881/1000, Loss: 0.6922836899757385, Validation Loss: 0.8946577906608582\n",
      "Step 882/1000, Loss: 0.6922788023948669, Validation Loss: 0.8946533203125\n",
      "Step 883/1000, Loss: 0.6922738552093506, Validation Loss: 0.894649088382721\n",
      "Step 884/1000, Loss: 0.692268967628479, Validation Loss: 0.8946447968482971\n",
      "Step 885/1000, Loss: 0.6922640800476074, Validation Loss: 0.8946404457092285\n",
      "Step 886/1000, Loss: 0.6922591328620911, Validation Loss: 0.8946360349655151\n",
      "Step 887/1000, Loss: 0.6922542452812195, Validation Loss: 0.8946316242218018\n",
      "Step 888/1000, Loss: 0.6922493577003479, Validation Loss: 0.8946274518966675\n",
      "Step 889/1000, Loss: 0.6922443509101868, Validation Loss: 0.8946231007575989\n",
      "Step 890/1000, Loss: 0.6922394633293152, Validation Loss: 0.8946187496185303\n",
      "Step 891/1000, Loss: 0.6922345161437988, Validation Loss: 0.8946143388748169\n",
      "Step 892/1000, Loss: 0.6922297477722168, Validation Loss: 0.8946099281311035\n",
      "Step 893/1000, Loss: 0.6922247409820557, Validation Loss: 0.8946056365966797\n",
      "Step 894/1000, Loss: 0.6922197937965393, Validation Loss: 0.8946012854576111\n",
      "Step 895/1000, Loss: 0.6922149062156677, Validation Loss: 0.8945969343185425\n",
      "Step 896/1000, Loss: 0.6922100186347961, Validation Loss: 0.8945926427841187\n",
      "Step 897/1000, Loss: 0.6922050714492798, Validation Loss: 0.8945883512496948\n",
      "Step 898/1000, Loss: 0.6922001242637634, Validation Loss: 0.8945839405059814\n",
      "Step 899/1000, Loss: 0.6921952366828918, Validation Loss: 0.8945796489715576\n",
      "Step 900/1000, Loss: 0.6921903491020203, Validation Loss: 0.894575297832489\n",
      "Step 901/1000, Loss: 0.6921853423118591, Validation Loss: 0.8945710062980652\n",
      "Step 902/1000, Loss: 0.6921804547309875, Validation Loss: 0.8945688009262085\n",
      "Step 903/1000, Loss: 0.6921780705451965, Validation Loss: 0.8945665955543518\n",
      "Step 904/1000, Loss: 0.6921756863594055, Validation Loss: 0.8945644497871399\n",
      "Step 905/1000, Loss: 0.692173182964325, Validation Loss: 0.8945622444152832\n",
      "Step 906/1000, Loss: 0.6921706795692444, Validation Loss: 0.8945601582527161\n",
      "Step 907/1000, Loss: 0.6921682953834534, Validation Loss: 0.8945579528808594\n",
      "Step 908/1000, Loss: 0.6921657919883728, Validation Loss: 0.8945558071136475\n",
      "Step 909/1000, Loss: 0.6921634078025818, Validation Loss: 0.8945536613464355\n",
      "Step 910/1000, Loss: 0.6921609044075012, Validation Loss: 0.8945515155792236\n",
      "Step 911/1000, Loss: 0.6921585202217102, Validation Loss: 0.8945493102073669\n",
      "Step 912/1000, Loss: 0.6921560168266296, Validation Loss: 0.8945472240447998\n",
      "Step 913/1000, Loss: 0.6921535730361938, Validation Loss: 0.8945450186729431\n",
      "Step 914/1000, Loss: 0.6921510696411133, Validation Loss: 0.8945428133010864\n",
      "Step 915/1000, Loss: 0.6921486854553223, Validation Loss: 0.8945407867431641\n",
      "Step 916/1000, Loss: 0.6921463012695312, Validation Loss: 0.8945385217666626\n",
      "Step 917/1000, Loss: 0.6921437978744507, Validation Loss: 0.8945364356040955\n",
      "Step 918/1000, Loss: 0.6921414136886597, Validation Loss: 0.894534170627594\n",
      "Step 919/1000, Loss: 0.6921389698982239, Validation Loss: 0.8945320248603821\n",
      "Step 920/1000, Loss: 0.6921365261077881, Validation Loss: 0.8945299386978149\n",
      "Step 921/1000, Loss: 0.6921340227127075, Validation Loss: 0.894527792930603\n",
      "Step 922/1000, Loss: 0.692131519317627, Validation Loss: 0.8945255875587463\n",
      "Step 923/1000, Loss: 0.6921291947364807, Validation Loss: 0.8945233225822449\n",
      "Step 924/1000, Loss: 0.6921267509460449, Validation Loss: 0.8945213556289673\n",
      "Step 925/1000, Loss: 0.6921243071556091, Validation Loss: 0.8945190906524658\n",
      "Step 926/1000, Loss: 0.6921218633651733, Validation Loss: 0.8945169448852539\n",
      "Step 927/1000, Loss: 0.6921194195747375, Validation Loss: 0.8945149183273315\n",
      "Step 928/1000, Loss: 0.6921169757843018, Validation Loss: 0.8945125937461853\n",
      "Step 929/1000, Loss: 0.6921146512031555, Validation Loss: 0.8945105075836182\n",
      "Step 930/1000, Loss: 0.6921120882034302, Validation Loss: 0.8945084810256958\n",
      "Step 931/1000, Loss: 0.6921096444129944, Validation Loss: 0.8945061564445496\n",
      "Step 932/1000, Loss: 0.6921072006225586, Validation Loss: 0.8945040702819824\n",
      "Step 933/1000, Loss: 0.6921047568321228, Validation Loss: 0.8945018649101257\n",
      "Step 934/1000, Loss: 0.692102313041687, Validation Loss: 0.8944997191429138\n",
      "Step 935/1000, Loss: 0.6920998692512512, Validation Loss: 0.8944975733757019\n",
      "Step 936/1000, Loss: 0.6920974254608154, Validation Loss: 0.8944953083992004\n",
      "Step 937/1000, Loss: 0.6920949816703796, Validation Loss: 0.8944932818412781\n",
      "Step 938/1000, Loss: 0.6920925378799438, Validation Loss: 0.8944911360740662\n",
      "Step 939/1000, Loss: 0.6920900940895081, Validation Loss: 0.8944888710975647\n",
      "Step 940/1000, Loss: 0.6920876502990723, Validation Loss: 0.8944867253303528\n",
      "Step 941/1000, Loss: 0.6920852661132812, Validation Loss: 0.8944846391677856\n",
      "Step 942/1000, Loss: 0.6920828223228455, Validation Loss: 0.8944823741912842\n",
      "Step 943/1000, Loss: 0.6920803785324097, Validation Loss: 0.894480288028717\n",
      "Step 944/1000, Loss: 0.6920779347419739, Validation Loss: 0.8944781422615051\n",
      "Step 945/1000, Loss: 0.6920754909515381, Validation Loss: 0.8944759368896484\n",
      "Step 946/1000, Loss: 0.6920730471611023, Validation Loss: 0.894473671913147\n",
      "Step 947/1000, Loss: 0.6920706629753113, Validation Loss: 0.8944715261459351\n",
      "Step 948/1000, Loss: 0.6920681595802307, Validation Loss: 0.8944694399833679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 949/1000, Loss: 0.6920657753944397, Validation Loss: 0.894467294216156\n",
      "Step 950/1000, Loss: 0.6920633912086487, Validation Loss: 0.8944650888442993\n",
      "Step 951/1000, Loss: 0.6920608878135681, Validation Loss: 0.8944629430770874\n",
      "Step 952/1000, Loss: 0.6920585036277771, Validation Loss: 0.8944608569145203\n",
      "Step 953/1000, Loss: 0.6920560002326965, Validation Loss: 0.8944585919380188\n",
      "Step 954/1000, Loss: 0.6920536160469055, Validation Loss: 0.8944565057754517\n",
      "Step 955/1000, Loss: 0.692051112651825, Validation Loss: 0.8944542407989502\n",
      "Step 956/1000, Loss: 0.6920486092567444, Validation Loss: 0.8944521546363831\n",
      "Step 957/1000, Loss: 0.6920462250709534, Validation Loss: 0.8944500088691711\n",
      "Step 958/1000, Loss: 0.6920438408851624, Validation Loss: 0.894447922706604\n",
      "Step 959/1000, Loss: 0.6920413374900818, Validation Loss: 0.8944456577301025\n",
      "Step 960/1000, Loss: 0.6920389533042908, Validation Loss: 0.8944435715675354\n",
      "Step 961/1000, Loss: 0.6920364499092102, Validation Loss: 0.8944414258003235\n",
      "Step 962/1000, Loss: 0.692034125328064, Validation Loss: 0.894439160823822\n",
      "Step 963/1000, Loss: 0.6920316219329834, Validation Loss: 0.8944371342658997\n",
      "Step 964/1000, Loss: 0.6920291781425476, Validation Loss: 0.8944348692893982\n",
      "Step 965/1000, Loss: 0.692026674747467, Validation Loss: 0.8944327235221863\n",
      "Step 966/1000, Loss: 0.6920242309570312, Validation Loss: 0.8944306969642639\n",
      "Step 967/1000, Loss: 0.6920218467712402, Validation Loss: 0.894428551197052\n",
      "Step 968/1000, Loss: 0.6920194029808044, Validation Loss: 0.8944263458251953\n",
      "Step 969/1000, Loss: 0.6920168995857239, Validation Loss: 0.8944242000579834\n",
      "Step 970/1000, Loss: 0.6920145750045776, Validation Loss: 0.8944219350814819\n",
      "Step 971/1000, Loss: 0.6920121312141418, Validation Loss: 0.8944198489189148\n",
      "Step 972/1000, Loss: 0.6920096278190613, Validation Loss: 0.8944177031517029\n",
      "Step 973/1000, Loss: 0.692007303237915, Validation Loss: 0.894415557384491\n",
      "Step 974/1000, Loss: 0.6920047998428345, Validation Loss: 0.894413411617279\n",
      "Step 975/1000, Loss: 0.6920023560523987, Validation Loss: 0.8944112062454224\n",
      "Step 976/1000, Loss: 0.6920000314712524, Validation Loss: 0.8944091200828552\n",
      "Step 977/1000, Loss: 0.6919975280761719, Validation Loss: 0.8944069147109985\n",
      "Step 978/1000, Loss: 0.6919950246810913, Validation Loss: 0.8944048285484314\n",
      "Step 979/1000, Loss: 0.6919927000999451, Validation Loss: 0.8944026827812195\n",
      "Step 980/1000, Loss: 0.6919902563095093, Validation Loss: 0.8944004774093628\n",
      "Step 981/1000, Loss: 0.6919877529144287, Validation Loss: 0.8943983316421509\n",
      "Step 982/1000, Loss: 0.6919852495193481, Validation Loss: 0.8943962454795837\n",
      "Step 983/1000, Loss: 0.6919829249382019, Validation Loss: 0.8943939805030823\n",
      "Step 984/1000, Loss: 0.6919804811477661, Validation Loss: 0.8943918347358704\n",
      "Step 985/1000, Loss: 0.6919779777526855, Validation Loss: 0.894389808177948\n",
      "Step 986/1000, Loss: 0.6919755935668945, Validation Loss: 0.8943875432014465\n",
      "Step 987/1000, Loss: 0.691973090171814, Validation Loss: 0.8943853974342346\n",
      "Step 988/1000, Loss: 0.691970705986023, Validation Loss: 0.8943831324577332\n",
      "Step 989/1000, Loss: 0.6919682025909424, Validation Loss: 0.8943811655044556\n",
      "Step 990/1000, Loss: 0.6919658780097961, Validation Loss: 0.8943789601325989\n",
      "Step 991/1000, Loss: 0.6919633746147156, Validation Loss: 0.894376814365387\n",
      "Step 992/1000, Loss: 0.6919609904289246, Validation Loss: 0.8943746089935303\n",
      "Step 993/1000, Loss: 0.6919586062431335, Validation Loss: 0.8943725228309631\n",
      "Step 994/1000, Loss: 0.6919561624526978, Validation Loss: 0.8943703770637512\n",
      "Step 995/1000, Loss: 0.691953718662262, Validation Loss: 0.8943682909011841\n",
      "Step 996/1000, Loss: 0.6919512748718262, Validation Loss: 0.8943660259246826\n",
      "Step 997/1000, Loss: 0.6919487118721008, Validation Loss: 0.8943638801574707\n",
      "Step 998/1000, Loss: 0.6919465065002441, Validation Loss: 0.8943617939949036\n",
      "Step 999/1000, Loss: 0.6919440031051636, Validation Loss: 0.8943595290184021\n",
      "Step 1000/1000, Loss: 0.6919416189193726, Validation Loss: 0.894357442855835\n",
      "fixing (0,0,0) with 1/sqrt(x), r2=0.9732282161712646\n",
      "fixing (0,0,1) with erfc, r2=0.601159393787384\n",
      "fixing (0,1,0) with erfc, r2=0.9766783714294434\n",
      "fixing (0,1,1) with erfc, r2=0.9785842299461365\n",
      "fixing (1,0,0) with 1/sqrt(x), r2=0.9968171715736389\n",
      "fixing (1,0,1) with 1/sqrt(x), r2=0.962705671787262\n",
      "fixing (1,1,0) with erfc, r2=0.8731398582458496\n",
      "fixing (1,1,1) with erfc, r2=0.7001431584358215\n",
      "fixing (2,0,0) with 1/sqrt(x), r2=0.9965226054191589\n",
      "fixing (2,1,0) with erfc, r2=0.9866896867752075\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sympy import symbols, sympify\n",
    "from scipy.special import erfc\n",
    "from kan import KAN, add_symbolic, create_dataset\n",
    "\n",
    "# Parameters\n",
    "alpha = 2.5e-6\n",
    "T0 = 20\n",
    "T1 = 40\n",
    "L = 4\n",
    "dx = 0.1\n",
    "dt = 1800\n",
    "tMax = 86400\n",
    "W = 30\n",
    "\n",
    "# Time and space grid\n",
    "x = np.arange(0, L + dx, dx)\n",
    "t = np.arange(dt, tMax + dt, dt)\n",
    "\n",
    "# Initialize temperature data storage\n",
    "TemperatureData = np.zeros((len(t), len(x)))\n",
    "\n",
    "# Compute temperature distribution and store results\n",
    "for k in range(len(t)):\n",
    "    TemperatureData[k, :] = T0 + (T1 - T0) * erfc(x / (2 * np.sqrt(alpha * t[k])))\n",
    "\n",
    "# Prepare the data\n",
    "x_data = np.array([[x_val, t_val] for t_val in t for x_val in x])\n",
    "y_data = TemperatureData.flatten()\n",
    "\n",
    "# # Normalize the data\n",
    "x_mean = np.mean(x_data, axis=0)\n",
    "x_std = np.std(x_data, axis=0)\n",
    "y_mean = np.mean(y_data)\n",
    "y_std = np.std(y_data)\n",
    "\n",
    "x_data_normalized = (x_data - x_mean) / x_std\n",
    "y_data_normalized = (y_data - y_mean) / y_std\n",
    "# x_data_normalized = x_data\n",
    "# y_data_normalized = y_data\n",
    "\n",
    "# Convert to tensors\n",
    "x_tensor = torch.tensor(x_data_normalized, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_data_normalized, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Split dataset into training and test sets\n",
    "train_size = int(0.8 * len(x_tensor))\n",
    "test_size = len(x_tensor) - train_size\n",
    "train_input, test_input = torch.split(x_tensor, [train_size, test_size])\n",
    "train_label, test_label = torch.split(y_tensor, [train_size, test_size])\n",
    "\n",
    "# Create the dataset dictionary as expected by KAN\n",
    "dataset = {\n",
    "    'train_input': train_input,\n",
    "    'train_label': train_label,\n",
    "    'test_input': test_input,\n",
    "    'test_label': test_label\n",
    "}\n",
    "\n",
    "# Add erfc to the symbolic library\n",
    "add_symbolic('erfc', torch.special.erfc)\n",
    "\n",
    "# Function to train KAN with early stopping and learning rate scheduling\n",
    "def train_with_early_stopping(model, dataset, opt=\"LBFGS\", steps=100, patience=20, initial_lr=0.0001, min_lr=1e-7, lr_decay=0.5):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=initial_lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience_counter = 0\n",
    "    current_lr = initial_lr\n",
    "\n",
    "    for step in range(steps):\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(dataset['train_input'])\n",
    "            loss = criterion(outputs, dataset['train_label'])\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            return loss\n",
    "\n",
    "        loss = optimizer.step(closure).item()\n",
    "\n",
    "        # Validation step\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(dataset['test_input'])\n",
    "            val_loss = criterion(val_outputs, dataset['test_label']).item()\n",
    "\n",
    "        print(f'Step {step + 1}/{steps}, Loss: {loss}, Validation Loss: {val_loss}')\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "        # Learning rate scheduling\n",
    "        if step % patience == 0 and step > 0:\n",
    "            current_lr = max(min_lr, current_lr * lr_decay)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = current_lr\n",
    "\n",
    "        # Check for NaN values\n",
    "        if np.isnan(loss) or np.isnan(val_loss):\n",
    "            print(\"NaN detected, stopping training\")\n",
    "            break\n",
    "\n",
    "    # Load the best model\n",
    "    if best_model is not None:\n",
    "        model.load_state_dict(best_model)\n",
    "\n",
    "# Initial training with a coarse grid\n",
    "initial_grid = 3\n",
    "model = KAN(width=[2, 2, 2, 1], grid=initial_grid, k=3, seed=0)\n",
    "train_with_early_stopping(model, dataset, steps=1000, patience=100, initial_lr=0.002)\n",
    "\n",
    "# Iteratively refine the grid and retrain the model\n",
    "grids = [5, 10, 20]\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for grid in grids:\n",
    "    new_model = KAN(width=[2, 2, 2, 1], grid=grid, k=3).initialize_from_another_model(model, dataset['train_input'])\n",
    "    train_with_early_stopping(new_model, dataset, steps=1000, patience=100, initial_lr=0.00025)\n",
    "    model = new_model  # Update the model to the new refined grid model\n",
    "\n",
    "    # Collect training and test losses\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(dataset['train_input'])\n",
    "        train_loss = torch.nn.functional.mse_loss(train_outputs, dataset['train_label']).item()\n",
    "        test_outputs = model(dataset['test_input'])\n",
    "        test_loss = torch.nn.functional.mse_loss(test_outputs, dataset['test_label']).item()\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "# Automatically set activation functions to be symbolic\n",
    "lib = ['x', '1/sqrt(x)', 'erfc']\n",
    "model.auto_symbolic(lib=lib)\n",
    "\n",
    "# Prune the model\n",
    "model = model.prune()\n",
    "\n",
    "# Obtain the symbolic formula and denormalize it\n",
    "symbolic_formula = model.symbolic_formula()[0][0]\n",
    "# symbolic_formula_denormalized = sympify(symbolic_formula.replace('x_1', '(x_1*{:.6f}+{:.6f})'.format(x_std[0], x_mean[0]))\n",
    "#                                         .replace('x_2', '(x_2*{:.6f}+{:.6f})'.format(x_std[1], x_mean[1])))\n",
    "# symbolic_formula_denormalized = (symbolic_formula_denormalized * y_std + y_mean).simplify()\n",
    "\n",
    "# print(\"Discovered Symbolic Formula:\")\n",
    "# print(symbolic_formula_denormalized)\n",
    "\n",
    "# Create output directory for plots\n",
    "outputDir = 'TemperaturePlots'\n",
    "if not os.path.exists(outputDir):\n",
    "    os.makedirs(outputDir)\n",
    "\n",
    "# Plot predicted temperature distribution\n",
    "x_test = np.array([[x_val, tMax] for x_val in x])  # Use the final time step for testing\n",
    "x_test_normalized = (x_test - x_mean) / x_std\n",
    "x_test_tensor = torch.tensor(x_test_normalized, dtype=torch.float32)\n",
    "\n",
    "# x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "predicted_temperature = model(x_test_tensor).detach().numpy().flatten() * y_std + y_mean\n",
    "# predicted_temperature = model(x_test_tensor).detach().numpy().flatten()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, predicted_temperature, label='Predicted')\n",
    "plt.plot(x, TemperatureData[-1, :], label='Actual', linestyle='--')\n",
    "plt.xlabel('Position along the wall thickness (m)')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('Steady-State Temperature Distribution in the Wall')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(f'{outputDir}/PredictedTemperatureDistribution.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot training and test losses over different grid refinements\n",
    "plt.figure()\n",
    "plt.plot(grids, train_losses, marker='o', label='Train Loss')\n",
    "plt.plot(grids, test_losses, marker='o', label='Test Loss')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Grid Size')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Training and Test Losses over Grid Refinements')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14422989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered Symbolic Formula (Normalized):\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - 0.1 \\operatorname{erfc}{\\left(- 0.53 \\operatorname{erfc}{\\left(1.71 \\operatorname{erfc}{\\left(- 8.74 x_{1} - 9.8 \\right)} + 91.92 \\operatorname{erfc}{\\left(1.45 x_{2} - 0.14 \\right)} - 2.78 \\right)} + 76.72 - \\frac{58.59}{\\sqrt{-0.43 + \\frac{1}{\\sqrt{0.05 x_{1} + 1}}}} \\right)} - 0.33 + \\frac{0.13}{\\sqrt{- 0.01 \\operatorname{erfc}{\\left(2.52 \\operatorname{erfc}{\\left(- 8.74 x_{1} - 9.8 \\right)} + 135.75 \\operatorname{erfc}{\\left(1.45 x_{2} - 0.14 \\right)} - 8.32 \\right)} + 1 - \\frac{0.56}{\\sqrt{1 - \\frac{0.64}{\\sqrt{0.05 x_{1} + 1}}}}}}$"
      ],
      "text/plain": [
       "-0.1*erfc(-0.53*erfc(1.71*erfc(-8.74*x_1 - 9.8) + 91.92*erfc(1.45*x_2 - 0.14) - 2.78) + 76.72 - 58.59/sqrt(-0.43 + 1/sqrt(0.05*x_1 + 1))) - 0.33 + 0.13/sqrt(-0.e-2*erfc(2.52*erfc(-8.74*x_1 - 9.8) + 135.75*erfc(1.45*x_2 - 0.14) - 8.32) + 1 - 0.56/sqrt(1 - 0.64/sqrt(0.05*x_1 + 1)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Discovered Symbolic Formula (Normalized):\")\n",
    "model.symbolic_formula()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd4dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Discovered Symbolic Formula (Original):\")\n",
    "# print(original_formula)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
